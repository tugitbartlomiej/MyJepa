\clearpage
%% ============================================================
\section{Ogólny obraz: co budujemy i po co?}
\label{sec:big_picture}
%% ============================================================

Zanim wejdziemy w matematykę, ustalmy \textbf{co jest czym}.
W tym projekcie mamy do czynienia z trzema rzeczami, które łatwo pomylić:

\begin{center}
\renewcommand{\arraystretch}{1.8}
\begin{tabular}{cp{10cm}}
\toprule
\textbf{Nazwa} & \textbf{Czym jest?} \\
\midrule
\textbf{ViT} \newline (Vision Transformer) &
\textbf{Sieć neuronowa} -- architektura encodera.
Zwykła maszyna: obraz wchodzi, wektor liczb (embedding) wychodzi.
ViT sam z siebie \textit{nie wie}, czego się uczyć -- potrzebuje metody treningu. \\
\textbf{LeJEPA} \newline (metoda treningu) &
\textbf{Przepis na trening} ViT-a bez etykiet (self-supervised).
Mówi: ``podaj sieci dwa widoki tego samego kadru, policz taki-a-taki loss,
zaktualizuj wagi''. LeJEPA \textit{nie jest} siecią neuronową -- to algorytm,
który \textit{używa} ViT-a jako swojego encodera. \\
\textbf{SIGReg} \newline (regularyzator) &
\textbf{Składnik lossu} wewnątrz LeJEPA.
Wymusza, żeby embeddingi miały rozkład $\mathcal{N}(\mathbf{0}, \mathbf{I})$
(izotropowy Gauss -- temu poświęcony jest cały ten dokument). \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Jak to się łączy? Schemat}

Cały system wygląda tak:

\begin{center}
\fbox{\parbox{0.92\textwidth}{
\centering
\textbf{PRETRAINING (uczenie bez etykiet)} \\[0.8em]
\begin{tabular}{ccccc}
Ramka wideo & $\xrightarrow{\text{augmentacje}}$ & Dwa widoki &
$\xrightarrow{\quad\text{ViT}\quad}$ & Embeddingi \\
$\mathbf{x}$ & & $\mathbf{x}', \mathbf{x}''$ & & $\mathbf{z}', \mathbf{z}'' \in \mathbb{R}^{384}$
\end{tabular}
\\[0.8em]
$\downarrow$ \\[0.3em]
\textbf{Loss LeJEPA} = $\lambda \cdot \underbrace{\text{SIGReg}(\mathbf{z})}_{\text{wymusza } \mathcal{N}(\mathbf{0}, \mathbf{I})}
+ (1 - \lambda) \cdot \underbrace{\text{prediction loss}}_{\text{podobne widoki} \to \text{podobne embeddingi}}$
\\[0.5em]
$\downarrow$ \\[0.3em]
Gradient descent aktualizuje wagi $\theta$ ViT-a
}}
\end{center}

\bigskip

\begin{center}
\fbox{\parbox{0.92\textwidth}{
\centering
\textbf{DOWNSTREAM (użycie po treningu)} \\[0.8em]
\begin{tabular}{ccccc}
Nowy obraz & $\xrightarrow{\quad\text{ViT (zamrożony)}\quad}$ & Embedding &
$\xrightarrow{\text{mały klasyfikator}}$ & Wynik \\
$\mathbf{x}$ & & $\mathbf{z} \in \mathbb{R}^{384}$ & & ``faza phaco''
\end{tabular}
\\[0.5em]
Wagi $\theta$ \textbf{nie zmieniają się} -- ViT działa jak zamrożona ``czarna skrzynka''.\\
Uczymy \textit{tylko} mały klasyfikator (linear probe / k-NN).
}}
\end{center}

\subsection{Analogia}

\begin{itemize}[leftmargin=2em]
  \item \textbf{ViT} = \textbf{uczeń} (sieć neuronowa z wagami $\theta$).
  \item \textbf{LeJEPA} = \textbf{program nauczania w szkole}
        -- mówi uczniowi \textit{jak} się uczyć (jakie ćwiczenia robić, jak oceniać postępy).
  \item \textbf{SIGReg} = \textbf{jedno z ćwiczeń} w programie
        -- ``upewnij się, że twoje notatki (embeddingi) są równomiernie rozłożone''.
  \item \textbf{Downstream} = \textbf{praca po szkole}
        -- uczeń (ViT) stosuje swoją wiedzę (zamrożone $\theta^*$) do nowych zadań.
        Program nauczania (LeJEPA) już \textit{nie istnieje} -- został tylko wytrenowany uczeń.
\end{itemize}

\begin{keyinsight}[Zapamiętaj to!]
\textbf{ViT} produkuje embeddingi. \textbf{LeJEPA} mówi ViT-owi \textit{jak się uczyć}.
\textbf{SIGReg} pilnuje, żeby embeddingi miały \textit{właściwy rozkład}.

Po treningu LeJEPA i SIGReg ``znikają'' -- zostaje \textbf{wytrenowany ViT}
z wagami $\theta^*$, który produkuje dobre embeddingi.

Cały ten dokument wyjaśnia \textit{dlaczego} SIGReg wymusza akurat izotropowy Gauss
$\mathcal{N}(\mathbf{0}, \mathbf{I})$ -- i dlaczego to jest matematycznie optymalne.
\end{keyinsight}

