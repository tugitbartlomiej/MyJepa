\clearpage
%% ============================================================
\section{Jak działa Vision Transformer (ViT)?}
\label{sec:vit}
%% ============================================================

W LeJEPA encoder $f_\theta$ to \textbf{Vision Transformer} (ViT).
Zanim powiemy \textit{jaki rozkład powinny mieć embeddingi},
musimy zrozumieć \textit{jak ViT je produkuje}.

\subsection{Krok 1: Obraz $\to$ patche (łatki)}

ViT \textbf{nie przetwarza pikseli po kolei} jak CNN.
Zamiast tego dzieli obraz na siatkę kwadratowych kawałków:

\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{rl}
Obraz wejściowy: & $224 \times 224 \times 3$ pikseli (RGB) \\
Rozmiar patcha: & $16 \times 16$ pikseli \\
Liczba patchy: & $\frac{224}{16} \times \frac{224}{16} = 14 \times 14 = \mathbf{196}$ patchy \\
Wymiar jednego patcha: & $16 \times 16 \times 3 = \mathbf{768}$ liczb (spłaszczone piksele) \\
\end{tabular}
\end{center}

Każdy patch to kawałek obrazu (np.\ fragment tęczówki, kawałek narzędzia).

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/vit_patches.pdf}
\caption{Od obrazu do tokenów. \textbf{1}: Obraz wejściowy.
\textbf{2}: Podział na $14 \times 14 = 196$ patchy (żółty = przykładowy patch).
\textbf{3}: Każdy patch spłaszczony do wektora $\mathbb{R}^{768}$.
\textbf{4}: Projekcja liniowa do $\mathbb{R}^{384}$ + dodanie tokenu [CLS].}
\label{fig:patches}
\end{figure}

\subsection{Krok 2: Projekcja liniowa (Patch Embedding)}

Wektor 768-wymiarowy to za dużo. ViT kompresuje go \textbf{mnożeniem macierzowym}:

\begin{equation}
\mathbf{e}_i = \mathbf{W}_{\text{patch}} \cdot \mathbf{p}_i + \mathbf{b}_{\text{patch}}
\quad \text{gdzie } \mathbf{W}_{\text{patch}} \in \mathbb{R}^{384 \times 768}
\end{equation}

To zwykłe mnożenie macierzy: $768$ pikseli $\to$ $384$ wymiarów.
Macierz $\mathbf{W}_{\text{patch}}$ jest \textbf{uczona} — sieć sama uczy się,
jakie cechy wyciągać z patcha.

\bigskip

% ============================================================
% ROZBUDOWANY PRZYKŁAD PROJEKCJI LINIOWEJ
% ============================================================

\begin{tcolorbox}[
  colback=lejepaBlue!5,
  colframe=lejepaBlue!80,
  fonttitle=\bfseries,
  title={Przykład krok po kroku: co dokładnie robi mnożenie macierzowe?},
  breakable,
]

Prawdziwe wymiary ($384 \times 768$) są za duże, żeby je zobaczyć.
Użyjemy \textbf{miniaturowego przykładu}: patch $3$ pikseli $\to$ embedding $2$ cech.
Zasada jest \textit{identyczna} — zmienia się tylko rozmiar.

\subsubsection{Krok A: Mamy patch (wektor pikseli)}

Wyobraź sobie maleńki patch $1 \times 1 \times 3$ (jeden piksel RGB):

\begin{equation}
\mathbf{p} = \begin{bmatrix} 0.8 \\ 0.2 \\ 0.1 \end{bmatrix}
\quad \leftarrow \text{3 liczby: } (\underbrace{0.8}_{\text{Red}},\; \underbrace{0.2}_{\text{Green}},\; \underbrace{0.1}_{\text{Blue}})
\end{equation}

To jest nasz ``wektor wejściowy'' $\mathbf{p} \in \mathbb{R}^3$.

\subsubsection{Krok B: Mamy macierz wag (uczoną)}

Sieć ma macierz $\mathbf{W} \in \mathbb{R}^{2 \times 3}$ — dwa wiersze, trzy kolumny:

\begin{equation}
\mathbf{W} = \begin{bmatrix}
w_{11} & w_{12} & w_{13} \\
w_{21} & w_{22} & w_{23}
\end{bmatrix}
= \begin{bmatrix}
0.5 & -0.3 & 0.1 \\
-0.2 & 0.7 & 0.4
\end{bmatrix}
\end{equation}

\textbf{Skąd te liczby?} Na początku treningu — \textbf{losowe}!
Potem gradient descent je zmienia, żeby dawały coraz lepsze cechy.

\subsubsection{Krok C: Mnożenie macierz $\times$ wektor}

\begin{equation}
\mathbf{e} = \mathbf{W} \cdot \mathbf{p} =
\begin{bmatrix}
w_{11} & w_{12} & w_{13} \\
w_{21} & w_{22} & w_{23}
\end{bmatrix}
\begin{bmatrix} p_1 \\ p_2 \\ p_3 \end{bmatrix}
= \begin{bmatrix}
w_{11} p_1 + w_{12} p_2 + w_{13} p_3 \\
w_{21} p_1 + w_{22} p_2 + w_{23} p_3
\end{bmatrix}
\end{equation}

\textbf{Reguła}: każdy element wyniku to \textbf{iloczyn skalarny} wiersza macierzy z wektorem wejściowym.

\medskip

Podstawiamy liczby:

\begin{align}
e_1 &= \underbrace{0.5}_{\mathclap{w_{11}}} \cdot \underbrace{0.8}_{\mathclap{p_1}}
+ \underbrace{(-0.3)}_{\mathclap{w_{12}}} \cdot \underbrace{0.2}_{\mathclap{p_2}}
+ \underbrace{0.1}_{\mathclap{w_{13}}} \cdot \underbrace{0.1}_{\mathclap{p_3}}
\nonumber\\
&= 0.40 + (-0.06) + 0.01 = \mathbf{0.35}
\label{eq:e1_example}
\\[0.8em]
e_2 &= \underbrace{(-0.2)}_{\mathclap{w_{21}}} \cdot \underbrace{0.8}_{\mathclap{p_1}}
+ \underbrace{0.7}_{\mathclap{w_{22}}} \cdot \underbrace{0.2}_{\mathclap{p_2}}
+ \underbrace{0.4}_{\mathclap{w_{23}}} \cdot \underbrace{0.1}_{\mathclap{p_3}}
\nonumber\\
&= (-0.16) + 0.14 + 0.04 = \mathbf{0.02}
\label{eq:e2_example}
\end{align}

\subsubsection{Krok D: Dodajemy bias}

Bias $\mathbf{b} \in \mathbb{R}^2$ to dodatkowy uczony wektor (przesunięcie):

\begin{equation}
\mathbf{e} = \mathbf{W} \cdot \mathbf{p} + \mathbf{b}
= \begin{bmatrix} 0.35 \\ 0.02 \end{bmatrix}
+ \begin{bmatrix} 0.1 \\ -0.05 \end{bmatrix}
= \begin{bmatrix} 0.45 \\ -0.03 \end{bmatrix}
\end{equation}

\textbf{Wynik}: z 3 pikseli RGB dostaliśmy 2-wymiarowy embedding $\mathbf{e} = (0.45,\; {-0.03})^\top$.

\subsubsection{Krok E: Co to znaczy geometrycznie?}

\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{cp{10cm}}
\toprule
\textbf{Element} & \textbf{Interpretacja} \\
\midrule
Wiersz 1 macierzy: $(0.5,\; {-0.3},\; 0.1)$ &
\textbf{Filtr cechy 1}: ``dużo Red, mało Green, trochę Blue''
$\Rightarrow$ reaguje na \textit{ciepłe} kolory (czerwień). \\
Wiersz 2 macierzy: $(-0.2,\; 0.7,\; 0.4)$ &
\textbf{Filtr cechy 2}: ``anty-Red, dużo Green, trochę Blue''
$\Rightarrow$ reaguje na \textit{zimne} kolory (zieleń/cyan). \\
$e_1 = 0.45$ & Nasz patch jest dość ``ciepły'' (dużo Red). \\
$e_2 = -0.03$ & Nasz patch jest prawie neutralny w ``zimnej'' cesze. \\
\bottomrule
\end{tabular}
\end{center}

Każdy wiersz macierzy $\mathbf{W}$ to \textbf{jeden ``detektor cechy''}.
Iloczyn skalarny mierzy \textbf{jak bardzo} patch pasuje do danego detektora.

\subsubsection{Krok F: Skalowanie do prawdziwego ViT-Small}

Teraz podstaw prawdziwe wymiary:

\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{rll}
\toprule
 & \textbf{Nasz przykład} & \textbf{ViT-Small (prawdziwy)} \\
\midrule
Patch (wejście) & $\mathbf{p} \in \mathbb{R}^{3}$ (3 piksele RGB) & $\mathbf{p} \in \mathbb{R}^{768}$ ($16 \times 16 \times 3$ pikseli) \\
Macierz wag & $\mathbf{W} \in \mathbb{R}^{2 \times 3}$ (6 wag) & $\mathbf{W} \in \mathbb{R}^{384 \times 768}$ (294\,912 wag) \\
Bias & $\mathbf{b} \in \mathbb{R}^{2}$ (2 biasy) & $\mathbf{b} \in \mathbb{R}^{384}$ (384 biasy) \\
Embedding (wyjście) & $\mathbf{e} \in \mathbb{R}^{2}$ (2 cechy) & $\mathbf{e} \in \mathbb{R}^{384}$ (384 cechy) \\
\midrule
\textit{Operacja na 1 cechę} & $3$ mnożenia + $3$ dodawania & $768$ mnożeń + $768$ dodawań \\
\textit{Wszystkie cechy} & $2 \times 3 = 6$ operacji & $384 \times 768 = 294\,912$ operacji \\
\textit{Wszystkie patche} & -- & $196 \times 294\,912 \approx \mathbf{57.8\text{M}}$ operacji \\
\bottomrule
\end{tabular}
\end{center}

\subsubsection{Wzorzec ogólny: co robi \textit{każdy} wiersz macierzy?}

Zapiszmy $k$-tą cechę embeddingu ($k = 1, \ldots, 384$):

\begin{equation}
\boxed{
e_k = \sum_{j=1}^{768} w_{kj}\, p_j + b_k
= \underbrace{w_{k1}\, p_1 + w_{k2}\, p_2 + \cdots + w_{k,768}\, p_{768}}_{\text{iloczyn skalarny: wiersz $k$ macierzy $\mathbf{W}$ z wektorem $\mathbf{p}$}} + \;b_k
}
\end{equation}

\textbf{Podsumowanie}: Każda z $384$ cech embeddingu to \textbf{ważona suma wszystkich 768 pikseli patcha}, z wagami wyuczonymi podczas treningu.
Sieć sama decyduje, jakie kombinacje pikseli są informatywne — to jej ``receptory''.

\end{tcolorbox}

\subsection{Krok 3: Embedding pozycyjny}

Po projekcji ViT dodaje \textbf{informację o pozycji} — inaczej nie wiedziałby,
\textit{gdzie} na obrazie jest dany patch (bo Transformer nie ma poczucia kolejności):

\begin{equation}
\mathbf{h}_i^{(0)} = \mathbf{e}_i + \mathbf{pos}_i
\quad \text{gdzie } \mathbf{pos}_i \in \mathbb{R}^{384} \text{ — uczony wektor dla pozycji } i
\end{equation}

Jest $196$ wektorów pozycyjnych (po jednym na każdy patch).

\bigskip

\begin{tcolorbox}[
  colback=lejepaBlue!5,
  colframe=lejepaBlue!80,
  fonttitle=\bfseries,
  title={Pogłębione wyjaśnienie: embedding pozycyjny},
  breakable,
]

\subsubsection{Co znaczy ``uczony wektor''?}

Embedding pozycyjny to \textbf{parametr sieci} -- nie jest wyliczony z żadnej formuły
matematycznej, tylko \textbf{trenowany przez gradient descent} razem z resztą wag.

W kodzie PyTorch to dosłownie jedna linia:
\begin{center}
\texttt{self.pos\_embed = nn.Parameter(torch.randn(1, 196, 384))}
\end{center}

To tworzy tablicę $196$ losowych wektorów po $384$ liczb każdy (łącznie $196 \times 384 = 75\,264$ parametrów).
Są one aktualizowane przez backpropagation tak samo, jak macierze
$\mathbf{W}_Q$, $\mathbf{W}_K$, $\mathbf{W}_V$ i wszystkie inne wagi sieci.

Alternatywą byłyby \textbf{stałe} (fixed) embeddingi pozycyjne -- np.\ sinusoidalne
jak w oryginalnym Transformerze (Vaswani et~al., 2017),
gdzie pozycje koduje się wzorem $\sin(\text{pos}/10000^{2i/d})$.
Ale ViT (Dosovitskiy et~al., 2020) wybrał wariant \textbf{uczony},
bo działa lepiej na obrazach.

\subsubsection{Jak działa mapowanie: patch $i$ $\to$ $\mathbf{pos}_i$?}

To jest \textbf{zwykłe indeksowanie tablicy} -- najprostsza możliwa operacja:

\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{cl}
\toprule
\textbf{Numer patcha} & \textbf{Wektor pozycyjny} \\
\midrule
$1$ (pozycja $(1,1)$ -- lewy górny róg) & $\mathbf{pos}_1 = [0.31,\; 0.28,\; {-0.15},\; \ldots]$ \\
$2$ (pozycja $(1,2)$ -- obok w prawo) & $\mathbf{pos}_2 = [0.33,\; 0.27,\; {-0.14},\; \ldots]$ \\
$15$ (pozycja $(2,1)$ -- rząd niżej) & $\mathbf{pos}_{15} = [0.30,\; 0.31,\; {-0.12},\; \ldots]$ \\
$\vdots$ & $\vdots$ \\
$196$ (pozycja $(14,14)$ -- prawy dolny róg) & $\mathbf{pos}_{196} = [{-0.45},\; {-0.38},\; 0.52,\; \ldots]$ \\
\bottomrule
\end{tabular}
\end{center}

Numeracja patchy jest \textbf{ustalona z góry} -- od lewej do prawej, z góry na dół:
\begin{center}
\begin{tabular}{cccccc}
$1$ & $2$ & $3$ & $4$ & $\cdots$ & $14$ \\
$15$ & $16$ & $17$ & $18$ & $\cdots$ & $28$ \\
$29$ & $30$ & $31$ & $32$ & $\cdots$ & $42$ \\
$\vdots$ & & & & & $\vdots$ \\
$183$ & $184$ & $185$ & $186$ & $\cdots$ & $196$ \\
\end{tabular}
\end{center}

Mapowanie jest \textbf{sztywne} i \textbf{stałe} przez cały trening:
patch w pozycji $(r, c)$ na obrazie \textit{zawsze} dostaje wektor
$\mathbf{pos}_{(r-1) \cdot 14 + c}$.
Zmienia się \textbf{zawartość} wektorów (gradient je modyfikuje),
ale \textbf{przypisanie} numer $\to$ wektor się nigdy nie zmienia.

\subsubsection{Ale skoro na starcie wektory są losowe -- skąd sieć wie, co jest obok czego?}

\textbf{Na starcie -- nie wie!} I to jest OK, bo na początku sieć produkuje śmieciowe embeddingi.
Trening to naprawia krok po kroku:

\textbf{Epoka 0} (przed treningiem):
\begin{center}
$\mathbf{pos}_1 = [0.52,\; {-0.11},\; 0.87,\; \ldots]$ -- losowe \\
$\mathbf{pos}_2 = [{-0.34},\; 0.65,\; 0.02,\; \ldots]$ -- losowe \\
$\mathbf{pos}_{15} = [0.91,\; {-0.73},\; 0.44,\; \ldots]$ -- losowe
\end{center}
Sieć nie ma pojęcia, kto jest obok kogo.
Attention jest równomierny -- każdy patch ``patrzy'' na wszystkich po równo.
Embedding $=$ losowy szum.

\medskip

\textbf{W trakcie treningu}: LeJEPA podaje dwa widoki (augmentacje) tego samego kadru
i mówi: ``embeddingi powinny być podobne''. Żeby to osiągnąć,
sieć \textit{musi} rozumieć \textbf{co gdzie jest} na obrazie -- inaczej
nie potrafi dopasować widoków.

Gradient descent zmienia wektory pozycyjne:
\begin{itemize}[leftmargin=2em]
  \item Patch 1 i Patch 2 (sąsiedzi) \textbf{wielokrotnie} widzą
        podobne rzeczy w danych (sąsiednie kawałki tej samej tęczówki).
        Loss wymusza, żeby attention między nimi działał dobrze.
        Gradient popycha $\mathbf{pos}_1$ i $\mathbf{pos}_2$
        w kierunku \textbf{podobnych} wartości.
  \item Patch 1 i Patch 196 (przeciwne rogi) rzadko widzą podobne rzeczy
        -- gradient je \textbf{odpycha}.
\end{itemize}

\textbf{Po 100 epokach}:
\begin{center}
$\mathbf{pos}_1 = [0.31,\; 0.28,\; {-0.15},\; \ldots]$ \\
$\mathbf{pos}_2 = [0.33,\; 0.27,\; {-0.14},\; \ldots]$
\quad $\leftarrow$ podobny do $\mathbf{pos}_1$ (sąsiad w rzędzie!) \\
$\mathbf{pos}_{15} = [0.30,\; 0.31,\; {-0.12},\; \ldots]$
\quad $\leftarrow$ podobny do $\mathbf{pos}_1$ (sąsiad w kolumnie!) \\
$\mathbf{pos}_{196} = [{-0.45},\; {-0.38},\; 0.52,\; \ldots]$
\quad $\leftarrow$ zupełnie inny (daleki róg)
\end{center}

Sieć \textbf{sama odkryła geometrię} siatki -- nikt jej tego nie powiedział.
To wyłoniło się z danych.

\begin{keyinsight}[Analogia: puzzle]
Embedding pozycyjny to jak \textbf{składanie puzzli}:
\begin{itemize}
  \item \textbf{Epoka 0}: wysypujesz puzzle na stół --
        na odwrocie każdego kawałka jest numer, ale nie wiesz, co pasuje do czego.
  \item \textbf{W trakcie treningu}: zauważasz wzorce --
        kawałek z fragmentem tęczówki pasuje do innego z fragmentem tęczówki
        (attention uczy się, kto jest ważny).
  \item \textbf{Po treningu}: ``ułożyłeś puzzle'' --
        numery na odwrocie nabrały sensu, bo wiesz,
        że kawałek 33 jest zawsze obok kawałka 34.
\end{itemize}
\end{keyinsight}

\subsubsection{Dlaczego dodawanie $\mathbf{e}_i + \mathbf{pos}_i$, a nie konkatenacja?}

Mamy dwie opcje łączenia informacji z patcha ($\mathbf{e}_i$) z pozycją ($\mathbf{pos}_i$):

\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{lcc}
\toprule
\textbf{Metoda} & \textbf{Wymiar wyniku} & \textbf{Koszt obliczeniowy} \\
\midrule
Dodawanie: $\mathbf{h} = \mathbf{e} + \mathbf{pos}$ & $384$ (bez zmiany) & zerowy \\
Konkatenacja: $\mathbf{h} = [\mathbf{e}\;;\;\mathbf{pos}]$ & $768$ (podwojenie!) & $2\times$ w każdej warstwie \\
\bottomrule
\end{tabular}
\end{center}

Dodawanie jest ``za darmo'' i w praktyce daje tak samo dobre wyniki
(sprawdzone empirycznie w artykule o ViT).
Sieć sama uczy się ``rozdzielać'' wymiary -- część wymiarów
w $\mathbf{h}_i$ będzie zdominowana przez $\mathbf{e}_i$ (treść wizualna),
część przez $\mathbf{pos}_i$ (pozycja), a część będzie mieszanką.

\subsubsection{Jak gradient przepływa przez dodawanie?}

Zanim zastosujemy to do ViT-a, przypomnijmy \textbf{podstawową regułę pochodnych}:

\medskip

\textbf{Reguła: pochodna sumy.}
Jeśli $f(x) = x + c$, gdzie $c$ nie zależy od $x$, to:
\begin{equation}
\frac{\partial f}{\partial x} = \frac{\partial (x + c)}{\partial x}
= \underbrace{\frac{\partial x}{\partial x}}_{= 1}
+ \underbrace{\frac{\partial c}{\partial x}}_{= 0}
= 1
\end{equation}

\textbf{Dlaczego?}
\begin{itemize}[leftmargin=2em]
  \item Pochodna $x$ po $x$ $= 1$, bo zmiana $x$ o $\Delta$ zmienia wynik dokładnie o $\Delta$.
  \item Pochodna $c$ po $x$ $= 0$, bo $c$ jest niezależne od $x$ -- zmiana $x$ nie rusza $c$.
\end{itemize}

\textbf{Przykład liczbowy.}
Niech $f(x) = x + 7$. Wtedy:
\begin{center}
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{ccc}
$x$ & $f(x) = x + 7$ & Zmiana $f$ gdy $x$ rośnie o $1$ \\
\midrule
$3$ & $10$ & -- \\
$4$ & $11$ & $+1$ \\
$5$ & $12$ & $+1$ \\
$6$ & $13$ & $+1$
\end{tabular}
\end{center}
Każdy wzrost $x$ o $1$ daje wzrost $f$ o dokładnie $1$. Stąd $\frac{\partial f}{\partial x} = 1$.
Stała $7$ nic nie zmienia -- przesuwa krzywą w górę, ale nie zmienia nachylenia.

\medskip

\textbf{Zastosowanie do ViT-a.}
Sieć liczy loss na podstawie $\mathbf{h}_i$, a $\mathbf{h}_i = \mathbf{e}_i + \mathbf{pos}_i$.
Dla dowolnego wymiaru $k$ (od $1$ do $384$):
\begin{equation}
h_i[k] = \underbrace{e_i[k]}_{\text{nasza ``$x$''}} + \underbrace{\text{pos}_i[k]}_{\text{nasze ``$c$'' (inna zmienna)}}
\end{equation}

Stosujemy regułę pochodnej sumy:
\begin{equation}
\frac{\partial h_i[k]}{\partial e_i[k]}
= \underbrace{\frac{\partial\, e_i[k]}{\partial\, e_i[k]}}_{=1}
+ \underbrace{\frac{\partial\, \text{pos}_i[k]}{\partial\, e_i[k]}}_{=0,\text{ bo } \text{pos}_i \text{ nie zależy od } e_i}
= 1
\end{equation}

Analogicznie:
\begin{equation}
\frac{\partial h_i[k]}{\partial\, \text{pos}_i[k]}
= \underbrace{\frac{\partial\, e_i[k]}{\partial\, \text{pos}_i[k]}}_{=0}
+ \underbrace{\frac{\partial\, \text{pos}_i[k]}{\partial\, \text{pos}_i[k]}}_{=1}
= 1
\end{equation}

Ponieważ to zachodzi dla \textit{każdego} wymiaru $k$, zapisujemy wektorowo:
\begin{equation}
\frac{\partial \mathbf{h}_i}{\partial \mathbf{e}_i} = \mathbf{1}, \qquad
\frac{\partial \mathbf{h}_i}{\partial \mathbf{pos}_i} = \mathbf{1}
\end{equation}

Więc przez chain rule \textbf{oba dostają ten sam gradient}:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \mathbf{e}_i}
= \frac{\partial \mathcal{L}}{\partial \mathbf{h}_i} \cdot 1
= \frac{\partial \mathcal{L}}{\partial \mathbf{h}_i}, \qquad
\frac{\partial \mathcal{L}}{\partial \mathbf{pos}_i}
= \frac{\partial \mathcal{L}}{\partial \mathbf{h}_i} \cdot 1
= \frac{\partial \mathcal{L}}{\partial \mathbf{h}_i}
\end{equation}

Ale potem gradient \textbf{rozchodzi się} w dwie różne gałęzie:

\begin{center}
\begin{tabular}{cp{10.5cm}}
\toprule
\textbf{Gałąź} & \textbf{Co się dzieje z gradientem?} \\
\midrule
$\mathbf{pos}_i$ &
To jest \textbf{parametr sieci} (liść w grafie obliczeń).
Gradient \textbf{bezpośrednio} go aktualizuje:
$\mathbf{pos}_i \leftarrow \mathbf{pos}_i - \eta \cdot \frac{\partial \mathcal{L}}{\partial \mathbf{pos}_i}$ \\
\midrule
$\mathbf{e}_i$ &
To \textbf{nie jest} parametr -- to wynik obliczenia
$\mathbf{e}_i = \mathbf{W}_{\text{patch}} \cdot \mathbf{p}_i + \mathbf{b}_{\text{patch}}$.
Gradient płynie \textbf{dalej wstecz}:
$\frac{\partial \mathcal{L}}{\partial \mathbf{W}_{\text{patch}}}
= \frac{\partial \mathcal{L}}{\partial \mathbf{e}_i} \cdot \mathbf{p}_i^\top$
$\Rightarrow$ aktualizuje $\mathbf{W}_{\text{patch}}$ i $\mathbf{b}_{\text{patch}}$. \\
\bottomrule
\end{tabular}
\end{center}

Schematycznie -- gradient rozchodzi się w dwie gałęzie.

\textbf{Uwaga:} $\mathcal{L}$ to \textbf{pełna funkcja straty LeJEPA} (szczegółowo opisana
w Sekcji~\ref{sec:fulloss}, rów.~\ref{eq:lejepa}):
\[
\mathcal{L} = \lambda \cdot \text{SIGReg}(\mathbf{z}) + (1-\lambda) \cdot \|\text{centroid} - \mathbf{z}_v\|^2
\]
Pierwszy człon wymusza rozkład $\mathcal{N}(\mathbf{0}, \mathbf{I})$,
drugi -- żeby widoki tego samego obrazu miały podobne embeddingi.

\begin{center}
$\mathcal{L}$ (loss) \\[0.3em]
$\downarrow$ \small{gradient $\frac{\partial \mathcal{L}}{\partial \mathbf{h}_i}$} \\[0.3em]
$\mathbf{h}_i = \mathbf{e}_i + \mathbf{pos}_i$ \\[0.5em]
\begin{tabular}{c@{\hspace{2cm}}c}
$\swarrow$ \textbf{Gałąź lewa} & $\searrow$ \textbf{Gałąź prawa} \\[0.5em]
$\mathbf{e}_i$ & $\mathbf{pos}_i$ \\
\small{(to nie jest parametr!} & \small{(to JEST parametr)} \\
\small{gradient płynie dalej $\downarrow$)} & \small{$\Downarrow$ aktualizuj bezpośrednio:} \\[0.3em]
$\downarrow$ & $\mathbf{pos}_i \leftarrow \mathbf{pos}_i - \eta \cdot \frac{\partial \mathcal{L}}{\partial \mathbf{h}_i}$ \\[0.8em]
$\mathbf{W}_{\text{patch}} \cdot \mathbf{p}_i + \mathbf{b}_{\text{patch}}$ & \\[0.3em]
$\swarrow$ \hspace{1cm} $\searrow$ & \\[0.3em]
$\mathbf{W}_{\text{patch}} \leftarrow$ \small{aktual.}
\hspace{0.5cm} $\mathbf{b}_{\text{patch}} \leftarrow$ \small{aktual.} & \\
\end{tabular}
\end{center}

\begin{keyinsight}[Dlaczego uczą się różnych rzeczy, skoro gradient jest ten sam?]
Bo mają \textbf{różne źródła}:
\begin{itemize}
  \item $\mathbf{e}_i$ zależy od \textbf{pikseli patcha} $\mathbf{p}_i$
        -- więc $\mathbf{W}_{\text{patch}}$ uczy się wyciągać \textit{cechy wizualne}
        (krawędzie, kolory, tekstury).
  \item $\mathbf{pos}_i$ zależy \textbf{tylko od numeru pozycji} -- nie widzi pikseli,
        więc może kodować wyłącznie \textit{``gdzie na obrazie''}.
\end{itemize}
Ten sam gradient, ale dwa \textbf{różne} cele uczenia -- bo informacja
o treści (piksele) i o pozycji (numer) są \textit{rozdzielone architekturalnie}.
\end{keyinsight}

\end{tcolorbox}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/vit_positions.pdf}
\caption{\textbf{Lewo}: Wizualizacja jednego wymiaru embeddingu pozycyjnego na siatce $14 \times 14$.
Bliskie pozycje mają podobne wartości — sieć ``nauczyła się'' geometrii obrazu.
\textbf{Prawo}: Podobieństwo cosinusowe między wybranymi pozycjami —
bliskie pozycje (zielony) vs.\ dalekie (czerwony).}
\label{fig:positions}
\end{figure}

\subsection{Krok 4: Token [CLS]}

Przed wejściem do Transformera dodajemy \textbf{specjalny token} [CLS]
(class token) — dodatkowy uczony wektor $\mathbf{h}_{\text{CLS}}^{(0)} \in \mathbb{R}^{384}$.

Teraz mamy $196 + 1 = \mathbf{197}$ tokenów, każdy o wymiarze $384$.

\begin{tcolorbox}[colback=blue!3, colframe=blue!60!black, breakable,
  title=\textbf{Co to jest token [CLS] i po co go dodajemy?}]

\textbf{Problem}: mamy 196 patchy, każdy z embeddingiem w $\mathbb{R}^{384}$.
Potrzebujemy \textbf{jednego} wektora, który opisuje \textbf{cały obraz}.
Jak go uzyskać?

\textbf{Naiwne podejście}: uśrednić wszystkie 196 patchy: $\bar{\mathbf{h}} = \frac{1}{196}\sum_{i=1}^{196} \mathbf{h}_i$.
Ale to traci informację -- patch z narzędziem dostaje taką samą wagę jak patch z tłem.

\textbf{Rozwiązanie}: dodajemy \textbf{sztuczny token nr 0} -- [CLS] (od ``classification''):

\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{cl}
\toprule
\textbf{Token} & \textbf{Czym jest} \\
\midrule
$\mathbf{h}_1, \ldots, \mathbf{h}_{196}$ & Embeddingi 196 patchy (fragmenty obrazu) \\
$\mathbf{h}_{\text{CLS}}$ & \textbf{Sztuczny token} -- nie odpowiada żadnemu patchowi! \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Kluczowe właściwości [CLS]:}

\begin{enumerate}[leftmargin=2em]
\item \textbf{Nie ma „swoich'' pikseli} -- nie pochodzi z żadnego fragmentu obrazu.
      Na starcie to po prostu \textbf{uczony wektor} (384 losowych liczb, które się zmieniają podczas treningu).

\item \textbf{Uczestniczy w attention} jak każdy inny token --
      ma swoje $\mathbf{Q}_{\text{CLS}}, \mathbf{K}_{\text{CLS}}, \mathbf{V}_{\text{CLS}}$.

\item \textbf{[CLS] pyta, patche odpowiadają}:
      w mechanizmie attention [CLS] liczy $\mathbf{Q}_{\text{CLS}} \cdot \mathbf{K}_j$
      dla każdego patcha $j$. Patche z ważną informacją (narzędzie, tęczówka)
      dostaną \textbf{duże wagi} $\alpha_{\text{CLS},j}$, a tło -- małe.

\item \textbf{Po 12 warstwach} [CLS] ``zebrał'' informację ze wszystkich patchy,
      ale \textbf{ważoną} przez attention -- ważniejsze patche miały większy wpływ.
\end{enumerate}

\textbf{Analogia}: [CLS] to \textbf{reporter} na konferencji prasowej.
196 patchy to mówcy. Reporter (CLS) słucha wszystkich, ale notuje głównie
to, co mówią najważniejsi (narzędzie, tęczówka). Na końcu reporter pisze
\textbf{jedno podsumowanie} (embedding $\mathbf{z}$) -- to jest wynik ViT.

\medskip

\textbf{Ale po co [CLS] w ogóle jest? Co wnosi?}

[CLS] \textbf{nic nie dodaje} do obrazu -- to działa \textbf{odwrotnie}!
To patche modyfikują [CLS], nie [CLS] modyfikuje patche.

\textbf{Problem}: ViT na wyjściu daje \textbf{197 wektorów}
(196 patchy + [CLS]). Ale do klasyfikacji czy SIGReg potrzebujemy
\textbf{jednego} wektora opisującego cały obraz.
Jak skompresować 197 wektorów do 1?

\begin{center}
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{lcp{6cm}}
\toprule
\textbf{Metoda} & \textbf{Wynik} & \textbf{Problem} \\
\midrule
Uśrednij 196 patchy & $\bar{\mathbf{h}} = \frac{1}{196}\sum_i \mathbf{h}_i$
  & Każdy patch ma wagę $0.5\%$ -- tło = narzędzie. Tracimy informację o ważności. \\
Weź losowy patch & $\mathbf{h}_{42}$
  & Opisuje tylko $\frac{1}{196}$ obrazu. Reszta jest ignorowana. \\
\textbf{[CLS] + attention} & $\mathbf{h}_{\text{CLS}}^{(12)}$
  & \textbf{Brak!} Sieć sama uczy się, ile uwagi dać każdemu patchowi. \\
\bottomrule
\end{tabular}
\end{center}

[CLS] to \textbf{``pusty kubek''}, który przez 12 warstw attention
\textbf{napełnia się} najważniejszą informacją z patchy:

\begin{center}
\begin{tabular}{lcl}
Warstwa 0: & $\mathbf{h}_{\text{CLS}}^{(0)}$ & = losowy wektor (pusty kubek) \\
Warstwa 1: & $\mathbf{h}_{\text{CLS}}^{(1)}$ & = kubek + trochę info z patchy \\
Warstwa 6: & $\mathbf{h}_{\text{CLS}}^{(6)}$ & = kubek + dużo info (krawędzie, kolory) \\
Warstwa 12: & $\mathbf{h}_{\text{CLS}}^{(12)}$ & = \textbf{pełne podsumowanie obrazu} $= \mathbf{z}$
\end{tabular}
\end{center}

\textbf{Kluczowy punkt}: [CLS] \textbf{nie zmienia} patchy -- to patche zmieniają [CLS].
Każda warstwa attention pozwala [CLS] ``wchłonąć'' więcej informacji
z ważnych regionów obrazu.

\medskip

\textbf{Co konkretnie zawiera $\mathbf{z} \in \mathbb{R}^{384}$?}

To \textbf{384 liczb} opisujących \textbf{semantykę} całego obrazu.
Ale te liczby \textbf{nie mają ludzkich nazw} -- nie ma
``cecha nr 47 = kolor tęczówki'' ani ``cecha nr 200 = typ narzędzia''.
To \textbf{abstrakcyjne cechy}, które sieć sama wymyśliła podczas treningu.

Czego \textbf{nie wiemy}: co oznacza każda z 384 liczb z osobna.

Czego \textbf{wiemy}: dwa obrazy o \textbf{podobnej semantyce}
(np.\ ta sama faza operacji) mają \textbf{bliskie} wektory $\mathbf{z}$:
\[
\text{podobne obrazy} \;\Rightarrow\; \|\mathbf{z}_A - \mathbf{z}_B\| \text{ jest \textbf{mały}}
\]
\[
\text{różne obrazy} \;\Rightarrow\; \|\mathbf{z}_A - \mathbf{z}_B\| \text{ jest \textbf{duży}}
\]

To dlatego potem działa \textbf{linear probe} -- wystarczy prosta linia
(granica decyzyjna) w $\mathbb{R}^{384}$, żeby oddzielić klasy,
bo \textbf{sieć już ułożyła} podobne obrazy blisko siebie.

\medskip

\textbf{Przykład} (uproszczony do 3 wymiarów zamiast 384):
\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{lc}
\toprule
\textbf{Obraz} & $\mathbf{z}$ (embedding) \\
\midrule
Incision, klatka 1 & $(0.8, \; -0.3, \; 1.2)$ \\
Incision, klatka 2 & $(0.7, \; -0.2, \; 1.1)$ \quad $\leftarrow$ bliski! \\
Phaco, klatka 1 & $(-1.5, \; 0.9, \; -0.4)$ \quad $\leftarrow$ daleki! \\
\bottomrule
\end{tabular}
\end{center}

Dwa kadry incision mają prawie identyczne $\mathbf{z}$,
a phaco jest daleko -- mimo że \textbf{piksele} mogą wyglądać zupełnie inaczej
(inny kąt kamery, inne oświetlenie). To jest siła embeddingów:
\textbf{kodują znaczenie, nie wygląd}.

\medskip

\textbf{Czy [CLS] ma własne macierze $\mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V$?}

\textbf{Nie!} [CLS] używa \textbf{tych samych} macierzy co wszystkie 196 patchy.
To jest cała idea self-attention -- jedna wspólna trójka macierzy
$(\mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V)$ obsługuje \textbf{wszystkich} 197 tokenów:

\[
\mathbf{Q}_{42} = \mathbf{W}_Q \cdot \mathbf{h}_{42}, \qquad
\mathbf{Q}_{\text{CLS}} = \mathbf{W}_Q \cdot \mathbf{h}_{\text{CLS}}
\qquad \longleftarrow \text{ta sama } \mathbf{W}_Q\text{!}
\]

Różnica nie jest w macierzach, tylko we \textbf{wejściu}:
$\mathbf{h}_{42}$ zawiera cechy patcha nr 42 (piksele),
a $\mathbf{h}_{\text{CLS}}$ zawiera dotychczas zebraną informację o obrazie.
Po mnożeniu przez tę samą $\mathbf{W}_Q$ dostają \textbf{różne} pytania Q,
bo wejścia są różne.

\end{tcolorbox}

\subsection{Krok 5: Bloki Transformera (encoder)}

Serce ViT to $L = 12$ identycznych bloków (w ViT-Small).
Każdy blok składa się z dwóch części:

\subsubsection{(a) Self-Attention: ``kto na kogo patrzy''}

Każdy token tworzy trzy wektory przez mnożenie macierzowe:

\begin{equation}
\mathbf{Q}_i = \mathbf{W}_Q \mathbf{h}_i, \quad
\mathbf{K}_i = \mathbf{W}_K \mathbf{h}_i, \quad
\mathbf{V}_i = \mathbf{W}_V \mathbf{h}_i
\end{equation}

\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{cl}
\toprule
\textbf{Wektor} & \textbf{Rola} \\
\midrule
$\mathbf{Q}_i$ (Query) & ``Czego szukam?'' — pytanie tokenu $i$ \\
$\mathbf{K}_j$ (Key) & ``Co mam do zaoferowania?'' — opis tokenu $j$ \\
$\mathbf{V}_j$ (Value) & ``Jaka jest moja treść?'' — informacja tokenu $j$ \\
\bottomrule
\end{tabular}
\end{center}

% ============ SZCZEGÓŁOWE WYJAŚNIENIE MACIERZY W_Q, W_K, W_V ============
\begin{tcolorbox}[colback=blue!3, colframe=blue!60!black, breakable,
  title=\textbf{Co to są macierze $\mathbf{W}_Q$, $\mathbf{W}_K$, $\mathbf{W}_V$?}]

\textbf{Jeden token} $\mathbf{h}_i \in \mathbb{R}^{384}$ zawiera \textit{wszystko} o patchu $i$:
kolor, teksturę, kształt, pozycję\ldots To jest 384-wymiarowy opis ``wszystkiego naraz''.

Problem: żeby mechanizm attention mógł działać, potrzebujemy \textbf{trzech różnych perspektyw}
na ten sam token -- pytanie, odpowiedź, treść. Gdybyśmy używali $\mathbf{h}_i$ bezpośrednio
do wszystkich trzech ról, nie moglibyśmy oddzielić ``czego szukam'' od ``co oferuję''.

\textbf{Rozwiązanie}: mnożymy $\mathbf{h}_i$ przez trzy \textit{różne} macierze,
które ``wyciągają'' różne aspekty informacji:

\vspace{6pt}

\textbf{Wymiary macierzy} (ViT-Small z 6 głowicami, $d = 384/6 = 64$ na głowicę):

\begin{center}
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{lccl}
\toprule
\textbf{Macierz} & \textbf{Wymiar} & \textbf{Operacja} & \textbf{Efekt} \\
\midrule
$\mathbf{W}_Q$ & $64 \times 384$ & $\mathbf{W}_Q \cdot \mathbf{h}_i = \mathbf{Q}_i$ & $\mathbb{R}^{384} \to \mathbb{R}^{64}$ \\
$\mathbf{W}_K$ & $64 \times 384$ & $\mathbf{W}_K \cdot \mathbf{h}_i = \mathbf{K}_i$ & $\mathbb{R}^{384} \to \mathbb{R}^{64}$ \\
$\mathbf{W}_V$ & $64 \times 384$ & $\mathbf{W}_V \cdot \mathbf{h}_i = \mathbf{V}_i$ & $\mathbb{R}^{384} \to \mathbb{R}^{64}$ \\
\bottomrule
\end{tabular}
\end{center}

Każda macierz \textbf{kompresuje} 384 wymiarów do 64 wymiarów,
ale każda robi to \textbf{inaczej} -- wyciągając inne cechy!

\end{tcolorbox}

% ============ DLACZEGO W_Q "PYTA", A W_K "ODPOWIADA"? ============
\begin{tcolorbox}[breakable, colback=red!3, colframe=red!60!black,
  title=\textbf{Ale dlaczego $\mathbf{W}_Q$ ``pyta'', a $\mathbf{W}_K$ ``odpowiada''?}]

To kluczowe pytanie! Odpowiedź jest zaskakująca:

\textbf{Nic w samych macierzach nie sprawia, że jedna jest ``pytająca'' a druga ``odpowiadająca''.}
Na początku treningu wszystkie trzy ($\mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V$)
to \textbf{losowe macierze} -- identyczne co do natury.

To, co nadaje im rolę, to ich \textbf{pozycja w obliczeniach} (architektura):

\begin{center}
\begin{tabular}{l}
$\underbrace{\mathbf{Q}_i \cdot \mathbf{K}_j}_{\text{iloczyn skalarny}}
\;\to\; \alpha_{ij} \;\text{(waga attention: \textbf{KTO} jest ważny)}$ \\[10pt]
$\underbrace{\sum_j \alpha_{ij} \cdot \mathbf{V}_j}_{\text{ważona suma}}
\;\to\; \text{wynik (\textbf{TREŚĆ} do przekazania dalej)}$
\end{tabular}
\end{center}

gdzie:
\begin{itemize}[leftmargin=2em]
\item $i$ = token, który \textbf{pyta} (``ja'') -- np.\ patch nr 42 (fragment narzędzia),
\item $j$ = token, który jest \textbf{odpytywany} (``inni'') -- przebiega po \textit{wszystkich}
      197 tokenach ($j = 1, 2, \ldots, 197$),
\item $\alpha_{ij}$ = ile uwagi token $i$ poświęca tokenowi $j$
      (po softmax: $\sum_j \alpha_{ij} = 1$),
\item $\mathbf{V}_j$ = treść, którą token $j$ ``przekazuje'', gdy zostanie wybrany.
\end{itemize}
Innymi słowy: dla \textbf{każdego} tokenu $i$, obliczamy jego podobieństwo
do \textbf{każdego} tokenu $j$ (włącznie z samym sobą, $j = i$).

\textbf{Dlaczego to wystarczy?} Bo gradient ``widzi'' różne ścieżki:

\begin{itemize}[leftmargin=2em]
\item \textbf{$\mathbf{W}_Q$ i $\mathbf{W}_K$} uczą się ``współpracować'' --
      bo ich wyniki ($\mathbf{Q}$ i $\mathbf{K}$) są mnożone skalarnie ($\mathbf{Q} \cdot \mathbf{K}$).
      Gradient wymusza: duży iloczyn skalarny dla \textit{ważnych} par tokenów,
      mały dla \textit{nieważnych}.

\item \textbf{$\mathbf{W}_V$} uczy się czegoś \textbf{zupełnie innego} --
      bo jej wynik nie wpływa na to, \textit{kto} jest ważny,
      tylko \textit{jaką informację} przekazać, gdy token zostanie ``wybrany''.
\end{itemize}

\medskip

\textbf{Analogia}: Trzy osoby na budowie -- na początku identyczne.
Ale jeden dostał \textbf{łopatę} (= pozycja Q·K w równaniu),
drugi \textbf{wiertło} (= pozycja K·Q),
trzeci \textbf{pędzel} (= pozycja V w sumie ważonej).
\textbf{Narzędzie} (pozycja w architekturze) determinuje ich specjalizację!

\medskip

\textbf{Dowód}: gdybyśmy zamienili $\mathbf{W}_Q$ z $\mathbf{W}_K$
(ale nie zmienili architektury), sieć po treningu dałaby
\textbf{identyczne wyniki} -- bo $\mathbf{Q}_i \cdot \mathbf{K}_j = \mathbf{K}_j \cdot \mathbf{Q}_i$
(iloczyn skalarny jest przemienny).
Natomiast zamiana $\mathbf{W}_Q$ z $\mathbf{W}_V$ \textbf{zepsułaby} sieć,
bo $\mathbf{V}$ jest używane \textit{inaczej} w obliczeniach.

\end{tcolorbox}

% ============ MULTI-HEAD: DLACZEGO 6 GŁOWIC ============
\begin{tcolorbox}[breakable, colback=green!3, colframe=green!60!black,
  title=\textbf{Dlaczego 6 głowic (Multi-Head Attention)?}]

Jedna głowica (jedna trójka $\mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V$) liczy Q, K i V
-- ale ponieważ ma \textbf{jeden} zestaw macierzy, potrafi wykrywać
\textbf{tylko jeden typ relacji} między tokenami.
Np.\ jeśli ta głowica nauczyła się, że $\mathbf{Q} \cdot \mathbf{K}$ daje duży wynik
dla patchy o podobnym \textit{kolorze}, to nie potrafi \textbf{jednocześnie}
szukać patchy \textit{bliskich przestrzennie} -- bo ma tylko jedną parę
$(\mathbf{W}_Q, \mathbf{W}_K)$.

\textbf{Rozwiązanie: 6 niezależnych głowic}, każda z własnym zestawem macierzy:

\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{clc}
\toprule
\textbf{Głowica} & \textbf{Czego się uczy szukać (przykład)} & \textbf{Macierze} \\
\midrule
1 & ``Kto ma podobną \textit{teksturę}?'' & $\mathbf{W}_Q^{(1)}, \mathbf{W}_K^{(1)}, \mathbf{W}_V^{(1)}$ \\
2 & ``Kto jest \textit{blisko} przestrzennie?'' & $\mathbf{W}_Q^{(2)}, \mathbf{W}_K^{(2)}, \mathbf{W}_V^{(2)}$ \\
3 & ``Kto ma ten sam \textit{kolor}?'' & $\mathbf{W}_Q^{(3)}, \mathbf{W}_K^{(3)}, \mathbf{W}_V^{(3)}$ \\
4 & ``Gdzie jest \textit{krawędź} narzędzia?'' & $\mathbf{W}_Q^{(4)}, \mathbf{W}_K^{(4)}, \mathbf{W}_V^{(4)}$ \\
5 & ``Kto wygląda jak \textit{tęczówka}?'' & $\mathbf{W}_Q^{(5)}, \mathbf{W}_K^{(5)}, \mathbf{W}_V^{(5)}$ \\
6 & ``Kto jest w \textit{tle} (nieistotny)?'' & $\mathbf{W}_Q^{(6)}, \mathbf{W}_K^{(6)}, \mathbf{W}_V^{(6)}$ \\
\bottomrule
\end{tabular}
\end{center}

Wyniki 6 głowic są \textbf{sklejane} (konkatenacja) i przepuszczane przez jedną macierz wyjściową:
\begin{equation}
\text{MultiHead}(\mathbf{h}) = \mathbf{W}_O \cdot
\underbrace{\text{Concat}\!\left(
\text{head}_1, \;
\text{head}_2, \;
\ldots, \;
\text{head}_6
\right)}_{6 \times 64 = 384}
\end{equation}
gdzie $\mathbf{W}_O \in \mathbb{R}^{384 \times 384}$ -- jeszcze jedna uczona macierz.

\medskip

\textbf{Dlaczego akurat 6?} Bo $384 / 6 = 64$, a $64$ wymiary na głowicę to sprawdzona wartość:

\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{lccc}
\toprule
\textbf{Wariant ViT} & \textbf{$d_{\text{model}}$} & \textbf{Głowice} & \textbf{$d_{\text{head}}$} \\
\midrule
ViT-Tiny & 192 & 3 & $192/3 = 64$ \\
\textbf{ViT-Small} & \textbf{384} & \textbf{6} & $\mathbf{384/6 = 64}$ \\
ViT-Base & 768 & 12 & $768/12 = 64$ \\
ViT-Large & 1024 & 16 & $1024/16 = 64$ \\
\bottomrule
\end{tabular}
\end{center}

Zawsze $\mathbf{d_{\text{head}} = 64}$ -- to \textbf{stała}!
Większe modele mają więcej głowic (więcej ``pytań'' równolegle),
a nie większe głowice.

\medskip

\textbf{Ale dlaczego 6 głowic nie uczą się tego samego?}

Nic nie \textit{gwarantuje}, że się nie nauczą -- ale w praktyce uczą się
\textbf{różnych} relacji z dwóch powodów:

\begin{enumerate}[leftmargin=2em]
\item \textbf{Losowa inicjalizacja}: każda głowica startuje z \textit{innymi} losowymi macierzami.
      Gradient descent to optymalizacja \textbf{lokalna} -- różne punkty startowe
      prowadzą do różnych rozwiązań (lokalnych minimów).

\item \textbf{Gradient ``nagradza'' za nowe informacje}:
      jeśli głowica 1 już nauczyła się wykrywać \textit{kolor},
      to loss jest częściowo zaspokojony w tym kierunku.
      Gradient dla pozostałych głowic popycha je w stronę
      \textbf{innych} wzorców (tekstura, pozycja\ldots),
      bo \textit{tam} jest jeszcze pole do poprawy lossu.
\end{enumerate}

\textbf{Analogia}: zespół 6 pracowników z losowo przydzielonymi zadaniami.
Jeśli pracownik 1 już robi zadanie A dobrze, szef (gradient) nie nagradza
pracownika 2 za \textit{powtórzenie} A -- nagroda idzie do tego, kto weźmie się za B.

\medskip

\textbf{Uwaga}: w praktyce głowice \textbf{nie zawsze} uczą się idealnie
różnych rzeczy -- część jest redundantna.
Badania (Michel et al., 2019) pokazują, że w wytrenowanym Transformerze
można \textbf{usunąć} nawet 40\% głowic bez dużej utraty jakości (\textit{head pruning}).
To potwierdza, że pewna nadmiarowość jest normalna.

\end{tcolorbox}

% ============ MINI-PRZYKŁAD ============
\begin{tcolorbox}[breakable, colback=yellow!5, colframe=orange!70!black,
  title=\textbf{Mini-przykład: $\mathbf{h}_i \in \mathbb{R}^{4} \to$ Q, K, V $\in \mathbb{R}^{2}$}]

Niech token $\mathbf{h}_i$ ma 4 cechy: $\mathbf{h}_i = \begin{bmatrix} 0.5 \\ 1.0 \\ -0.3 \\ 0.8 \end{bmatrix}$

Trzy macierze (uczone podczas treningu, tu uproszczone):

\[
\mathbf{W}_Q = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \end{bmatrix}, \quad
\mathbf{W}_K = \begin{bmatrix} 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix}, \quad
\mathbf{W}_V = \begin{bmatrix} 0.5 & 0.5 & 0 & 0 \\ 0 & 0 & 0.5 & 0.5 \end{bmatrix}
\]

\textbf{Obliczamy:}
\begin{align}
\mathbf{Q}_i &= \mathbf{W}_Q \cdot \mathbf{h}_i
= \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \end{bmatrix}
\begin{bmatrix} 0.5 \\ 1.0 \\ -0.3 \\ 0.8 \end{bmatrix}
= \begin{bmatrix} 0.5 \\ 1.0 \end{bmatrix}
\quad \text{\small(wyciąga cechy 1--2)} \nonumber\\[6pt]
\mathbf{K}_i &= \mathbf{W}_K \cdot \mathbf{h}_i
= \begin{bmatrix} 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix}
\begin{bmatrix} 0.5 \\ 1.0 \\ -0.3 \\ 0.8 \end{bmatrix}
= \begin{bmatrix} -0.3 \\ 0.8 \end{bmatrix}
\quad \text{\small(wyciąga cechy 3--4)} \nonumber\\[6pt]
\mathbf{V}_i &= \mathbf{W}_V \cdot \mathbf{h}_i
= \begin{bmatrix} 0.5 & 0.5 & 0 & 0 \\ 0 & 0 & 0.5 & 0.5 \end{bmatrix}
\begin{bmatrix} 0.5 \\ 1.0 \\ -0.3 \\ 0.8 \end{bmatrix}
= \begin{bmatrix} 0.75 \\ 0.25 \end{bmatrix}
\quad \text{\small(uśrednia pary cech)} \nonumber
\end{align}

\textbf{Obserwacja}: ten sam token $\mathbf{h}_i$ dał \textbf{trzy różne} wektory!
\begin{itemize}[leftmargin=2em]
\item $\mathbf{Q}_i = (0.5, \; 1.0)$ -- ``to czego szukam'' (np. cechy krawędziowe),
\item $\mathbf{K}_i = (-0.3, \; 0.8)$ -- ``to co oferuję'' (np. cechy kolorowe),
\item $\mathbf{V}_i = (0.75, \; 0.25)$ -- ``moja faktyczna treść'' (uśrednione cechy).
\end{itemize}

\end{tcolorbox}

% ============ ANALOGIA ============
\begin{tcolorbox}[breakable, colback=green!3, colframe=green!60!black,
  title=\textbf{Analogia: trzy różne okulary}]

Wyobraź sobie, że $\mathbf{h}_i$ to \textbf{pełny opis} osoby (wzrost, waga, kolor oczu,
wykształcenie, dochód, hobby\ldots).

\begin{itemize}[leftmargin=2em]
\item $\mathbf{W}_Q$ = ``okulary pytające'' -- pokazują cechy, których ta osoba \textit{szuka}
      u innych (np. wykształcenie, hobby).
\item $\mathbf{W}_K$ = ``okulary reklamowe'' -- pokazują cechy, które ta osoba \textit{oferuje}
      (np. zawód, umiejętności).
\item $\mathbf{W}_V$ = ``okulary treściowe'' -- pokazują \textit{konkretną informację},
      którą ta osoba przekaże, gdy ktoś ją ``wybierze''.
\end{itemize}

\textbf{Kluczowy punkt}: wszystkie trzy macierze $\mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V$
są \textbf{parametrami uczonymi} -- na początku treningu zawierają losowe wartości.
Sieć sama uczy się, jakie ``okulary'' są najlepsze!

\end{tcolorbox}

% ============ PEŁNA PROCEDURA Q*K ============
\begin{tcolorbox}[breakable, colback=blue!3, colframe=blue!60!black,
  title=\textbf{Pełna procedura: od $\mathbf{h}$ do wag attention (mini-przykład z 3 tokenami)}]

Mamy 3 tokeny z $d=2$ (wymiar wektora Q i K \textbf{wewnątrz jednej głowicy},
w prawdziwym ViT-Small $d = 384/6 = 64$; tu używamy $d=2$, żeby można było liczyć ręcznie).
Po projekcji przez $\mathbf{W}_Q$ i $\mathbf{W}_K$:

\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{lcc}
\toprule
\textbf{Token} & $\mathbf{Q}_i$ & $\mathbf{K}_i$ \\
\midrule
$i=1$ (narzędzie) & $(1.0, \; 0.2)$ & $(0.1, \; 0.9)$ \\
$i=2$ (tęczówka) & $(0.3, \; 0.8)$ & $(0.7, \; 0.5)$ \\
$i=3$ (tło) & $(0.1, \; 0.1)$ & $(0.2, \; 0.3)$ \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Krok 1}: Macierz podobieństw $S_{ij} = \mathbf{Q}_i \cdot \mathbf{K}_j$:

\[
S = \begin{bmatrix}
Q_1 \cdot K_1 & Q_1 \cdot K_2 & Q_1 \cdot K_3 \\
Q_2 \cdot K_1 & Q_2 \cdot K_2 & Q_2 \cdot K_3 \\
Q_3 \cdot K_1 & Q_3 \cdot K_2 & Q_3 \cdot K_3
\end{bmatrix}
= \begin{bmatrix}
0.28 & 0.80 & 0.26 \\
0.75 & 0.61 & 0.30 \\
0.10 & 0.12 & 0.05
\end{bmatrix}
\]

Np. $S_{12} = Q_1 \cdot K_2 = 1.0 \times 0.7 + 0.2 \times 0.5 = 0.80$ -- \textbf{wysoka wartość!}
Token 1 (narzędzie) ``szuka'' czegoś, co token 2 (tęczówka) ``oferuje''.

\textbf{Krok 2}: Dzielimy przez $\sqrt{d} = \sqrt{2} \approx 1.41$:
\[
\tilde{S} = S / \sqrt{2} \approx \begin{bmatrix} 0.20 & 0.57 & 0.18 \\ 0.53 & 0.43 & 0.21 \\ 0.07 & 0.08 & 0.04 \end{bmatrix}
\]

\textbf{Krok 3}: Softmax (wiersz po wierszu).

\textbf{Wzór na softmax} -- zamienia dowolne liczby rzeczywiste na prawdopodobieństwa (nieujemne, sumujące się do 1):

\begin{equation}
\text{softmax}(x_1, x_2, \ldots, x_n)_k = \frac{e^{x_k}}{\sum_{m=1}^{n} e^{x_m}}
\end{equation}

\textbf{Co robi?} Bierze wektor dowolnych wartości i zamienia go na rozkład prawdopodobieństwa:
\begin{itemize}[leftmargin=2em]
\item duże $x_k$ $\to$ duże $e^{x_k}$ $\to$ duży udział (blisko 1),
\item małe $x_k$ $\to$ małe $e^{x_k}$ $\to$ mały udział (blisko 0),
\item zawsze: $\sum_k \text{softmax}(x)_k = 1$ (suma wag = 100\%).
\end{itemize}

\textbf{Przykład na wierszu 1} macierzy $\tilde{S}$: \; $(\tilde{s}_{11},\; \tilde{s}_{12},\; \tilde{s}_{13}) = (0.20,\; 0.57,\; 0.18)$

\[
e^{0.20} = 1.22, \quad e^{0.57} = 1.77, \quad e^{0.18} = 1.20
\quad \Rightarrow \quad \text{suma} = 4.19
\]
\[
\alpha_{11} = \frac{1.22}{4.19} = 0.29, \quad
\alpha_{12} = \frac{1.77}{4.19} = \mathbf{0.42}, \quad
\alpha_{13} = \frac{1.20}{4.19} = 0.29
\]

Sprawdzenie: $0.29 + 0.42 + 0.29 = 1.00$ \; \checkmark

Stosujemy softmax do każdego wiersza:
\[
\alpha = \text{softmax}(\tilde{S}) \approx \begin{bmatrix}
0.29 & \mathbf{0.42} & 0.29 \\
\mathbf{0.39} & 0.35 & 0.26 \\
0.34 & 0.34 & 0.33
\end{bmatrix}
\]

\textbf{Interpretacja}: Token 1 zwraca $42\%$ uwagi na token 2, bo
$\mathbf{W}_Q$ wyciągnęła z narzędzia pytanie ``gdzie tęczówka?'',
a $\mathbf{W}_K$ wyciągnęła z tęczówki odpowiedź ``tutaj jestem!''.

Token 3 (tło) nie szuka niczego konkretnego $\rightarrow$ uwaga prawie równomierna (33\%).
\end{tcolorbox}

% ============ WIZUALIZACJA: CO W ROBI ============
\begin{tcolorbox}[breakable, colback=gray!5, colframe=gray!60!black,
  title=\textbf{Wizualizacja: jak $\mathbf{W}$ transformuje przestrzeń}]

Mnożenie wektora przez macierz $\mathbf{W}$ to \textbf{transformacja liniowa} --
obrót, rozciągnięcie i/lub kompresja przestrzeni:

\begin{center}
\begin{tabular}{ccc}
$\mathbf{h}_i \in \mathbb{R}^{384}$ & $\xrightarrow{\quad \mathbf{W}_Q \quad}$ & $\mathbf{Q}_i \in \mathbb{R}^{64}$ \\[4pt]
\small{384 cech (``wszystko'')} & \small{projekcja} & \small{64 cechy (``pytanie'')} \\[10pt]
\multicolumn{3}{c}{\textit{Co robi ta projekcja geometrycznie?}} \\[6pt]
\multicolumn{3}{l}{\textbf{1.} Wybiera 64 ``najważniejszych'' kierunków z 384-wymiarowej przestrzeni} \\
\multicolumn{3}{l}{\textbf{2.} Obraca je tak, żeby iloczyn skalarny Q $\cdot$ K mierzył ``to co trzeba''} \\
\multicolumn{3}{l}{\textbf{3.} Różne macierze $\mathbf{W}_Q$ vs $\mathbf{W}_K$ = różne ``co trzeba''}
\end{tabular}
\end{center}

\textbf{Ile parametrów?} Każda macierz $\mathbf{W}$ to $64 \times 384 = 24\,576$ wag.
Dla 3 macierzy: $3 \times 24\,576 = 73\,728$.
Ale mamy 6 głowic, każda z własnymi macierzami: $6 \times 73\,728 = 442\,368$ parametrów
-- sam attention to $\sim 0.44$M parametrów na blok!

\end{tcolorbox}

Teraz budujemy wzór na wagę uwagi (attention) krok po kroku.

\medskip

\textbf{Krok 1: Ile token $i$ ``interesuje się'' tokenem $j$?}

Liczymy \textbf{iloczyn skalarny} Query tokenu $i$ z Key tokenu $j$:
\begin{equation}
s_{ij} = \mathbf{Q}_i \cdot \mathbf{K}_j = \sum_{m=1}^{d} Q_i[m] \cdot K_j[m]
\end{equation}

Iloczyn skalarny mierzy \textbf{podobieństwo kierunków}:
\begin{itemize}[leftmargin=2em]
  \item $s_{ij}$ duże i dodatnie $\Rightarrow$ pytanie $i$ pasuje do opisu $j$ (token $j$ jest ``interesujący''),
  \item $s_{ij} \approx 0$ $\Rightarrow$ brak związku,
  \item $s_{ij}$ ujemne $\Rightarrow$ ``odpychają się'' (token $j$ jest nieistotny dla $i$).
\end{itemize}

\textbf{Krok 2: Normalizacja przez $\sqrt{d}$.}

$d$ to \textbf{wymiar wektorów Q/K w jednej głowie attention}.
ViT-Small ma $6$ głów, a pełny wymiar to $384$, więc każda głowa operuje na:
\begin{equation}
d = \frac{384}{6} = 64, \qquad \sqrt{d} = \sqrt{64} = 8
\end{equation}

\textbf{Po co dzielenie?} Iloczyn skalarny to suma $d$ składników.
Jeśli składowe $Q$ i $K$ mają wariancję $\approx 1$,
to wariancja sumy rośnie proporcjonalnie do $d$:
\begin{equation}
\text{Var}(s_{ij}) = \text{Var}\!\left(\sum_{m=1}^{d} Q_i[m] \cdot K_j[m]\right) \approx d
\end{equation}

Więc $s_{ij}$ ma odchylenie standardowe $\approx \sqrt{d}$.
Dzielenie przez $\sqrt{d}$ sprowadza skalę z powrotem do $\approx 1$:
\begin{equation}
\tilde{s}_{ij} = \frac{s_{ij}}{\sqrt{d}} = \frac{\mathbf{Q}_i \cdot \mathbf{K}_j}{\sqrt{d}}
\quad \Rightarrow \quad \text{Var}(\tilde{s}_{ij}) \approx 1
\end{equation}

Bez tego dzielenia: $s_{ij}$ miałoby wartości rzędu $\pm\sqrt{64} = \pm 8$,
a $e^8 \approx 2981$ vs $e^{-8} \approx 0.0003$
-- softmax dałby prawie \textit{one-hot} (jeden token dostaje $\approx 100\%$ uwagi).
Po dzieleniu: $\tilde{s}_{ij}$ ma wartości rzędu $\pm 1$, a $e^1 \approx 2.7$ vs $e^{-1} \approx 0.37$
-- softmax daje ``miękkie'' wagi i sieć może patrzeć na \textit{wielu} tokenów.

\textbf{Krok 3: Zamiana na prawdopodobieństwa (softmax).}

Mamy wyniki $\tilde{s}_{i1}, \tilde{s}_{i2}, \ldots, \tilde{s}_{i,197}$ -- jak bardzo
token $i$ interesuje się każdym z $197$ tokenów.
Chcemy zamienić je na \textbf{wagi sumujące się do 1} (żeby wynik był ważoną średnią).

Funkcja \textbf{softmax} robi dokładnie to:
\begin{equation}
\text{softmax}(x_j) = \frac{e^{x_j}}{\sum_{k} e^{x_k}}
\end{equation}

Dlaczego $e^x$, a nie np.\ samo $x$?
\begin{itemize}[leftmargin=2em]
  \item $e^x > 0$ zawsze -- wagi są nieujemne (prawdopodobieństwa),
  \item $e^x$ rośnie \textit{wykładniczo} -- duże $\tilde{s}_{ij}$ dominują
        (sieć ``skupia uwagę'' na najważniejszych tokenach),
  \item dzielenie przez sumę gwarantuje $\sum_j \alpha_{ij} = 1$.
\end{itemize}

\textbf{Krok 4: Składamy wszystko razem.}

Podstawiamy $x_j = \tilde{s}_{ij} = \mathbf{Q}_i \cdot \mathbf{K}_j / \sqrt{d}$ do softmax:
\begin{equation}
\boxed{
\alpha_{ij} = \frac{e^{\,\mathbf{Q}_i \cdot \mathbf{K}_j \,/\, \sqrt{d}}}
{\sum_{k=1}^{197} e^{\,\mathbf{Q}_i \cdot \mathbf{K}_k \,/\, \sqrt{d}}}
}
\end{equation}

\textbf{Przykład liczbowy} (uproszczony, 3 tokeny, $d=2$):
\begin{align}
\mathbf{Q}_1 &= (1,\; 0), \quad
\mathbf{K}_1 = (1,\; 0), \quad
\mathbf{K}_2 = (0,\; 1), \quad
\mathbf{K}_3 = (-1,\; 0) \nonumber\\[0.3em]
s_{11} &= 1 \cdot 1 + 0 \cdot 0 = 1, \quad
s_{12} = 1 \cdot 0 + 0 \cdot 1 = 0, \quad
s_{13} = 1 \cdot ({-1}) + 0 \cdot 0 = {-1} \nonumber\\[0.3em]
\tilde{s}_{11} &= 1/\sqrt{2} \approx 0.71, \quad
\tilde{s}_{12} = 0/\sqrt{2} = 0, \quad
\tilde{s}_{13} = {-1}/\sqrt{2} \approx {-0.71} \nonumber\\[0.3em]
e^{0.71} &\approx 2.03, \quad e^{0} = 1, \quad e^{-0.71} \approx 0.49
\quad \Rightarrow \quad \text{suma} = 3.52 \nonumber\\[0.3em]
\alpha_{11} &= 2.03 / 3.52 \approx \mathbf{0.58}, \quad
\alpha_{12} = 1 / 3.52 \approx \mathbf{0.28}, \quad
\alpha_{13} = 0.49 / 3.52 \approx \mathbf{0.14}
\end{align}

Token 1 patrzy głównie na siebie (58\%), trochę na token 2 (28\%), prawie ignoruje token 3 (14\%)
-- bo Query 1 jest najbardziej ``zgodne'' z Key 1.

\medskip

\textbf{Krok 5: Ważona suma wartości.}

Nowa wartość tokenu $i$:
\begin{equation}
\mathbf{h}_i' = \sum_{j=1}^{197} \alpha_{ij}\,\mathbf{V}_j
= \alpha_{i1}\,\mathbf{V}_1 + \alpha_{i2}\,\mathbf{V}_2 + \cdots + \alpha_{i,197}\,\mathbf{V}_{197}
\end{equation}

Token $i$ \textbf{zbiera informację} od wszystkich tokenów,
ale \textit{więcej} od tych, na które ``patrzy'' (wysokie $\alpha_{ij}$).

\begin{keyinsight}[Intuicja: self-attention to ``rozmowa'']
Każdy token ``rozmawia'' ze wszystkimi innymi:
\begin{itemize}
  \item Token patcha z tęczówką pyta (Q): ``Kto jeszcze jest tęczówką?''
  \item Inne patche tęczówki odpowiadają (K): ``Ja!'' $\Rightarrow$ wysoki $\alpha_{ij}$
  \item Token zbiera ich treść (V) i aktualizuje swoją reprezentację
  \item \textbf{[CLS]} pyta WSZYSTKICH i zbiera globalne podsumowanie
\end{itemize}
\end{keyinsight}

\subsubsection{(b) MLP: przetwarzanie lokalne}

Po attention każdy token przechodzi przez sieć MLP \textbf{niezależnie od innych tokenów}.
O ile Self-Attention odpowiada na pytanie ``\textit{kto na kogo patrzy}'',
MLP odpowiada na pytanie ``\textit{co to wszystko razem znaczy}''.

\subsubsection{Wzór MLP -- krok po kroku}

Pełny wzór MLP w jednej linii:
\begin{equation}
\boxed{
\text{MLP}(\mathbf{h}) = \mathbf{W}_2\,\text{GELU}(\mathbf{W}_1 \mathbf{h} + \mathbf{b}_1) + \mathbf{b}_2
}
\label{eq:mlp}
\end{equation}
gdzie $\mathbf{W}_1 \in \mathbb{R}^{1536 \times 384}$,
$\mathbf{W}_2 \in \mathbb{R}^{384 \times 1536}$,
$\mathbf{b}_1 \in \mathbb{R}^{1536}$,
$\mathbf{b}_2 \in \mathbb{R}^{384}$.

Rozpiszmy to na trzy oddzielne kroki:

\textbf{Krok 1 -- Rozszerzenie liniowe} ($384 \to 1536$, czyli $4\times$ więcej wymiarów):
\begin{equation}
\mathbf{z} = \mathbf{W}_1 \mathbf{h} + \mathbf{b}_1 \in \mathbb{R}^{1536}
\end{equation}

Każdy element $z_j$ to \textbf{ważona suma} wszystkich $384$ wymiarów wejścia:
\[
z_j = \sum_{i=1}^{384} W_{1,ji} \cdot h_i + b_{1,j} \qquad \text{dla } j = 1, \ldots, 1536
\]
Jeden wiersz macierzy $\mathbf{W}_1$ definiuje jeden ``kandydat na cechę'' --
inną liniową kombinację wejściowych wymiarów.
Mamy $1536$ takich kandydatów, czyli sieć tworzy $1536$ różnych ``hipotez''
o tym, co wejściowy token oznacza.

\textbf{Krok 2 -- GELU (nieliniowość)} ($1536 \to 1536$, ten sam wymiar):
\begin{equation}
\tilde{\mathbf{z}} = \text{GELU}(\mathbf{z}) \in \mathbb{R}^{1536}
\end{equation}
GELU działa \textbf{element po elemencie}: $\tilde{z}_j = \text{GELU}(z_j)$ dla każdego $j$.
\begin{itemize}[leftmargin=2em]
\item Jeśli $z_j > 0$: cecha ``przepuszczona'' prawie bez zmian ($\tilde{z}_j \approx z_j$).
\item Jeśli $z_j \ll 0$: cecha ``wyciszona'' ($\tilde{z}_j \approx 0$).
\item To jest \textbf{selekcja cech}: z~$1536$ kandydatów GELU wybiera te, które są aktywne
      (wartość dodatnia), a resztę wycisza. Typowo około 40--60\% neuronów jest wyciszonych.
\end{itemize}

\textbf{Dlaczego GELU jest konieczne?} Bez nieliniowości MLP byłby złożeniem dwóch transformacji liniowych:
\[
\mathbf{W}_2(\mathbf{W}_1 \mathbf{h} + \mathbf{b}_1) + \mathbf{b}_2
= \underbrace{(\mathbf{W}_2 \mathbf{W}_1)}_{\mathbf{W}'} \mathbf{h}
+ \underbrace{(\mathbf{W}_2 \mathbf{b}_1 + \mathbf{b}_2)}_{\mathbf{b}'}
\]
Złożenie dwóch macierzy to wciąż jedna macierz -- cała ukryta warstwa byłaby zbędna.
GELU ``łamie'' tę liniowość i umożliwia tworzenie \textbf{nieliniowych kombinacji cech}.

\textbf{Krok 3 -- Kompresja liniowa} ($1536 \to 384$, powrót do oryginalnego wymiaru):
\begin{equation}
\mathbf{o} = \mathbf{W}_2 \tilde{\mathbf{z}} + \mathbf{b}_2 \in \mathbb{R}^{384}
\end{equation}

Każdy element wyjścia $o_i$ to ważona suma \textbf{przefiltrowanych} cech:
\[
o_i = \sum_{j=1}^{1536} W_{2,ij} \cdot \tilde{z}_j + b_{2,i} \qquad \text{dla } i = 1, \ldots, 384
\]
Ponieważ GELU wyzerowała część $\tilde{z}_j$, ta suma efektywnie bierze pod uwagę
tylko \textbf{aktywne} cechy -- różny podzbiór dla różnych tokenów.

\begin{center}
\includegraphics[width=0.98\textwidth]{figures/mlp_architecture.pdf}
\captionof{figure}{Schemat MLP: rozszerzenie $4\times$ daje ``przestrzeń roboczą'' na cechy pośrednie,
GELU filtruje je, a kompresja $4\times$ pakuje wynik z powrotem do $384$ wymiarów.}
\label{fig:mlp_arch}
\end{center}

\subsubsection{Przykład liczbowy MLP}

Dla czytelności użyjemy $d = 4$ (zamiast $384$) i warstwy ukrytej $d_h = 8$ (zamiast $1536$).

Niech wejście po Self-Attention: $\mathbf{h} = (1.2, \; -0.8, \; 0.5, \; -1.5)$.

\textbf{Krok 1}: Mnożymy przez macierz $\mathbf{W}_1 \in \mathbb{R}^{8 \times 4}$ i dodajemy bias $\mathbf{b}_1$:
\[
\mathbf{z} = \mathbf{W}_1 \mathbf{h} + \mathbf{b}_1
\]
Przykładowe obliczenie dla $z_1$ (pierwszy wiersz $\mathbf{W}_1 = [0.6, \; -0.3, \; 0.8, \; 0.1]$):
\begin{align}
z_1 &= 0.6 \cdot 1.2 + (-0.3) \cdot (-0.8) + 0.8 \cdot 0.5 + 0.1 \cdot (-1.5) + 0.1 \nonumber \\
    &= 0.72 + 0.24 + 0.40 - 0.15 + 0.1 = 1.31 \nonumber
\end{align}
Analogicznie dla pozostałych 7 elementów:
\[
\mathbf{z} = (1.31, \; -0.37, \; 0.06, \; 0.33, \; 1.18, \; 0.84, \; -0.22, \; 1.44)
\]

\textbf{Krok 2}: Stosujemy GELU element po elemencie:
\begin{center}
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{c|r|r|l}
\toprule
$j$ & $z_j$ & $\tilde{z}_j = \text{GELU}(z_j)$ & \textbf{Efekt} \\
\midrule
1 & $+1.31$ & $+1.19$ & przepuszczony \\
2 & $-0.37$ & $-0.13$ & \textcolor{red}{stłumiony} \\
3 & $+0.06$ & $+0.03$ & na granicy \\
4 & $+0.33$ & $+0.22$ & przepuszczony \\
5 & $+1.18$ & $+1.06$ & przepuszczony \\
6 & $+0.84$ & $+0.69$ & przepuszczony \\
7 & $-0.22$ & $-0.09$ & \textcolor{red}{stłumiony} \\
8 & $+1.44$ & $+1.34$ & przepuszczony \\
\bottomrule
\end{tabular}
\end{center}

GELU wyciszyła elementy $z_2$ i $z_7$ (ujemne wartości $\to$ prawie zero).
Pozostałe $6$ z $8$ cech zostało ``przepuszczonych'' -- to jest \textbf{selekcja cech}.

\textbf{Krok 3}: Kompresja $\mathbf{W}_2 \tilde{\mathbf{z}} + \mathbf{b}_2$ z powrotem do $\mathbb{R}^4$:
\[
\mathbf{o} = \mathbf{W}_2 \tilde{\mathbf{z}} + \mathbf{b}_2 = (0.83, \; 0.34, \; 0.89, \; 0.62)
\]

\textbf{Wynik MLP}: $(1.2, \; -0.8, \; 0.5, \; -1.5) \xrightarrow{\text{MLP}} (0.83, \; 0.34, \; 0.89, \; 0.62)$

Każda wartość wyjściowa to nieliniowa kombinacja cech wejściowych,
gdzie GELU zdecydowała, \textit{które} cechy pośrednie brały udział w obliczeniu.

\begin{center}
\includegraphics[width=0.98\textwidth]{figures/mlp_step_by_step.pdf}
\captionof{figure}{Wizualizacja MLP krok po kroku.
Od lewej: wejście $\mathbf{h}$ ($d=4$), po rozszerzeniu $\mathbf{W}_1 \mathbf{h} + \mathbf{b}_1$ ($d=8$),
po GELU (wartości ujemne wyciszone -- selekcja cech),
wyjście $\mathbf{W}_2 \tilde{\mathbf{z}} + \mathbf{b}_2$ ($d=4$, kompresja).
Kolor niebieski = wartość dodatnia, czerwony = ujemna.}
\label{fig:mlp_steps}
\end{center}

\subsubsection{Co GELU robi z neuronami ukrytej warstwy?}

\begin{center}
\includegraphics[width=0.98\textwidth]{figures/mlp_gelu_effect.pdf}
\captionof{figure}{Lewo: transformacja GELU -- każdy neuron ukrytej warstwy (punkt)
jest przepuszczany (prawa strona) lub wyciszany (lewa strona, region czerwony).
Prawo: rozkład wartości $1000$ neuronów przed i po GELU -- GELU ``ściąga''
ujemne wartości do zera, tworząc efektywną selekcję cech.}
\label{fig:mlp_gelu}
\end{center}

\begin{tcolorbox}[
  colback=lejepaBlue!5,
  colframe=lejepaBlue!80,
  fonttitle=\bfseries,
  title={Dlaczego takie wymiary? Rozszerzenie $4\times$ i kompresja},
  breakable,
]

\textbf{Skąd $1536 = 4 \times 384$?}

MLP działa jak \textbf{odwrócony bottleneck} (inverted bottleneck):

\begin{center}
\begin{tabular}{ccccc}
$\mathbf{h} \in \mathbb{R}^{384}$
& $\xrightarrow{\;\mathbf{W}_1\;}$
& $\mathbb{R}^{1536}$
& $\xrightarrow{\;\mathbf{W}_2\;}$
& $\mathbf{h}_{\text{out}} \in \mathbb{R}^{384}$ \\
\small{wejście} & \small{rozszerzenie $4\times$} & \small{ukryta warstwa} & \small{kompresja $4\times$} & \small{wyjście}
\end{tabular}
\end{center}

\textbf{Dlaczego nie zostać przy $384$?}
Attention zbiera informację \textit{między} tokenami (``kto jest obok kogo'').
Ale potem każdy token musi tę informację \textbf{przetworzyć lokalnie}
-- np.\ ``widzę krawędź narzędzia + czerwony kolor + bliskość tęczówki $\Rightarrow$
to jest końcówka phaco''.

Takie \textbf{nieliniowe kombinowanie cech} wymaga dużo parametrów.
Rozszerzenie do $1536$ wymiarów daje sieci ``przestrzeń roboczą'',
w której może tworzyć skomplikowane cechy pośrednie.

\end{tcolorbox}

% ============ GELU: SZCZEGÓŁOWE WYJAŚNIENIE ============
\begin{tcolorbox}[breakable, colback=blue!3, colframe=blue!60!black,
  title=\textbf{GELU: czym jest i jak działa?}]

Funkcja GELU (Gaussian Error Linear Unit) to \textbf{gładka wersja ReLU}:

\begin{equation}
\text{GELU}(x) = x \cdot \Phi(x) \qquad \text{gdzie } \Phi(x) = \frac{1}{2}\left[1 + \text{erf}\!\left(\frac{x}{\sqrt{2}}\right)\right]
\end{equation}

$\Phi(x)$ to \textbf{dystrybuanta rozkładu normalnego} -- prawdopodobieństwo,
że zmienna losowa z $\mathcal{N}(0,1)$ jest $\leq x$.

\textbf{Intuicja}: GELU mnoży wartość $x$ przez ``prawdopodobieństwo, że $x$ jest ważne'':
\begin{itemize}[leftmargin=2em]
\item Jeśli $x \gg 0$: \; $\Phi(x) \approx 1$ \; $\Rightarrow$ \; $\text{GELU}(x) \approx x$ \quad (przepuść bez zmian)
\item Jeśli $x \ll 0$: \; $\Phi(x) \approx 0$ \; $\Rightarrow$ \; $\text{GELU}(x) \approx 0$ \quad (wycisz)
\item Jeśli $x \approx 0$: \; $\Phi(x) \approx 0.5$ \; $\Rightarrow$ \; $\text{GELU}(x) \approx 0.5x$ \quad (\textbf{łagodne tłumienie})
\end{itemize}

\textbf{Porównanie wartości:}
\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{r|ccc}
$x$ & $\text{ReLU}(x)$ & $\Phi(x)$ & $\text{GELU}(x) = x\Phi(x)$ \\
\hline
$-3$ & $0$ & $0.001$ & $-0.004$ \\
$-1$ & $0$ & $0.159$ & $-0.159$ \\
$0$ & $0$ & $0.500$ & $0$ \\
$+1$ & $1$ & $0.841$ & $0.841$ \\
$+3$ & $3$ & $0.999$ & $2.996$ \\
\end{tabular}
\end{center}

\textbf{Dlaczego GELU a nie ReLU?}
\begin{itemize}[leftmargin=2em]
\item \textbf{ReLU}: ostro zeruje wszystko $<0$ (pochodna = 0 dla $x<0$ $\to$ ``martwe neurony'')
\item \textbf{GELU}: łagodnie tłumi -- wartości bliskie zeru są \textit{częściowo} przepuszczane,
      co daje lepszy przepływ gradientu.
\end{itemize}

\begin{center}
\includegraphics[width=0.95\textwidth]{figures/gelu_activation.pdf}
\captionof{figure}{Lewo: porównanie GELU z ReLU -- GELU łagodnie tłumi wartości ujemne zamiast
ostro je zerować. Prawo: efekt GELU na rozkład wartości -- wartości ujemne są ``spychane''
w kierunku zera, ale nie wycinane.}
\label{fig:gelu}
\end{center}

\end{tcolorbox}

\begin{tcolorbox}[
  colback=lejepaBlue!5,
  colframe=lejepaBlue!80,
  fonttitle=\bfseries,
  title={Dlaczego akurat rozszerzenie $4\times$?},
  breakable,
]

\textbf{Dlaczego akurat $4\times$?}
To wartość ustalona empirycznie przez Vaswani et~al.\ (2017).
Testowano $1\times$, $2\times$, $4\times$, $8\times$:
\begin{itemize}[leftmargin=2em]
  \item $1\times$ lub $2\times$: za mało pojemności -- sieć nie potrafi tworzyć złożonych cech,
  \item $4\times$: dobry kompromis wydajność/koszt,
  \item $8\times$: marginalna poprawa, ale $2\times$ więcej parametrów i obliczeń.
\end{itemize}
Wartość $4\times$ stała się \textbf{standardem} w prawie wszystkich Transformerach
(ViT, BERT, GPT, LLaMA, \ldots).

\textbf{Ile to parametrów?}
\begin{align}
\mathbf{W}_1&: \; 1536 \times 384 = 589\,824 \text{ wag} + 1536 \text{ biasów} \nonumber\\
\mathbf{W}_2&: \; 384 \times 1536 = 589\,824 \text{ wag} + 384 \text{ biasy} \nonumber\\
&\text{Razem na 1 blok MLP:} \approx \mathbf{1.18\text{M}} \text{ parametrów}
\end{align}

Przy $12$ blokach: $12 \times 1.18\text{M} \approx 14.2\text{M}$
-- to \textbf{ponad połowa} wszystkich $\sim 22$M parametrów ViT-Small!

\end{tcolorbox}

% -- LAYERNORM (przed pierwszym użyciem w równaniu rezydualnym) --
\begin{tcolorbox}[colback=gray!5, colframe=gray!60!black, title=\textbf{Co to jest LayerNorm?}, breakable]

Zanim pokażemy pełne równanie bloku Transformera, musimy wyjaśnić operację
$\text{LayerNorm}$, która pojawia się w nim dwukrotnie.

\subsubsection{Po co normalizacja w Transformerze?}

\textbf{Problem}: po wielu warstwach wartości w wektorze $\mathbf{h}$ mogą stać się bardzo duże
(np.\ rzędu tysięcy) lub bardzo małe (rzędu $10^{-6}$). To destabilizuje trening --
softmax i GELU działają dobrze tylko dla wartości w ``rozsądnym'' zakresie.

Konkretnie, bez normalizacji:
\begin{itemize}[leftmargin=2em]
\item \textbf{Softmax w attention} -- jeśli wartości $\mathbf{Q}\mathbf{K}^\top$ są rzędu tysięcy,
      softmax daje rozkład prawie jedynkowo skoncentrowany na jednym tokenie
      (tzw.\ \textit{attention collapse}), co niszczy zdolność do łączenia informacji.
\item \textbf{GELU w MLP} -- dla $|x| \gg 3$ GELU zachowuje się prawie liniowo,
      więc warstwa MLP traci swoją nieliniowość i staje się zwykłą transformacją afiniczną.
\item \textbf{Gradienty} -- duże wartości aktywacji powodują duże gradienty (\textit{exploding gradients}),
      a bardzo małe wartości prowadzą do zaniku gradientów (\textit{vanishing gradients}).
      W obu przypadkach trening staje się niestabilny.
\end{itemize}

\textbf{Rozwiązanie}: LayerNorm \textbf{normalizuje} wektor do średniej $\approx 0$
i wariancji $\approx 1$ przed każdą operacją, utrzymując wartości w zakresie,
w~którym softmax, GELU i gradienty działają prawidłowo.

\subsubsection{Wzór krok po kroku}

Dany wektor $\mathbf{h} = (h_1, h_2, \ldots, h_d) \in \mathbb{R}^d$ (w ViT-Small $d = 384$):
\begin{itemize}[leftmargin=2em]
\item $\mathbf{h}$ -- cały wektor embeddingu \textbf{jednego tokenu} (wszystkie 384 liczb naraz),
\item $h_i$ -- \textbf{i-ta wartość} w tym wektorze (jedna liczba, np.\ $h_1$ = pierwsza, $h_{384}$ = ostatnia).
\end{itemize}

\textbf{Krok 1 -- Średnia} (centrowanie):
\[
\mu = \frac{1}{d}\sum_{i=1}^{d} h_i
\]
Obliczamy jedną liczbę $\mu$ -- średnią po \textit{wszystkich} $d$ wymiarach tokenu.
Intuicja: $\mu$ mierzy ``ogólny poziom'' aktywacji -- czy wartości są globalnie przesunięte
w górę czy w dół.

\textbf{Krok 2 -- Wariancja} (jak bardzo wartości ``rozbiegają się'' od średniej):
\[
\sigma^2 = \frac{1}{d}\sum_{i=1}^{d} (h_i - \mu)^2
\]
Jedna liczba $\sigma^2$ mierzy ``rozrzut'' wartości wokół średniej.
Jeśli $\sigma^2$ jest duże, wartości mocno się różnią; jeśli małe -- są blisko siebie.

\textbf{Krok 3 -- Normalizacja} (centruj + podziel przez odchylenie):
\[
\hat{h}_i = \frac{h_i - \mu}{\sqrt{\sigma^2 + \epsilon}}
\]
\begin{itemize}[leftmargin=2em]
\item Odejmujemy $\mu$ -- \textbf{centrujemy} wektor (średnia $\to 0$).
\item Dzielimy przez $\sqrt{\sigma^2 + \epsilon}$ -- \textbf{skalujemy} do wariancji $\approx 1$.
\item $\epsilon = 10^{-5}$ -- mała stała zapobiegająca dzieleniu przez zero
      (gdyby $\sigma^2 = 0$, czyli wszystkie $h_i$ były identyczne).
\end{itemize}

Po tym kroku: $\hat{\mathbf{h}}$ ma średnią $= 0$ i wariancję $= 1$ (znormalizowany wektor).

\textbf{Krok 4 -- Skalowanie i przesunięcie} (uczone parametry $\gamma$, $\beta$):
\begin{equation}
\text{LayerNorm}(\mathbf{h})_i = \gamma_i \cdot \hat{h}_i + \beta_i
\label{eq:layernorm_elementwise}
\end{equation}

Parametry $\gamma_i, \beta_i \in \mathbb{R}$ (po jednym na każdy z $d$ wymiarów) są \textbf{uczone} --
sieć sama decyduje, jaka skala i przesunięcie są optymalne dla każdego wymiaru.

Inicjalizacja: $\gamma_i = 1$, $\beta_i = 0$ dla wszystkich $i$
-- na starcie treningu LayerNorm po prostu zwraca $\hat{\mathbf{h}}$ bez zmian.

\textbf{Wzór wektorowy} (cała operacja w jednej linii):
\begin{equation}
\boxed{
\text{LayerNorm}(\mathbf{h}) = \boldsymbol{\gamma} \odot \frac{\mathbf{h} - \mu \cdot \mathbf{1}}{\sqrt{\sigma^2 + \epsilon}} + \boldsymbol{\beta}
}
\label{eq:layernorm_vector}
\end{equation}

gdzie $\odot$ oznacza mnożenie element-po-elemencie (Hadamard product),
$\boldsymbol{\gamma} = (\gamma_1, \ldots, \gamma_d)$,
$\boldsymbol{\beta} = (\beta_1, \ldots, \beta_d)$,
$\mathbf{1} = (1, \ldots, 1) \in \mathbb{R}^d$.

\subsubsection{Dlaczego uczone $\gamma$ i $\beta$?}

Na pierwszy rzut oka uczone $\gamma$ i $\beta$ wydają się sprzeczne z celem normalizacji
-- skoro właśnie wymusiliśmy średnią $0$ i wariancję $1$, po co pozwalać sieci to zmienić?

\textbf{Odpowiedź}: normalizacja do $\hat{\mathbf{h}}$ jest potrzebna, żeby ustabilizować
\textit{przepływ gradientów} przez sieć. Ale \textit{optymalna reprezentacja} dla konkretnej
warstwy niekoniecznie ma średnią $0$ i wariancję $1$ w każdym wymiarze.

Parametry $\gamma$, $\beta$ rozwiązują oba problemy jednocześnie:
\begin{itemize}[leftmargin=2em]
\item \textbf{Stabilizacja}: krok normalizacji (kroki 1--3) gwarantuje, że wartości
      wchodzące do skalowania są zawsze w kontrolowanym zakresie, niezależnie od tego,
      jak duże lub małe były oryginalne $h_i$.
\item \textbf{Ekspresywność}: $\gamma_i$ i $\beta_i$ pozwalają sieci przywrócić
      \textit{dowolną} skalę i przesunięcie, jeśli to konieczne.
      W szczególności, jeśli $\gamma_i = \sigma$ i $\beta_i = \mu$,
      LayerNorm staje się operacją tożsamościową (nic nie zmienia).
      Ale w praktyce sieć uczy się innych, optymalnych wartości.
\end{itemize}

\subsubsection{Ile parametrów dodaje LayerNorm?}

W ViT-Small ($d = 384$) każda warstwa LayerNorm ma:
\[
\underbrace{384}_{\gamma} + \underbrace{384}_{\beta} = 768 \text{ parametrów}
\]
Każdy blok Transformera zawiera \textbf{2 LayerNormy} (przed Self-Attention i przed MLP), więc:
\[
12 \text{ bloków} \times 2 \times 768 = 18{,}432 \text{ parametrów}
\]
To mniej niż $0.1\%$ wszystkich $\sim$22M parametrów ViT-Small -- niemal zerowy koszt
w zamian za stabilny trening.

\subsubsection{Przykład liczbowy}

Niech $\mathbf{h} = (2.0, \; 4.0, \; 6.0, \; 8.0)$ (uproszczenie -- $d = 4$ zamiast 384).

\textbf{Kroki 1--3 (normalizacja):}

\begin{center}
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{ll}
\toprule
\textbf{Krok} & \textbf{Obliczenie} \\
\midrule
Średnia $\mu$ & $\frac{2+4+6+8}{4} = 5.0$ \\[4pt]
Wariancja $\sigma^2$ & $\frac{(2{-}5)^2 + (4{-}5)^2 + (6{-}5)^2 + (8{-}5)^2}{4} = \frac{9+1+1+9}{4} = 5.0$ \\[4pt]
Odchylenie $\sqrt{\sigma^2}$ & $\sqrt{5.0} \approx 2.236$ \\[4pt]
$\hat{h}_1 = \frac{2.0 - 5.0}{2.236}$ & $= -1.34$ \\[2pt]
$\hat{h}_2 = \frac{4.0 - 5.0}{2.236}$ & $= -0.45$ \\[2pt]
$\hat{h}_3 = \frac{6.0 - 5.0}{2.236}$ & $= +0.45$ \\[2pt]
$\hat{h}_4 = \frac{8.0 - 5.0}{2.236}$ & $= +1.34$ \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Wynik normalizacji}: $(2, 4, 6, 8) \to (-1.34, \; -0.45, \; +0.45, \; +1.34)$

Sprawdzenie: średnia $= \frac{-1.34 -0.45 +0.45 +1.34}{4} = 0$ \checkmark, \;
wariancja $= \frac{1.34^2 + 0.45^2 + 0.45^2 + 1.34^2}{4} = \frac{1.80 + 0.20 + 0.20 + 1.80}{4} = 1.0$ \checkmark

\textbf{Krok 4 (skalowanie i przesunięcie):}

Na starcie treningu $\gamma = (1,1,1,1)$, $\beta = (0,0,0,0)$, więc wynik $= \hat{\mathbf{h}}$
(tożsamość). Po kilku epokach sieć może nauczyć się np.\
$\boldsymbol{\gamma} = (2.0, \; 0.5, \; 1.0, \; 1.5)$,
$\boldsymbol{\beta} = (0.1, \; -0.3, \; 0.0, \; 0.2)$:

\begin{center}
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{lll}
\toprule
\textbf{Wymiar} & \textbf{Obliczenie} & \textbf{Wynik} \\
\midrule
$i=1$ & $2.0 \times (-1.34) + 0.1$ & $= -2.58$ \\
$i=2$ & $0.5 \times (-0.45) + (-0.3)$ & $= -0.52$ \\
$i=3$ & $1.0 \times (+0.45) + 0.0$ & $= +0.45$ \\
$i=4$ & $1.5 \times (+1.34) + 0.2$ & $= +2.21$ \\
\bottomrule
\end{tabular}
\end{center}

Zauważ, że po zastosowaniu $\gamma$ i $\beta$ średnia i wariancja \textbf{nie są} już
$0$ i $1$ -- i to jest zamierzone. Normalizacja w krokach 1--3 stabilizuje gradienty,
a uczone $\gamma$, $\beta$ przywracają sieci pełną ekspresywność.

\subsubsection{Gdzie LayerNorm działa w Transformerze?}

W ViT z \textit{Pre-Norm} (standard od 2020 r.) LayerNorm jest stosowany \textbf{przed}
każdą operacją, a nie po niej:

\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{cll}
\toprule
\textbf{Nr} & \textbf{LayerNorm} & \textbf{Co normalizuje?} \\
\midrule
1 & Przed Self-Attention & Wejście do $Q, K, V$ -- stabilizuje iloczyny skalarowe $QK^\top$ \\
2 & Przed MLP & Wejście do warstw liniowych -- stabilizuje GELU \\
\bottomrule
\end{tabular}
\end{center}

Każda z tych dwóch normalizacji ma \textbf{własne, niezależne} parametry $\gamma$ i $\beta$.
Dzięki temu sieć może nauczyć się różnych skal dla attention (gdzie ważna jest
``podobieństwo między tokenami'') i MLP (gdzie ważne jest ``przetwarzanie cech'').

Ponadto, w ViT istnieje jeszcze \textbf{trzeci LayerNorm} -- zastosowany na samym końcu,
po ostatnim (12-tym) bloku Transformera, bezpośrednio przed ekstrakcją tokenu [CLS]
jako końcowego embeddingu. Stabilizuje on skalę wyjściowej reprezentacji.

\subsubsection{Pre-Norm vs Post-Norm}

Oryginalny Transformer (Vaswani et~al., 2017) stosował LayerNorm \textbf{po} dodaniu residuala
(\textit{Post-Norm}):
\[
\mathbf{h}' = \text{LayerNorm}(\mathbf{h} + \text{Self-Attention}(\mathbf{h}))
\]

Współczesne ViT (od 2020 r.) stosują LayerNorm \textbf{przed} operacją (\textit{Pre-Norm}):
\[
\mathbf{h}' = \mathbf{h} + \text{Self-Attention}(\text{LayerNorm}(\mathbf{h}))
\]

\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{lcc}
\toprule
& \textbf{Post-Norm} & \textbf{Pre-Norm} \\
\midrule
Stabilność treningu & trudniejsza & \textbf{łatwiejsza} \\
Potrzeba warm-up? & tak, konieczny & mniej wrażliwy \\
Przepływ gradientów & gradient przechodzi przez LN & \textbf{gradient ma ścieżkę ``na skróty''} \\
Standard w ViT? & nie & \textbf{tak} \\
\bottomrule
\end{tabular}
\end{center}

Kluczowa przewaga Pre-Norm: w połączeniu rezydualnym $\mathbf{h}' = \mathbf{h} + f(\text{LN}(\mathbf{h}))$
gradient z lossu przepływa \textbf{bezpośrednio} przez ``$+\;\mathbf{h}$'' (ścieżka rezydualna)
bez przechodzenia przez LayerNorm. To zapobiega zanikowi gradientu w głębokich sieciach.

\subsubsection{Dlaczego LayerNorm a nie BatchNorm?}

\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{lcc}
\toprule
& \textbf{BatchNorm} & \textbf{LayerNorm} \\
\midrule
Normalizuje po & próbkach w batchu & wymiarach jednego tokenu \\
Statystyki ($\mu, \sigma^2$) & z batcha ($B$ próbek) & z jednego tokenu ($d$ wymiarów) \\
Zależy od batch size? & tak & \textbf{nie} \\
Działa przy batch=1? & źle & \textbf{dobrze} \\
Wymaga running mean/var? & tak (inferencja) & \textbf{nie} \\
Standard w Transformerach? & nie & \textbf{tak} \\
\bottomrule
\end{tabular}
\end{center}

Kluczowe różnice:
\begin{itemize}[leftmargin=2em]
\item \textbf{BatchNorm} oblicza $\mu$ i $\sigma^2$ \textbf{po batchu} -- uśrednia wartości
      wymiaru $i$ po wszystkich $B$ próbkach. To powoduje, że wynik dla jednego obrazu
      zależy od tego, jakie inne obrazy są w batchu -- niedeterminizm.
\item \textbf{LayerNorm} oblicza $\mu$ i $\sigma^2$ \textbf{wewnątrz jednego tokenu} --
      uśrednia po $d = 384$ wymiarach tego samego wektora.
      Wynik jest w~pełni \textbf{deterministyczny}: ten sam obraz zawsze
      daje ten sam wynik, niezależnie od reszty batcha.
\item W Transformerach sekwencja tokenów ma \textbf{zmienną długość}
      (różne obrazy mogą mieć różną liczbę patchy po maskingu).
      BatchNorm wymagałby uśredniania po tokenach z różnych pozycji,
      co nie ma sensu semantycznego. LayerNorm tego problemu nie ma.
\end{itemize}

\subsubsection{Podsumowanie LayerNorm}

\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{ll}
\toprule
\textbf{Aspekt} & \textbf{Szczegóły} \\
\midrule
Wejście & $\mathbf{h} \in \mathbb{R}^d$ (jeden token, $d = 384$ w ViT-Small) \\
Wyjście & $\text{LN}(\mathbf{h}) \in \mathbb{R}^d$ (ten sam rozmiar) \\
Uczone parametry & $\boldsymbol{\gamma}, \boldsymbol{\beta} \in \mathbb{R}^d$ ($2d = 768$ skalarów) \\
Inicjalizacja & $\boldsymbol{\gamma} = \mathbf{1}$, $\boldsymbol{\beta} = \mathbf{0}$ (tożsamość) \\
Gdzie w bloku & Przed Self-Attention i przed MLP (Pre-Norm) \\
Koszt & $<0.1\%$ parametrów, praktycznie zerowy koszt obliczeniowy \\
\bottomrule
\end{tabular}
\end{center}

\end{tcolorbox}

\subsubsection{Pełny blok z residualami i normalizacją}

Każdy z 12 bloków Transformera wykonuje dwie operacje -- Self-Attention i MLP --
ale \textbf{nie zastępuje} wejścia wynikiem. Zamiast tego \textbf{dodaje} wynik do wejścia:

\begin{equation}
\boxed{
\begin{aligned}
\mathbf{h}' &= \mathbf{h} + \text{Self-Attention}(\text{LayerNorm}(\mathbf{h})) \\
\mathbf{h}^{(\ell+1)} &= \mathbf{h}' + \text{MLP}(\text{LayerNorm}(\mathbf{h}'))
\end{aligned}
}
\end{equation}

To ``$+$'' w równaniu to \textbf{połączenie rezydualne} (residual connection) --
najważniejszy trick, który umożliwia trenowanie głębokich sieci.

% -- SCHEMAT BLOKOWY --
\begin{tcolorbox}[breakable, colback=blue!3, colframe=blue!60!black, title=\textbf{Schemat przepływu przez jeden blok Transformera}]

\begin{center}
\begin{tabular}{c}
\texttt{Wejście: h} \\[4pt]
$\downarrow$ \\[2pt]
\fbox{\texttt{LayerNorm}} \\[2pt]
$\downarrow$ \\[2pt]
\fbox{\texttt{Self-Attention}} \\[2pt]
$\downarrow$ \quad wynik = $\Delta_1$ \\[2pt]
\texttt{h' = h + } $\Delta_1$ \quad $\longleftarrow$ \textit{residual: dodaj do wejścia!} \\[8pt]
$\downarrow$ \\[2pt]
\fbox{\texttt{LayerNorm}} \\[2pt]
$\downarrow$ \\[2pt]
\fbox{\texttt{MLP}} \\[2pt]
$\downarrow$ \quad wynik = $\Delta_2$ \\[2pt]
\texttt{h\textsuperscript{(next)} = h' + } $\Delta_2$ \quad $\longleftarrow$ \textit{residual: dodaj do wejścia!} \\[4pt]
$\downarrow$ \\[2pt]
\texttt{Wyjście: h\textsuperscript{(next)}} \\
\end{tabular}
\end{center}

\textbf{Kluczowa obserwacja:} Attention i MLP nie produkują nowej wartości
od zera -- produkują jedynie \textbf{poprawkę} ($\Delta$), która jest
\textbf{dodawana} do oryginalnego sygnału.

\end{tcolorbox}

% -- CHAIN RULE OD PODSTAW --
\begin{tcolorbox}[breakable, colback=blue!3, colframe=blue!60!black, title=\textbf{Wstawka: reguła łańcuchowa (chain rule) -- od jednej funkcji do 12 warstw}]

Zanim pokażemy gradient w sieci, zbudujmy intuicję \textbf{od zera}.

\medskip
\textbf{Krok 0: Jedna funkcja}

Masz $y = f(x)$. Pochodna mówi ``jak zmiana $x$ wpływa na $y$'':
\[
\frac{\partial y}{\partial x} = f'(x)
\]
To znasz -- nic nowego.

\medskip
\textbf{Krok 1: Dwie funkcje złożone}

Teraz $y = g\!\big(f(x)\big)$ -- najpierw $f$, potem $g$. Pytamy: ``jak zmiana $x$ wpływa na $y$?''

Odpowiedź: zmiana $x$ najpierw zmienia $f$, a potem zmiana $f$ zmienia $g$. \textbf{Mnożymy} te dwa efekty:
\[
\frac{\partial y}{\partial x}
= \frac{\partial g}{\partial f} \cdot \frac{\partial f}{\partial x}
\]

\textit{Analogia}: Jeśli 1 obrót pedału kręci łańcuchem 3 razy ($\partial f/\partial x = 3$),
a 1 obrót łańcucha kręci kołem 2 razy ($\partial g/\partial f = 2$),
to 1 obrót pedału kręci kołem $3 \times 2 = 6$ razy.

\medskip
\textbf{Krok 2: Trzy funkcje = trzy czynniki}

$y = h\!\big(g\!\big(f(x)\big)\big)$ -- trzy ``warstwy''. Chain rule daje \textbf{iloczyn trzech} pochodnych:
\[
\frac{\partial y}{\partial x}
= \frac{\partial h}{\partial g} \cdot \frac{\partial g}{\partial f} \cdot \frac{\partial f}{\partial x}
\]

Wzorzec: \textbf{ile funkcji złożonych, tyle czynników w iloczynie}.

\medskip
\textbf{Krok 3: Symbol $\prod$ -- zwięzły zapis iloczynu}

Tak jak $\sum$ oznacza ``dodawaj kolejne elementy'',
$\prod$ oznacza ``\textbf{mnóż} kolejne elementy'':

\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{lcl}
$\displaystyle\sum_{i=1}^{3} a_i$ & $=$ & $a_1 + a_2 + a_3$ \quad (suma) \\[6pt]
$\displaystyle\prod_{i=1}^{3} a_i$ & $=$ & $a_1 \cdot a_2 \cdot a_3$ \quad (iloczyn) \\
\end{tabular}
\end{center}

Więc chain rule dla $L$ złożonych funkcji:
\[
\frac{\partial y}{\partial x} = \prod_{\ell=1}^{L} \frac{\partial f_\ell}{\partial f_{\ell-1}}
\quad = \quad
\underbrace{\frac{\partial f_L}{\partial f_{L-1}} \cdot \frac{\partial f_{L-1}}{\partial f_{L-2}}
\cdot \;\ldots\; \cdot \frac{\partial f_1}{\partial x}}_{L \text{ czynników}}
\]

\end{tcolorbox}

% -- DLACZEGO RESIDUALNE --
\begin{tcolorbox}[breakable, colback=red!3, colframe=red!60!black, title=\textbf{Problem: dlaczego bez residuali sieć nie działa?}]

\textbf{Bez połączeń rezydualnych} (naiwne podejście) każda warstwa \textit{zastępuje} wejście:
\[
\mathbf{h}^{(\ell+1)} = f_\ell(\mathbf{h}^{(\ell)})
\]

Rozpiszmy to dla 3 warstw, żeby zobaczyć jak chain rule się składa:
\begin{align*}
\mathbf{h}^{(1)} &= f_0(\mathbf{h}^{(0)}) \\
\mathbf{h}^{(2)} &= f_1(\mathbf{h}^{(1)}) = f_1\!\big(f_0(\mathbf{h}^{(0)})\big) \\
\mathbf{h}^{(3)} &= f_2(\mathbf{h}^{(2)}) = f_2\!\big(f_1\!\big(f_0(\mathbf{h}^{(0)})\big)\big)
\end{align*}

Gradient lossu $\mathcal{L}$ po wejściu $\mathbf{h}^{(0)}$ -- stosujemy chain rule (mnożymy pochodne):
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{h}^{(0)}}
= \frac{\partial \mathcal{L}}{\partial \mathbf{h}^{(3)}}
\cdot \underbrace{\frac{\partial f_2}{\partial \mathbf{h}^{(2)}}}_{\text{warstwa 3}}
\cdot \underbrace{\frac{\partial f_1}{\partial \mathbf{h}^{(1)}}}_{\text{warstwa 2}}
\cdot \underbrace{\frac{\partial f_0}{\partial \mathbf{h}^{(0)}}}_{\text{warstwa 1}}
\]

Dla 12 warstw -- 12 czynników, zapisujemy zwięźle jako $\prod$:
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{h}^{(0)}}
= \frac{\partial \mathcal{L}}{\partial \mathbf{h}^{(12)}} \cdot
\prod_{\ell=0}^{11} \frac{\partial f_\ell}{\partial \mathbf{h}^{(\ell)}}
\]

\textbf{Problem:} to jest \textbf{iloczyn} 12 liczb. Jeśli każda jest $< 1$, wynik znika:
\[
\underbrace{0.5 \times 0.5 \times \ldots \times 0.5}_{12 \text{ razy}} = 0.5^{12} = 0.000244
\quad \rightarrow \quad \text{gradient \textbf{zanika!}}
\]
Pierwsze warstwy prawie się nie uczą. A jeśli każda $> 1$:
\[
2.0^{12} = 4096 \quad \rightarrow \quad \text{gradient \textbf{eksploduje!}}
\]
\end{tcolorbox}

% -- Z RESIDUALAMI --
\begin{tcolorbox}[breakable, colback=green!3, colframe=green!60!black, title=\textbf{Rozwiązanie: połączenie rezydualne jako ``autostrada'' dla gradientu}]

\textbf{Z połączeniem rezydualnym} warstwa \textit{dodaje} poprawkę zamiast zastępować:
\[
\mathbf{h}^{(\ell+1)} = \mathbf{h}^{(\ell)} + f_\ell(\mathbf{h}^{(\ell)})
\]

\textbf{Krok A: Pochodna jednej warstwy residualnej.}
Mamy sumę dwóch wyrazów: $\mathbf{h}^{(\ell)}$ (przechodzi ``na skróty'') i $f_\ell(\mathbf{h}^{(\ell)})$ (poprawka).
Pochodna sumy = suma pochodnych:
\[
\frac{\partial \mathbf{h}^{(\ell+1)}}{\partial \mathbf{h}^{(\ell)}}
= \underbrace{\frac{\partial \mathbf{h}^{(\ell)}}{\partial \mathbf{h}^{(\ell)}}}_{= \; 1 \;\text{(skip connection!)}}
+ \frac{\partial f_\ell}{\partial \mathbf{h}^{(\ell)}}
= 1 + \frac{\partial f_\ell}{\partial \mathbf{h}^{(\ell)}}
\]

\textbf{Krok B: Składamy 3 warstwy} (chain rule -- mnożymy pochodne):
\begin{align*}
\frac{\partial \mathbf{h}^{(3)}}{\partial \mathbf{h}^{(0)}}
&= \frac{\partial \mathbf{h}^{(3)}}{\partial \mathbf{h}^{(2)}}
\cdot \frac{\partial \mathbf{h}^{(2)}}{\partial \mathbf{h}^{(1)}}
\cdot \frac{\partial \mathbf{h}^{(1)}}{\partial \mathbf{h}^{(0)}} \\[6pt]
&= \left(1 + \frac{\partial f_2}{\partial \mathbf{h}^{(2)}}\right)
\cdot \left(1 + \frac{\partial f_1}{\partial \mathbf{h}^{(1)}}\right)
\cdot \left(1 + \frac{\partial f_0}{\partial \mathbf{h}^{(0)}}\right)
\end{align*}

\textbf{Krok C: Uogólnienie na 12 warstw} -- zapisujemy zwięźle jako $\prod$:
\[
\boxed{
\frac{\partial \mathcal{L}}{\partial \mathbf{h}^{(0)}}
= \prod_{\ell=0}^{11} \left(1 + \frac{\partial f_\ell}{\partial \mathbf{h}^{(\ell)}}\right)
}
\]

\textbf{Krok D: Dlaczego to działa?}
Nawet jeśli $\frac{\partial f_\ell}{\partial \mathbf{h}^{(\ell)}} \approx 0$ (warstwa ``nic nie robi''):
\[
\prod_{\ell=0}^{11} (1 + 0) = 1^{12} = 1 \quad \text{-- gradient \textbf{przepływa bez strat!}}
\]

\vspace{6pt}
\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|l|c|c|}
\hline
& \textbf{Bez residuali} & \textbf{Z residualami} \\
\hline
Wzór na gradient & $\displaystyle\prod \frac{\partial f_\ell}{\partial h}$ & $\displaystyle\prod \!\left(1 + \frac{\partial f_\ell}{\partial h}\right)$ \\[8pt]
\hline
Jeśli $\frac{\partial f_\ell}{\partial h} = 0.5$ & $0.5^{12} = 0.00024$ & $1.5^{12} = 129.7$ \\
\hline
Jeśli $\frac{\partial f_\ell}{\partial h} = 0$ & $0^{12} = 0$ (martwa sieć) & $1^{12} = 1$ (OK!) \\
\hline
\end{tabular}
\end{center}
\end{tcolorbox}

% -- ANALOGIA --
\begin{tcolorbox}[breakable, colback=yellow!5, colframe=orange!70!black, title=\textbf{Analogia: autostrada z $12$ stacjami}]

Wyobraź sobie \textbf{autostradę} prowadzącą sygnał od wejścia do wyjścia:

\begin{center}
\begin{tabular}{ccccccc}
$\mathbf{h}^{(0)}$ & $\xrightarrow{\text{autostrada}}$ & $\mathbf{h}^{(1)}$
& $\xrightarrow{\text{autostrada}}$ & $\cdots$
& $\xrightarrow{\text{autostrada}}$ & $\mathbf{h}^{(12)}$ \\[4pt]
& $\uparrow$ \small{+$\Delta_1$} & & $\uparrow$ \small{+$\Delta_2$} & & $\uparrow$ \small{+$\Delta_{12}$} & \\[2pt]
& \fbox{\small{Blok 1}} & & \fbox{\small{Blok 2}} & & \fbox{\small{Blok 12}} & \\
\end{tabular}
\end{center}

\begin{itemize}[leftmargin=2em]
\item \textbf{Autostrada}: oryginalny sygnał \textbf{zawsze przepływa} od $\mathbf{h}^{(0)}$ do $\mathbf{h}^{(12)}$.
\item \textbf{Stacje} (bloki): każda dodaje swoją poprawkę $\Delta_\ell$, ale \textbf{nie blokuje} ruchu.
\item Nawet jeśli stacja ``zepsuje się'' ($\Delta_\ell \approx 0$), sygnał jedzie dalej.
\item Gradient wraca tą samą autostradą -- ma \textbf{gwarantowaną drogę} do pierwszych warstw.
\end{itemize}

\textbf{Bez residuali}: każda stacja to \textbf{bramka} -- sygnał musi przejść przez każdą.
Jeśli jedna bramka blokuje, wszystko staje.
\end{tcolorbox}

% -- PODSUMOWANIE PEŁNEGO BLOKU --
\begin{tcolorbox}[breakable, colback=blue!3, colframe=blue!70!black, title=\textbf{Podsumowanie: co robi jeden blok Transformera}]
\begin{enumerate}[leftmargin=2em]
\item \textbf{LayerNorm} -- stabilizuje wartości
\item \textbf{Self-Attention} -- tokeny ``rozmawiają'' ze sobą $\rightarrow$ poprawka $\Delta_1$
\item \textbf{Residual} -- dodaj $\Delta_1$ do wejścia: $\mathbf{h}' = \mathbf{h} + \Delta_1$
\item \textbf{LayerNorm} -- ponowna stabilizacja
\item \textbf{MLP} -- każdy token przetwarzany osobno $\rightarrow$ poprawka $\Delta_2$
\item \textbf{Residual} -- dodaj $\Delta_2$: $\mathbf{h}^{(\ell+1)} = \mathbf{h}' + \Delta_2$
\end{enumerate}
Ten cykl powtarza się \textbf{12 razy} -- na końcu token [CLS] zawiera
\textbf{bogate, wielopoziomowe} cechy wizualne.
\end{tcolorbox}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/vit_pipeline.pdf}
\caption{Pełny pipeline ViT-Small. Lewo: przepływ danych od obrazu do embeddingu.
Prawo: szczegóły self-attention (Q, K, V) i MLP wewnątrz bloku.
Na dole: notatka o inicjalizacji losowej.}
\label{fig:pipeline}
\end{figure}

\subsection{Krok 6: Wyjście — embedding}

Po $12$ blokach token [CLS] ``widział'' cały obraz przez wielokrotne attention.
Jego wartość to \textbf{globalny embedding obrazu}:

\begin{equation}
\boxed{
\mathbf{z} = f_\theta(\mathbf{x}) = \mathbf{h}_{\text{CLS}}^{(12)} \in \mathbb{R}^{384}
}
\end{equation}

Co oznacza ten wzór?
\begin{itemize}[leftmargin=2em]
\item $\mathbf{x}$ -- wejściowy obraz (np.\ $224 \times 224$ pikseli),
\item $f_\theta$ -- cała sieć ViT z parametrami $\theta$ (wszystkie $\mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V,
      \mathbf{W}_{\text{patch}}, \mathbf{pos}, \ldots$),
\item $\mathbf{h}_{\text{CLS}}^{(12)}$ -- wartość tokenu [CLS] \textbf{po przejściu przez 12 bloków}
      (nie oryginalna wartość, tylko po 12-krotnym attention + MLP),
\item $\mathbf{z} \in \mathbb{R}^{384}$ -- \textbf{finalny embedding}: 384 liczb opisujących cały obraz.
\end{itemize}

Ten wektor $\mathbf{z}$ to wynik ViT -- jest podawany do LeJEPA (prediction loss + SIGReg)
podczas treningu, a po treningu używany do zadań downstream (klasyfikacja, k-NN).

\textbf{Pozostałe 196 tokenów} (patche) po 12 warstwach też mają bogate reprezentacje,
ale \textbf{nie są używane} -- tylko [CLS] jest ``oficjalnym'' wyjściem ViT.

\begin{keyinsight}[Po co nam embedding $\mathbf{z}$?]
Embedding $\mathbf{z}$ to \textbf{jedyne}, co zostaje z całego ViT po treningu ---
384 liczby, które kodują sens obrazu. Cała reszta (patche, attention, MLP)
to tylko \textit{maszyneria} produkująca ten wektor. Po co?

\begin{enumerate}[leftmargin=2em]
  \item \textbf{Downstream}: Po treningu ViT jest \textit{zamrożony}.
        Nowy obraz wchodzi, $\mathbf{z}$ wychodzi, a mały klasyfikator
        (linear probe / k-NN) podejmuje decyzję \textit{wyłącznie} na podstawie $\mathbf{z}$.
        Jeśli $\mathbf{z}$ jest dobry --- klasyfikacja działa.
        Jeśli $\mathbf{z}$ jest zły --- żaden klasyfikator nie pomoże.

  \item \textbf{Podobieństwo}: Dwa obrazy o podobnym znaczeniu
        (np.\ dwie klatki z fazy phaco) dadzą wektory $\mathbf{z}$ \textit{bliskie sobie}
        w przestrzeni $\mathbb{R}^{384}$; obrazy różne --- daleko od siebie.
        To pozwala np.\ wyszukiwać podobne klatki (retrieval) przez zwykłe
        liczenie odległości $\|\mathbf{z}_1 - \mathbf{z}_2\|$.

  \item \textbf{SIGReg}: Cały sens regularyzacji SIGReg polega na wymuszeniu,
        żeby zbiór wektorów $\{\mathbf{z}_1, \ldots, \mathbf{z}_N\}$ z batcha
        miał rozkład $\mathcal{N}(\mathbf{0}, \mathbf{I})$.
        Gdyby embeddingi się \textit{skolapsowały} ($\mathbf{z} = \text{const}$),
        żaden downstream by nie działał.
        Gdyby były \textit{anizotropowe} (rozciągnięte wzdłuż jednej osi),
        zadania wymagające separacji w ``ściśniętym'' kierunku byłyby skazane na porażkę.
        Izotropowy Gauss gwarantuje, że $\mathbf{z}$ jest \textbf{równie użyteczny}
        w każdym kierunku przestrzeni --- niezależnie od tego, jakie zadanie downstream dostaniemy.
\end{enumerate}

Krótko: $\mathbf{z}$ to \textbf{produkt końcowy}, a cały LeJEPA + SIGReg istnieje po to,
żeby ten produkt był jak najlepszej jakości.
\end{keyinsight}

\subsection{Trening od zera: od szumu do sensownych cech}

Gdy inicjalizujemy ViT od zera (\textbf{random init}):

\begin{enumerate}
  \item \textbf{Epoka 0}: Wszystkie macierze ($\mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V, \mathbf{W}_{\text{patch}}, \ldots$)
        mają \textbf{losowe wartości}.
        Attention jest \textbf{równomierny} — każdy token ``patrzy'' na wszystkich po równo.
        Embedding = \textbf{losowy szum} (bezużyteczny).

  \item \textbf{Epoki 1--20}: Sieć uczy się podstaw:
  \begin{itemize}
    \item $\mathbf{W}_{\text{patch}}$: jakie cechy wyciągać z pikseli (krawędzie, kolory),
    \item Embeddingi pozycyjne: geometria siatki patchy,
    \item Attention: na co warto patrzeć (a na co nie).
  \end{itemize}

  \item \textbf{Epoki 20--100}: Cechy stają się semantyczne:
  \begin{itemize}
    \item Attention skupia się na istotnych regionach (narzędzie, tęczówka),
    \item {[CLS]} zbiera sensowne podsumowanie sceny,
    \item Embedding $\mathbf{z}$ zaczyna \textbf{separować} różne typy scen.
  \end{itemize}
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/vit_attention.pdf}
\caption{\textbf{Lewo}: Mapa attention [CLS] — czerwone patche to te, na które [CLS] ``patrzy'' najsilniej.
\textbf{Środek}: Macierz attention (fragment) — wiersz = kto pyta, kolumna = kto odpowiada.
\textbf{Prawo}: Krzywe treningu od zera — loss maleje, attention staje się ostrzejszy,
jakość embeddingów rośnie.}
\label{fig:attention}
\end{figure}

\begin{warningbox}[Dlaczego self-supervised (bez etykiet)?]
W LeJEPA ViT nie ma etykiet (``to jest incision'', ``to jest phaco'').
Zamiast tego uczy się przez \textbf{predykcję widoków}:
\begin{itemize}
  \item Dostaje dwa ``widoki'' tego samego kadru (różne augmentacje),
  \item Musi nauczyć się, że te widoki \textbf{powinny mieć podobny embedding},
  \item To zmusza ViT do wyciągania \textbf{semantycznych cech} (nie pikseli),
  \item SIGReg dodatkowo wymusza, żeby embeddingi miały rozkład $\mathcal{N}(\mathbf{0}, \mathbf{I})$.
\end{itemize}
\end{warningbox}

