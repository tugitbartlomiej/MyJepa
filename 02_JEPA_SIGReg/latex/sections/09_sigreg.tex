\clearpage
%% ============================================================
\section{Jak to wymusić? SIGReg}
\label{sec:sigreg}
%% ============================================================

Skoro wiemy, że embeddingi powinny mieć rozkład $\mathcal{N}(\mathbf{0}, \mathbf{I}_K)$,
potrzebujemy mechanizmu, który to wymusi podczas treningu.

\subsection{Idea: test statystyczny jako loss}

Zamiast heurystyk (whitening, stop-gradient, teacher-student), LeJEPA używa
\textbf{testu hipotez}:
\begin{equation}
H_0: P_\theta = \mathcal{N}(\mathbf{0}, \mathbf{I}_K)
\quad \text{vs.} \quad
H_1: P_\theta \neq \mathcal{N}(\mathbf{0}, \mathbf{I}_K)
\end{equation}

Problem: testowanie w $K$ wymiarach ($K = 128, 384, \ldots$) jest obliczeniowo trudne.

\subsection{Sketching: redukcja do testów 1D}

Kluczowy trik: zamiast testować w $\mathbb{R}^K$, rzutujemy na losowe kierunki $\mathbf{a} \in \mathbb{S}^{K-1}$:

\begin{equation}
\mathbf{a}^\top \mathbf{z} \sim \mathcal{N}(0, 1) \quad \forall\, \mathbf{a}
\quad \iff \quad
\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_K)
\end{equation}

To wynika z \textbf{Lematu Craméra-Wolda}: rozkład wielowymiarowy jest jednoznacznie
określony przez wszystkie swoje rzuty jednowymiarowe.

\paragraph{Dlaczego $\mathcal{N}(0,1)$ na rzutach gwarantuje izotropowość?}

Powyższy wzór ($\iff$) to potężne twierdzenie. Rozbijmy je na dwie strony:

\begin{tcolorbox}[
  colback=lejepaBlue!8,
  colframe=lejepaBlue!80,
  fonttitle=\bfseries,
  title={Strona $\Rightarrow$: jeśli $\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_K)$, to rzuty są $\mathcal{N}(0,1)$},
  breakable,
]
Jeśli embeddingi mają izotropowy rozkład Gaussa, to rzut na \textit{dowolny} kierunek
$\mathbf{a}$ daje $\mathcal{N}(0,1)$. Dlaczego?

Rzut $u = \mathbf{a}^\top \mathbf{z}$ to kombinacja liniowa składowych Gaussa,
więc sam jest gaussowski. Wystarczy obliczyć jego średnią i~wariancję:
\begin{align}
\mathbb{E}[u] &= \mathbb{E}[\mathbf{a}^\top \mathbf{z}]
= \mathbf{a}^\top \underbrace{\mathbb{E}[\mathbf{z}]}_{= \,\mathbf{0}} = 0 \nonumber \\[4pt]
\text{Var}[u] &= \text{Var}[\mathbf{a}^\top \mathbf{z}]
= \mathbf{a}^\top \underbrace{\text{Cov}(\mathbf{z})}_{= \,\mathbf{I}_K} \mathbf{a}
= \mathbf{a}^\top \mathbf{a}
= \underbrace{\|\mathbf{a}\|^2}_{= \,1} = 1 \nonumber
\end{align}
Średnia $= 0$, wariancja $= 1$, rozkład gaussowski $\Rightarrow$ $u \sim \mathcal{N}(0,1)$.

\textbf{Kluczowy moment:} wariancja wyszła $\mathbf{a}^\top \mathbf{I}_K \mathbf{a} = \|\mathbf{a}\|^2 = 1$
\textit{niezależnie od kierunku} $\mathbf{a}$.
To jest właśnie izotropowość --- macierz $\mathbf{I}_K$ traktuje wszystkie kierunki jednakowo.
\end{tcolorbox}

\begin{tcolorbox}[
  colback=lejepaRed!8,
  colframe=lejepaRed!80,
  fonttitle=\bfseries,
  title={Co by się stało, gdyby embeddingi NIE były izotropowe?},
  breakable,
]
Załóżmy, że embeddingi mają anizotropową macierz kowariancji, np.:
\[
\boldsymbol{\Sigma} = \begin{bmatrix} 4 & 0 \\ 0 & 0.25 \end{bmatrix}
\quad\text{(rozciągnięte wzdłuż $z_1$, ściśnięte wzdłuż $z_2$)}
\]

Teraz wariancja rzutu \textbf{zależy od kierunku}:
\begin{itemize}[leftmargin=2em]
  \item Kierunek $\mathbf{a} = (1, 0)$ (wzdłuż $z_1$):
        $\text{Var}[u] = \mathbf{a}^\top \boldsymbol{\Sigma}\, \mathbf{a}
        = (1,\; 0) \begin{bmatrix} 4 & 0 \\ 0 & 0.25 \end{bmatrix} \begin{bmatrix} 1 \\ 0 \end{bmatrix} = 4$
        \quad $\neq 1$ !
  \item Kierunek $\mathbf{a} = (0, 1)$ (wzdłuż $z_2$):
        $\text{Var}[u] = 0.25$
        \quad $\neq 1$ !
  \item Kierunek $\mathbf{a} = (1/\sqrt{2},\; 1/\sqrt{2})$ (po przekątnej):
        $\text{Var}[u] = 2.125$
        \quad $\neq 1$ !
\end{itemize}

Różne kierunki dają \textit{różne} wariancje $\Rightarrow$ rzuty \textit{nie} są $\mathcal{N}(0,1)$
$\Rightarrow$ test Eppsa-Pulleya to wykryje i~da duży loss.
\end{tcolorbox}

\begin{tcolorbox}[
  colback=lejepaGreen!8,
  colframe=lejepaGreen!80,
  fonttitle=\bfseries,
  title={Strona $\Leftarrow$ (Lemat Craméra-Wolda): jeśli WSZYSTKIE rzuty są $\mathcal{N}(0,1)$, to $\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_K)$},
  breakable,
]
To jest potężniejsze twierdzenie (i~trudniejsze do udowodnienia).
Mówi:

\textit{Jeśli rzut na \textbf{każdy} możliwy kierunek $\mathbf{a}$ daje
rozkład $\mathcal{N}(0,1)$, to jedynym rozkładem $K$-wymiarowym, który to spełnia,
jest $\mathcal{N}(\mathbf{0}, \mathbf{I}_K)$.}

Innymi słowy: nie ma ``podstępu'' --- nie istnieje żaden dziwny, nie-gaussowski rozkład
w~$\mathbb{R}^K$, który przypadkiem miałby wszystkie rzuty 1D gaussowskie.
Gaussowskość rzutów \textbf{jednoznacznie wymusza} gaussowskość całego rozkładu.

\textbf{Analogia:} Jeśli cień obiektu 3D jest okrągły z~\textit{każdej} strony
--- to obiekt \textit{musi} być kulą. Nie ma innego kształtu, który daje
okrągły cień ze~wszystkich kierunków.
\end{tcolorbox}

\begin{keyinsight}[Podsumowanie: dlaczego testujemy rzuty?]
\begin{enumerate}[leftmargin=2em, topsep=2pt]
  \item Chcemy: $\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_K)$ (izotropowy Gauss w~$K$ wymiarach).
  \item Lemat Craméra-Wolda mówi: wystarczy sprawdzić, że
        $\mathbf{a}^\top \mathbf{z} \sim \mathcal{N}(0,1)$ dla każdego kierunku $\mathbf{a}$.
  \item Jeśli jakikolwiek kierunek daje wariancję $\neq 1$ lub rozkład nie-gaussowski
        --- embeddingi \textit{nie} są izotropowe.
  \item SIGReg testuje $M = 1024$ losowych kierunków testem Eppsa-Pulleya
        i~minimalizuje odchylenia od $\mathcal{N}(0,1)$.
  \item Efekt: sieć jest ``pchana'' w~stronę $\mathcal{N}(\mathbf{0}, \mathbf{I}_K)$
        --- izotropowego Gaussa.
\end{enumerate}
\end{keyinsight}

%% ============================================================
\subsection{Co to jest rzut $u = \mathbf{a}^\top \mathbf{z}$?}
\label{sec:rzut}
%% ============================================================

W~powyższym wzorze pojawiły się trzy symbole: $u$, $\mathbf{a}$ i~$\mathbf{z}$.
Wyjaśnijmy \textit{dokładnie}, co każdy z~nich oznacza.

\subsubsection{$\mathbf{z}$ --- wektor embeddingu}

$\mathbf{z}$ to \textbf{wektor embeddingu} --- wynik działania sieci neuronowej (enkodera)
na jednym obrazie. Ma $K$ wymiarów (liczb):
\[
\mathbf{z} = \begin{bmatrix} z_1 \\ z_2 \\ \vdots \\ z_K \end{bmatrix}
\in \mathbb{R}^K
\]

W~LeJEPA typowo $K = 128$ (po projektorze MLP) lub $K = 384$ (surowy ViT CLS token).
Każdy obraz w~batchu daje swój wektor $\mathbf{z}_j$, więc z~batcha $N$ obrazów
dostajemy $N$ wektorów: $\mathbf{z}_1, \mathbf{z}_2, \ldots, \mathbf{z}_N$.

\textbf{Prosty przykład} ($K=3$, żebyśmy mogli to narysować):
\[
\mathbf{z} = \begin{bmatrix} 0.5 \\ -1.2 \\ 0.8 \end{bmatrix}
\quad \leftarrow \text{trzy liczby opisujące jeden obraz}
\]

\subsubsection{$\mathbf{a}$ --- losowy kierunek jednostkowy}

$\mathbf{a}$ to \textbf{losowy wektor kierunkowy} o~długości 1.
Też ma $K$ wymiarów (tyle samo co $\mathbf{z}$!):
\[
\mathbf{a} = \begin{bmatrix} a_1 \\ a_2 \\ \vdots \\ a_K \end{bmatrix}
\in \mathbb{R}^K, \qquad \|\mathbf{a}\| = \sqrt{a_1^2 + a_2^2 + \cdots + a_K^2} = 1
\]

Warunek $\|\mathbf{a}\| = 1$ oznacza, że $\mathbf{a}$ leży na \textbf{sferze jednostkowej}
$\mathbb{S}^{K-1}$. Co to takiego?

%% --- Co to jest sfera jednostkowa? ---
\paragraph{Co to jest sfera jednostkowa $\mathbb{S}^{K-1}$?}

\begin{tcolorbox}[
  colback=lejepaBlue!8,
  colframe=lejepaBlue!80,
  fonttitle=\bfseries,
  title={Definicja: sfera jednostkowa},
  breakable,
]
Sfera jednostkowa $\mathbb{S}^{K-1}$ to zbiór \textbf{wszystkich wektorów w~$\mathbb{R}^K$
o~długości dokładnie~1}:
\[
\mathbb{S}^{K-1} = \left\{\, \mathbf{a} \in \mathbb{R}^K \;:\; \|\mathbf{a}\| = 1 \,\right\}
\]

\textbf{Dlaczego $K\!-\!1$, a~nie $K$?}
Indeks górny oznacza \textit{wymiarowość samej sfery}, nie przestrzeni, w~której żyje:
\begin{itemize}[leftmargin=2em]
  \item $K=2$: $\mathbb{S}^1$ = \textbf{okrąg} --- krzywa 1-wymiarowa
        (żyje w~$\mathbb{R}^2$, ale sam okrąg to krzywa --- 1D).
  \item $K=3$: $\mathbb{S}^2$ = \textbf{sfera} --- powierzchnia 2-wymiarowa
        (żyje w~$\mathbb{R}^3$, ale sama powierzchnia kuli jest 2D).
  \item $K=128$: $\mathbb{S}^{127}$ = \textbf{hipersfera} 127-wymiarowa w~$\mathbb{R}^{128}$.
\end{itemize}

Ogólna reguła: warunek $\|\mathbf{a}\| = 1$ ``zabiera'' jeden stopień swobody,
więc sfera w~$\mathbb{R}^K$ ma wymiar $K - 1$.
\end{tcolorbox}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/unit_sphere.pdf}
\caption{\textbf{Lewo}: Dla $K=2$ sfera $\mathbb{S}^1$ to okrąg.
Każda strzałka to jeden losowy kierunek $\mathbf{a}$ --- wszystkie kończą się na okręgu
(bo $\|\mathbf{a}\|=1$).
\textbf{Środek}: Dla $K=3$ sfera $\mathbb{S}^2$ to powierzchnia kuli.
Kierunki $\mathbf{a}$ to strzałki od środka do powierzchni.
\textbf{Prawo}: Jak losujemy kierunek $\mathbf{a}$ --- trzy kroki.}
\label{fig:unit_sphere}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/unit_sphere_intuition.pdf}
\caption{\textbf{Lewo}: Zielone strzałki leżą na sferze ($\|\mathbf{a}\|=1$),
czerwone krzyżyki to wektory o~złej długości --- nie są na sferze.
\textbf{Prawo}: Dlaczego $\mathbb{S}^{K-1}$, a~nie $\mathbb{S}^K$
--- wymiarowość sfery jest o~1 mniejsza od wymiarowości przestrzeni.}
\label{fig:unit_sphere_intuition}
\end{figure}

%% --- Jak losujemy a? ---
\paragraph{Jak losujemy kierunek $\mathbf{a}$?}

Procedura jest prosta (3 kroki):
\begin{enumerate}[leftmargin=2em]
  \item \textbf{Losujemy} $K$ niezależnych liczb z~rozkładu $\mathcal{N}(0,1)$:
        \[
        \underbrace{v_1}_{\text{1 liczba}} ,\;\;
        \underbrace{v_2}_{\text{1 liczba}} ,\;\;
        \ldots ,\;\;
        \underbrace{v_K}_{\text{1 liczba}}
        \qquad\text{--- każda $v_k$ to osobna liczba z $\mathcal{N}(0,1)$}
        \]
        \textbf{Skąd się biorą te liczby?} Generuje je \textit{komputer} ---
        generator liczb pseudolosowych (w~PyTorch: \texttt{torch.randn(K)},
        w~NumPy: \texttt{np.random.randn(K)}).
        Rozkład $\mathcal{N}(0,1)$ oznacza, że:
        \begin{itemize}[leftmargin=2em, topsep=2pt]
          \item większość wylosowanych wartości będzie blisko~$0$ (np.\ między $-2$ a~$2$),
          \item wartości mogą być ujemne lub dodatnie (symetria wokół zera),
          \item bardzo duże $|v_k| > 3$ zdarzają się rzadko (poniżej $0.3\%$ szansy).
        \end{itemize}
        Te liczby są \textbf{losowe} --- za każdym razem inne.
        Nie są uczone, nie zależą od danych, nie mają gradientu.

        Razem tworzymy z~nich \textbf{wektor} o~$K$ elementach:
        $\mathbf{v} = (v_1, v_2, \ldots, v_K) \in \mathbb{R}^K$.
  \item \textbf{Obliczamy normę} (długość) tego wektora:
        $\|\mathbf{v}\| = \sqrt{v_1^2 + v_2^2 + \cdots + v_K^2}$ --- to jest \textit{jedna liczba}.
  \item \textbf{Normalizujemy} --- dzielimy każdy element wektora przez normę:
        $\mathbf{a} = \mathbf{v} \,/\, \|\mathbf{v}\|$,
        tzn.\ $a_k = v_k / \|\mathbf{v}\|$ dla $k = 1, \ldots, K$.
\end{enumerate}

\paragraph{Jak to wygląda w~praktyce? Konkretny przykład.}

W~kodzie to dosłownie \textbf{3 linijki}. Pokażmy dla $K = 5$
(w~LeJEPA będzie $K = 128$, ale zasada jest identyczna):

\begin{tcolorbox}[
  colback=black!3,
  colframe=black!50,
  fonttitle=\bfseries\ttfamily,
  title={Python / PyTorch},
  breakable,
]
\begin{verbatim}
import torch

K = 5
v = torch.randn(K)       # Krok 1: losujemy K liczb z N(0,1)
a = v / torch.norm(v)     # Kroki 2+3: normalizujemy
\end{verbatim}
\end{tcolorbox}

\textbf{Co dokładnie robi \texttt{torch.randn(5)}?}
Prosi komputer: ``daj mi 5 losowych liczb z~rozkładu Gaussa $\mathcal{N}(0,1)$''.
Za każdym uruchomieniem dostaniemy \textit{inne} liczby.

\begin{tcolorbox}[
  colback=lejepaGreen!8,
  colframe=lejepaGreen!80,
  fonttitle=\bfseries,
  title={Jak komputer generuje liczby z~rozkładu Gaussa?},
  breakable,
]
Komputer \textbf{nie} generuje Gaussa bezpośrednio. Robi to w~dwóch etapach:

\textbf{Etap~1: Liczby równomierne} $U_1, U_2 \in (0, 1)$.\\
To jest łatwe --- procesor ma wbudowany generator, który daje ``losowe'' liczby
równomiernie rozłożone między 0 a~1 (jak idealna kostka, ale ciągła).
Przykład: $U_1 = 0.374$, $U_2 = 0.821$.

\textbf{Etap~2: Transformacja Box-Mullera} --- klasyczny algorytm z~1958~roku
(G.E.P.~Box, M.E.~Muller, \textit{``A~Note on the Generation of Random Normal Deviates''},
The Annals of Mathematical Statistics).
Zamienia dwie liczby równomierne na dwie liczby gaussowskie:
\[
Z_1 = \sqrt{-2 \ln U_1} \cdot \cos(2\pi U_2), \qquad
Z_2 = \sqrt{-2 \ln U_1} \cdot \sin(2\pi U_2)
\]
Wynik: $Z_1$ i~$Z_2$ mają \textit{dokładnie} rozkład $\mathcal{N}(0,1)$.

\textbf{Dlaczego to działa?} Logarytm i~pierwiastek ``rozciągają'' równomierny rozkład
tak, że wartości blisko~$0$ stają się częste (środek dzwonu Gaussa),
a~wartości daleko od~$0$ stają się rzadkie (ogony dzwonu).
Cosinus i~sinus zapewniają symetrię (wartości ujemne i~dodatnie równie prawdopodobne).

\textbf{Przykład liczbowy:}
\[
U_1 = 0.374,\;\; U_2 = 0.821 \quad\Rightarrow\quad
Z_1 = \sqrt{-2 \ln 0.374} \cdot \cos(2\pi \cdot 0.821) = 1.40 \cdot 0.37 = 0.52
\]
Powtarzamy $K/2$ razy --- i~mamy $K$ liczb z~$\mathcal{N}(0,1)$.

\medskip
\textit{Uwaga:} W~praktyce PyTorch używa szybszego wariantu (algorytm Ziggurat),
ale idea jest ta sama: równomierne $\to$ przekształcenie $\to$ gaussowskie.
Jako użytkownik nie musisz o~tym myśleć --- \texttt{torch.randn(K)} robi to za Ciebie.
\end{tcolorbox}

Na przykład:

\medskip

\textbf{Uruchomienie 1:}
\begin{align}
\text{Krok 1:}\quad \mathbf{v} &= (\underset{\uparrow}{0.49},\;\; \underset{\uparrow}{-0.13},\;\; \underset{\uparrow}{0.65},\;\; \underset{\uparrow}{0.86},\;\; \underset{\uparrow}{-1.23})
\quad\leftarrow \text{5 losowych liczb} \nonumber \\
\text{Krok 2:}\quad \|\mathbf{v}\| &= \sqrt{0.49^2 + 0.13^2 + 0.65^2 + 0.86^2 + 1.23^2}
= \sqrt{2.83} = 1.683 \nonumber \\
\text{Krok 3:}\quad \mathbf{a} &= \mathbf{v} / 1.683
= (0.291,\;\; {-0.077},\;\; 0.386,\;\; 0.511,\;\; {-0.731}) \nonumber
\end{align}
Sprawdzenie: $\|\mathbf{a}\| = \sqrt{0.291^2 + 0.077^2 + 0.386^2 + 0.511^2 + 0.731^2} = 1.0$ \;\checkmark

\medskip

\textbf{Uruchomienie 2} (inne losowe liczby!):
\begin{align}
\text{Krok 1:}\quad \mathbf{v} &= (-1.07,\;\; 0.54,\;\; -0.31,\;\; 0.02,\;\; 1.51)
\quad\leftarrow \text{zupełnie inne 5 liczb} \nonumber \\
\text{Krok 2:}\quad \|\mathbf{v}\| &= 1.951 \nonumber \\
\text{Krok 3:}\quad \mathbf{a} &= (-0.548,\;\; 0.277,\;\; {-0.159},\;\; 0.010,\;\; 0.774) \nonumber
\end{align}

\begin{keyinsight}[Co się tu dzieje --- podsumowanie]
\begin{enumerate}[leftmargin=2em, topsep=2pt]
  \item Komputer ``rzuca kostką'' $K$ razy --- za każdym razem dostaje jedną losową
        liczbę z~$\mathcal{N}(0,1)$ (zwykle między $-2$ a~$2$).
  \item Te $K$ liczb składamy w~wektor $\mathbf{v}$.
  \item Wektor $\mathbf{v}$ ma jakąś długość (normę) --- dzielimy przez nią,
        żeby dostać wektor $\mathbf{a}$ o~długości dokładnie~1.
  \item Wynik: losowy kierunek $\mathbf{a}$ na sferze $\mathbb{S}^{K-1}$.
  \item Za każdym uruchomieniem kierunek jest \textit{inny} ---
        bo liczby $v_k$ są inne.
  \item W~SIGReg robimy to $M = 1024$ razy, dostając 1024 różnych kierunków.
\end{enumerate}
\end{keyinsight}

\begin{warningbox}[Uwaga: jaki to rozkład? Czy Gauss zawsze jest izotropowy?]
W~kroku~1 losujemy $K$ \textbf{oddzielnych, niezależnych} liczb, każdą z~\textit{jednowymiarowego}
Gaussa $\mathcal{N}(0,1)$. Każda $v_k$ to \textit{jedna} liczba (skalar).
Ale złożone razem w~wektor $\mathbf{v} = (v_1, \ldots, v_K)$ dają wektor o~$K$ wymiarach.

Ale ponieważ są niezależne --- to jest \textbf{dokładnie to samo}, co wylosowanie jednego
wektora z~$K$-wymiarowego Gaussa $\mathcal{N}(\mathbf{0}, \mathbf{I}_K)$:
\[
\underbrace{v_1 \sim \mathcal{N}(0,1), \;\; v_2 \sim \mathcal{N}(0,1), \;\; \ldots, \;\; v_K \sim \mathcal{N}(0,1)}_{\text{$K$ niezależnych losowań 1D}}
\quad\iff\quad
\underbrace{\mathbf{v} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_K)}_{\text{jedno losowanie $K$-wymiarowe}}
\]

To działa, bo macierz kowariancji $\mathbf{I}_K$ (macierz jednostkowa) oznacza:
każdy wymiar ma wariancję~1, i~wymiary są \textit{niezależne} (korelacja~= 0).

\bigskip

\textbf{Czy rozkład Gaussa zawsze jest izotropowy? NIE!}

Rozkład Gaussa wielowymiarowy $\mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$
może mieć \textit{dowolną} macierz kowariancji $\boldsymbol{\Sigma}$:
\begin{itemize}[leftmargin=2em]
  \item $\boldsymbol{\Sigma} = \mathbf{I}$ --- \textbf{izotropowy}: okręgi/sfery,
        \textit{żaden kierunek nie jest uprzywilejowany}.
  \item $\boldsymbol{\Sigma} = \text{diag}(\sigma_1^2, \ldots, \sigma_K^2)$ --- \textbf{anizotropowy}:
        elipsy/elipsoidy, różne wariancje w~różnych kierunkach.
  \item $\boldsymbol{\Sigma}$ z~elementami pozadiagonalnymi $\neq 0$ --- \textbf{skorelowany}:
        elipsy obrócone pod kątem.
\end{itemize}

\textbf{Dlaczego tu potrzebujemy izotropowego?}
Bo chcemy, żeby losowy kierunek $\mathbf{a}$ był \textbf{równomierny} na sferze ---
żaden kierunek nie może być bardziej prawdopodobny niż inny.
Gdybyśmy użyli anizotropowego Gaussa (np.\ $\sigma_1^2 = 10$, $\sigma_2^2 = 0.1$),
to po normalizacji kierunki byłyby ``skupione'' wokół osi~$z_1$
--- a~my byśmy testowali głównie ten jeden kierunek, pomijając inne.

Izotropowy Gauss $\mathcal{N}(\mathbf{0}, \mathbf{I}_K)$ gwarantuje, że po normalizacji
$\mathbf{a} = \mathbf{v}/\|\mathbf{v}\|$ dostajemy \textbf{rozkład jednostajny na sferze}
$\mathbb{S}^{K-1}$.
\end{warningbox}

\textbf{Przykład} ($K=3$):
\[
\mathbf{v} = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}
\quad\Rightarrow\quad
\|\mathbf{v}\| = \sqrt{1^2 + 1^2 + 1^2} = \sqrt{3}
\quad\Rightarrow\quad
\mathbf{a} = \frac{\mathbf{v}}{\|\mathbf{v}\|} = \begin{bmatrix} 1/\sqrt{3} \\ 1/\sqrt{3} \\ 1/\sqrt{3} \end{bmatrix}
\approx \begin{bmatrix} 0.577 \\ 0.577 \\ 0.577 \end{bmatrix}
\]
Sprawdzenie: $\|\mathbf{a}\| = \sqrt{3 \cdot (1/\sqrt{3})^2} = \sqrt{3 \cdot 1/3} = 1$ \;\checkmark

%% --- Rola a w SIGReg ---
\paragraph{Rola $\mathbf{a}$ w~SIGReg --- kiedy losujemy?}

\begin{tcolorbox}[
  colback=lejepaBlue!8,
  colframe=lejepaBlue!80,
  fonttitle=\bfseries,
  title={Kiedy losujemy kierunki $\mathbf{a}$? Raz czy wiele razy?},
  breakable,
]
W~oryginalnym LeJEPA kierunki losujemy \textbf{raz, przed rozpoczęciem treningu},
i~zostają \textbf{takie same} przez cały trening --- przez wszystkie epoki i~wszystkie batche:

\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{lcc}
\toprule
\textbf{Pytanie} & \textbf{Odpowiedź} \\
\midrule
Czy $\mathbf{a}$ zmienia się między epokami? & \textbf{Nie} --- te same kierunki \\
Czy $\mathbf{a}$ zmienia się między batchami? & \textbf{Nie} --- te same kierunki \\
Ile kierunków losujemy? & $M \approx 1024$ \\
Czy sieć ``uczy'' kierunki $\mathbf{a}$? & \textbf{Nie} --- brak gradientu \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Dlaczego to wystarczy?}
\begin{itemize}[leftmargin=2em, topsep=2pt]
  \item $M = 1024$ losowych kierunków w~$\mathbb{R}^{128}$ daje bardzo dobre pokrycie sfery
        $\mathbb{S}^{127}$ --- wystarczająco gęste, żeby wykryć anizotropię w~każdym kierunku.
  \item Lemat Craméra-Wolda teoretycznie wymaga \textit{wszystkich} kierunków
        ($\infty$), ale w~praktyce 1024 to wystarczające przybliżenie.
  \item Stałe kierunki mają zaletę: loss SIGReg jest \textbf{deterministyczny}
        (przy tym samym batchu daje tę samą wartość) --- to stabilizuje trening.
\end{itemize}

\textbf{Analogia:} Wyobraź sobie, że chcesz sprawdzić, czy piłka jest idealnie okrągła.
Mierzysz ją linijką w~1024 losowych kierunkach.
Nie musisz zmieniać kierunków przy każdym pomiarze ---
te same 1024 kierunków wystarczą, żeby wykryć każdą deformację.

\medskip

\textbf{Kod:}
\begin{verbatim}
# Przed treningiem (raz!):
A = torch.randn(M, K)                 # M=1024 wektorów, każdy K-wymiarowy
A = A / torch.norm(A, dim=1, keepdim=True)  # normalizacja

# W pętli treningowej (każdy batch):
for epoch in range(100):
    for batch in dataloader:
        z = encoder(batch)        # embeddingi [N, K]
        u = z @ A.T               # rzuty [N, M] - używamy TYCH SAMYCH A!
        loss = epps_pulley(u)      # test na każdym z M kierunków
\end{verbatim}
\end{tcolorbox}

\textbf{Geometrycznie:} $\mathbf{a}$ to po prostu \textit{kierunek w~przestrzeni} ---
strzałka wskazująca ``w~którą stronę patrzymy'' na nasze dane.

\subsubsection{$u = \mathbf{a}^\top \mathbf{z}$ --- iloczyn skalarny (rzut)}

Teraz najważniejsze: \textbf{iloczyn skalarny} $\mathbf{a}^\top \mathbf{z}$:
\begin{equation}
u = \mathbf{a}^\top \mathbf{z}
= a_1 \cdot z_1 + a_2 \cdot z_2 + \cdots + a_K \cdot z_K
= \sum_{k=1}^{K} a_k z_k
\label{eq:projection}
\end{equation}

\textbf{To jest po prostu suma iloczynów element-po-elemencie.}
Wynik to \textit{jedna liczba} $u \in \mathbb{R}$ --- nie wektor!
Z~wektora o~$K$ wymiarach zrobiliśmy jedną liczbę.

\textbf{Pełny przykład liczbowy} ($K=3$):
\begin{align}
u &= \mathbf{a}^\top \mathbf{z}
= a_1 \cdot z_1 + a_2 \cdot z_2 + a_3 \cdot z_3 \nonumber \\
&= 0.577 \cdot 0.5 + 0.577 \cdot (-1.2) + 0.577 \cdot 0.8 \nonumber \\
&= 0.289 + (-0.693) + 0.462 \nonumber \\
&= 0.058 \label{eq:projection_example}
\end{align}

Z~wektora 3D $(0.5, -1.2, 0.8)$ dostaliśmy jedną liczbę: $u = 0.058$.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/projection_example.pdf}
\caption{Przykład liczbowy: z~wektora $\mathbf{z} \in \mathbb{R}^3$ i~kierunku
$\mathbf{a} \in \mathbb{R}^3$ obliczamy jedną liczbę $u = \mathbf{a}^\top \mathbf{z}$.}
\label{fig:projection_example}
\end{figure}

\subsubsection{Geometryczna interpretacja --- rzut na prostą}

Iloczyn skalarny $u = \mathbf{a}^\top \mathbf{z}$ ma prostą interpretację geometryczną:

\textbf{$u$ to długość cienia} --- gdybyś postawił wektor $\mathbf{z}$ w~przestrzeni
i~poświecił na niego latarką z~kierunku prostopadłego do $\mathbf{a}$,
to cień $\mathbf{z}$ na osi $\mathbf{a}$ miałby długość $u$.

Formalnie: $u$ to \textbf{rzut ortogonalny} $\mathbf{z}$ na kierunek $\mathbf{a}$.
Punkt $u \cdot \mathbf{a}$ to najbliższy punkt na \textit{prostej wyznaczonej przez $\mathbf{a}$}
do~punktu~$\mathbf{z}$.

\begin{tcolorbox}[
  colback=lejepaGreen!8,
  colframe=lejepaGreen!80,
  fonttitle=\bfseries,
  title={Notacja: $\{\,t\,\mathbf{a} : t \in \mathbb{R}\,\}$ --- co to znaczy?},
  breakable,
]
Zapis $\{\,t\,\mathbf{a} : t \in \mathbb{R}\,\}$ to \textbf{notacja opisowa zbioru}
(ang.\ \textit{set-builder notation}). Czytamy ją tak:

\[
\underbrace{\{}_{\text{zbiór}}
\;\underbrace{t\,\mathbf{a}}_{\text{elementów postaci }t \cdot \mathbf{a}}
\;\underbrace{:}_{\text{takich, że}}
\;\underbrace{t \in \mathbb{R}}_{\text{$t$ jest dowolną liczbą rzeczywistą}}
\underbrace{\}}_{\text{koniec opisu}}
\]

Czyli: ``zbiór wszystkich punktów $t \cdot \mathbf{a}$, gdzie $t$ przebiega
od~$-\infty$ do~$+\infty$''. Geometrycznie to jest \textbf{prosta}
przechodząca przez początek układu współrzędnych w~kierunku $\mathbf{a}$:

\medskip
\begin{center}
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{rcl}
$t = 0$ & $\Rightarrow$ & punkt $\mathbf{0} = (0, 0, \ldots)$ --- początek układu \\
$t = 1$ & $\Rightarrow$ & punkt $\mathbf{a}$ --- sam wektor kierunkowy \\
$t = 2$ & $\Rightarrow$ & punkt $2\mathbf{a}$ --- dwa razy dalej w~tym samym kierunku \\
$t = -1$ & $\Rightarrow$ & punkt $-\mathbf{a}$ --- w~przeciwnym kierunku \\
$t = 0.5$ & $\Rightarrow$ & punkt $0.5\,\mathbf{a}$ --- w~połowie drogi do $\mathbf{a}$ \\
\end{tabular}
\end{center}

\medskip
Wszystkie te punkty leżą na jednej prostej ---
a~rzut $u$ mówi, \textit{w~którym miejscu} na tej prostej ``ląduje'' cień wektora $\mathbf{z}$.
\end{tcolorbox}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/projection_explanation.pdf}
\caption{\textbf{Lewo}: Embeddingi $\mathbf{z}_j$ w~2D.
\textbf{Środek}: Rzut na kierunek $\mathbf{a}$ --- każdy punkt ``spada'' na czerwoną prostą
(linia przerywana = prostopadła).
\textbf{Prawo}: Wynikowe wartości $u_j$ na osi liczbowej + histogram (wyjaśnienie niżej).}
\label{fig:projection_explanation}
\end{figure}

\paragraph{Po co histogram na prawym panelu?}

Na środkowym panelu zrzutowaliśmy $N = 8$ embeddingów na kierunek $\mathbf{a}$
i~dostaliśmy 8 liczb: $u_1, u_2, \ldots, u_8$.
Na prawym panelu robimy z~nimi dwie rzeczy:

\begin{enumerate}[leftmargin=2em]
  \item \textbf{Oś liczbowa} (górna część) --- pokazuje, \textit{gdzie} dokładnie
        wylądowała każda wartość $u_j$. Pomarańczowe kwadraty to poszczególne
        rzuty, podpisane liczbami (np.\ $u_3 = 0.8$, $u_5 = -1.1$).

  \item \textbf{Histogram} (dolna część) --- grupuje wartości $u_j$ w~``przedziały'' (bins)
        i~pokazuje, \textit{ile wartości} wpadło w~każdy przedział.
        Im wyższy słupek, tym więcej embeddingów ma rzut w~tym zakresie.

        \textbf{Po co?} Bo chcemy zobaczyć \textbf{kształt rozkładu} tych $N$ liczb.
        Histogram to najprostszy sposób na wizualizację rozkładu:
        \begin{itemize}[leftmargin=2em, topsep=2pt]
          \item Jeśli histogram wygląda jak \textbf{dzwonek} (dużo wartości blisko~0,
                mało daleko od~0) --- embeddingi w~tym kierunku wyglądają jak Gauss.
          \item Jeśli histogram jest \textbf{płaski}, \textbf{przesunięty} lub ma
                \textbf{dwa szczyty} --- embeddingi \textit{nie} są gaussowskie.
        \end{itemize}

  \item \textbf{Czarna krzywa} $\mathcal{N}(0,1)$ --- to nasz \textit{cel}.
        Pokazuje, jak powinien wyglądać idealny rozkład.
        Im bardziej histogram pokrywa się z~czarną krzywą, tym lepiej.
\end{enumerate}

\begin{keyinsight}[Histogram $\to$ test EP --- po co nam coś więcej niż histogram?]
Histogram to dobra \textit{wizualizacja} dla człowieka, ale zły \textit{loss} dla sieci:
\begin{itemize}[leftmargin=2em, topsep=2pt]
  \item Histogram jest ``schodkowy'' --- nie ma gładkiego gradientu
        (mała zmiana $u_j$ nie zmienia histogramu, dopóki punkt nie przeskoczy granicy binu).
  \item Wymaga sortowania danych --- $O(N \log N)$.
  \item Kształt zależy od wyboru binów (ile? jak szerokie?).
\end{itemize}
Dlatego SIGReg \textbf{nie} porównuje histogramów, lecz używa \textbf{funkcji charakterystycznych}
(sekcja~\ref{sec:sigreg}.4 dalej) --- mają gładki gradient, złożoność $O(N)$
i~nie wymagają żadnych arbitralnych wyborów.

\end{keyinsight}

\subsubsection{Od jednego kierunku do wielu --- dlaczego testujemy wiele $\mathbf{a}$?}

Jeden kierunek $\mathbf{a}$ daje nam jedną ``perspektywę'' na dane.
Ale różne kierunki mogą ujawnić różne problemy:
\begin{itemize}[leftmargin=2em]
\item Kierunek wzdłuż osi $z_1$ sprawdza, czy wariancja w~wymiarze 1 jest poprawna.
\item Kierunek pod kątem sprawdza, czy wymiary są nieskorelowane.
\item Tylko testując \textit{wiele} kierunków, możemy stwierdzić, że rozkład jest
      izotropowy w~\textit{wszystkich} wymiarach.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/projection_directions.pdf}
\caption{\textbf{Górny rząd}: Embeddingi izotropowe --- rzuty we wszystkich kierunkach
wyglądają jak $\mathcal{N}(0,1)$ (wariancja $\approx 1$).
\textbf{Dolny rząd}: Embeddingi anizotropowe (rozciągnięte wzdłuż $z_1$) ---
różne kierunki dają \textit{różne} wariancje (od $0.25$ do $4.0$).
SIGReg to wykrywa i~koryguje.}
\label{fig:projection_directions}
\end{figure}

\subsubsection{Podsumowanie: od obrazów do liczb 1D}

Cały pipeline wygląda tak:

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/projection_pipeline.pdf}
\caption{Pipeline SIGReg: batch obrazów $\to$ enkoder $\to$ embeddingi $K$-wymiarowe $\to$
rzut na losowy kierunek $\mathbf{a}$ $\to$ $N$ liczb 1D $\to$ test Eppsa-Pulleya $\to$ loss.}
\label{fig:projection_pipeline}
\end{figure}

\begin{keyinsight}[Dlaczego rzutujemy zamiast testować w~$K$ wymiarach?]
Test w~$K = 128$ wymiarach wymaga \textbf{astronomicznie dużo} danych
(tzw.\ ``klątwa wymiarowości'').
Rzut $u = \mathbf{a}^\top \mathbf{z}$ redukuje problem do 1D, gdzie test Eppsa-Pulleya
działa doskonale nawet z~małym batchem ($N = 64$).
Lemat Craméra-Wolda gwarantuje, że jeśli \textit{wszystkie} rzuty 1D
są $\mathcal{N}(0,1)$, to cały rozkład $K$-wymiarowy jest $\mathcal{N}(\mathbf{0}, \mathbf{I}_K)$.
W~praktyce losujemy $M \approx 1024$ kierunków --- to wystarczy.
\end{keyinsight}

%% ============================================================
\subsection{Test Eppsa-Pulleya na rzutach 1D}
%% ============================================================

Mamy rzut $u = \mathbf{a}^\top \mathbf{z}$ --- jedną liczbę dla każdego embeddingu.
Z~batcha $N$ embeddingów dostajemy $N$ takich liczb: $u_1, u_2, \ldots, u_N$.

\textbf{Pytanie:} czy te $N$ liczb wyglądają jak próbka z~$\mathcal{N}(0,1)$?

Żeby to sprawdzić, porównujemy dwa obiekty --- \textbf{funkcje charakterystyczne} (CF).
Zanim przejdziemy do wzorów SIGReg, wyjaśnijmy dokładnie, czym jest CF
i~skąd się bierze --- od samych podstaw.

%% ----- Element 1: Motywacja --- po co nam CF? -----
\subsubsection{Element 1: Po co nam funkcja charakterystyczna?}

Mamy $N$ liczb $u_1, \ldots, u_N$ (rzuty embeddingów).
Chcemy odpowiedzieć na pytanie: \textit{``Czy te liczby pochodzą z~rozkładu $\mathcal{N}(0,1)$?''}

Moglibyśmy narysować histogram i~``na oko'' porównać z~krzywą dzwonową ---
ale to nie jest precyzyjne i~nie nadaje się jako funkcja straty do treningu sieci.
Potrzebujemy narzędzia, które:
\begin{enumerate}[leftmargin=2em]
  \item \textbf{jednoznacznie identyfikuje} rozkład (nie tylko średnią czy wariancję,
        ale \textit{cały kształt}),
  \item daje \textbf{liczbę}, którą można zróżniczkować (gradient dla optymalizatora),
  \item działa dobrze nawet z~\textbf{małą próbką} ($N = 64$).
\end{enumerate}
Takim narzędziem jest właśnie \textbf{funkcja charakterystyczna}.

%% ----- Element 2: Liczby zespolone --- minimum potrzebne -----
\subsubsection{Element 2: Minimum o~liczbach zespolonych}

Funkcja charakterystyczna używa liczb zespolonych, więc przypomnijmy
absolutne minimum:

\begin{tcolorbox}[colback=gray!5, colframe=gray!60, breakable,
  title={Liczby zespolone --- co musimy wiedzieć}]

\textbf{Jednostka urojona:}
\[
i = \sqrt{-1}, \qquad i^2 = -1
\]
Liczba zespolona to para: $z = a + bi$, gdzie $a$ to \textbf{część rzeczywista},
$b$ to \textbf{część urojona}.

\textbf{Przykłady:}
\begin{itemize}[leftmargin=2em]
  \item $3 + 2i$ --- część rzeczywista $= 3$, część urojona $= 2$
  \item $5$ --- to też liczba zespolona (z~częścią urojoną $= 0$)
  \item $4i$ --- część rzeczywista $= 0$, część urojona $= 4$
\end{itemize}

\textbf{Geometrycznie:} Liczba zespolona to punkt na płaszczyźnie.
Oś pozioma to część rzeczywista, oś pionowa to część urojona:

\begin{center}
\begin{tikzpicture}[scale=0.9]
  \draw[->] (-1.5,0) -- (4.5,0) node[right] {Re (rzeczywista)};
  \draw[->] (0,-1.5) -- (0,3.5) node[above] {Im (urojona)};
  \fill[lejepaBlue] (3,2) circle (3pt) node[above right] {$3 + 2i$};
  \draw[dashed, gray] (3,0) node[below] {$3$} -- (3,2) -- (0,2) node[left] {$2$};
  \fill[lejepaRed] (0,0) circle (2pt);
\end{tikzpicture}
\end{center}

\textbf{Moduł} (odległość od zera):
$|a + bi| = \sqrt{a^2 + b^2}$

\end{tcolorbox}

%% ----- Element 3: Wzór Eulera -----
\subsubsection{Element 3: Wzór Eulera --- klucz do CF}

Najważniejszy wzór, na którym opiera się cała teoria CF:

\begin{tcolorbox}[colback=blue!5, colframe=blue!60!black, breakable,
  title={Wzór Eulera (Leonhard Euler, 1748)}]
\begin{equation}
e^{i\theta} = \cos\theta + i\sin\theta
\label{eq:euler}
\end{equation}
gdzie $\theta$ to dowolna liczba rzeczywista (kąt w~radianach).
\end{tcolorbox}

Skąd się bierze ten wzór? I~dlaczego kreśli okrąg?
Wyprowadźmy to krok po kroku --- dla czystej przyjemności matematyki.

%% --- Wyprowadzenie wzoru Eulera ---
\begin{tcolorbox}[colback=violet!5, colframe=violet!70!black, breakable,
  title={Wyprowadzenie wzoru Eulera z~szeregów Taylora}]

\textbf{Punkt wyjścia:} trzy funkcje, które znamy z~analizy matematycznej,
mają rozwinięcia w~\textbf{szereg Taylora} (nieskończoną sumę potęg):

\medskip
\textbf{Krok 1.} Szereg Taylora dla $e^x$ (wokół $x = 0$):
\[
e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \frac{x^4}{4!} + \frac{x^5}{5!} + \cdots
= \sum_{n=0}^{\infty} \frac{x^n}{n!}
\]
(Przypomnienie: $n! = 1 \cdot 2 \cdot 3 \cdots n$, np.\ $4! = 24$.)

\medskip
\textbf{Krok 2.} Szeregi Taylora dla $\cos$ i~$\sin$:
\[
\cos\theta = 1 - \frac{\theta^2}{2!} + \frac{\theta^4}{4!} - \frac{\theta^6}{6!} + \cdots
\qquad\text{(same \textbf{parzyste} potęgi, znaki naprzemienne)}
\]
\[
\sin\theta = \theta - \frac{\theta^3}{3!} + \frac{\theta^5}{5!} - \frac{\theta^7}{7!} + \cdots
\qquad\text{(same \textbf{nieparzyste} potęgi, znaki naprzemienne)}
\]

\medskip
\textbf{Krok 3.} Wstawiamy $x = i\theta$ do szeregu $e^x$:
\[
e^{i\theta} = 1 + (i\theta) + \frac{(i\theta)^2}{2!} + \frac{(i\theta)^3}{3!}
+ \frac{(i\theta)^4}{4!} + \frac{(i\theta)^5}{5!} + \cdots
\]

\textbf{Krok 4.} Obliczamy kolejne potęgi $i$:
\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{c c l}
\toprule
Potęga & Wartość & Komentarz \\
\midrule
$i^0$ & $1$   & (każda liczba do potęgi $0$ to $1$) \\
$i^1$ & $i$   &  \\
$i^2$ & $-1$  & (definicja $i$) \\
$i^3$ & $-i$  & ($i^2 \cdot i = -1 \cdot i$) \\
$i^4$ & $1$   & ($i^2 \cdot i^2 = (-1)(-1) = 1$ --- \textbf{cykl się powtarza!}) \\
$i^5$ & $i$   & jak $i^1$ \\
$i^6$ & $-1$  & jak $i^2$ \\
\bottomrule
\end{tabular}
\end{center}
Cykl: $1, \; i, \; -1, \; -i, \; 1, \; i, \; -1, \; -i, \; \ldots$

\medskip
\textbf{Krok 5.} Wstawiamy potęgi $i$ do każdego wyrazu:
\begin{align*}
e^{i\theta} &= \underbrace{1}_{i^0}
+ \underbrace{i\theta}_{i^1}
+ \underbrace{\frac{i^2\theta^2}{2!}}_{=\, -\frac{\theta^2}{2!}}
+ \underbrace{\frac{i^3\theta^3}{3!}}_{=\, -\frac{i\theta^3}{3!}}
+ \underbrace{\frac{i^4\theta^4}{4!}}_{=\, +\frac{\theta^4}{4!}}
+ \underbrace{\frac{i^5\theta^5}{5!}}_{=\, +\frac{i\theta^5}{5!}}
+ \cdots
\end{align*}

\textbf{Krok 6.} Grupujemy --- wyrazy \textbf{bez $i$} osobno, wyrazy \textbf{z~$i$} osobno:
\[
e^{i\theta} =
\underbrace{\left(1 - \frac{\theta^2}{2!} + \frac{\theta^4}{4!} - \cdots\right)}_{\text{to jest }\cos\theta\text{!}}
+ \;i\;\underbrace{\left(\theta - \frac{\theta^3}{3!} + \frac{\theta^5}{5!} - \cdots\right)}_{\text{to jest }\sin\theta\text{!}}
\]

\medskip
\begin{center}
\fbox{\Large $e^{i\theta} = \cos\theta + i\sin\theta \qquad\checkmark$}
\end{center}

Wzór wyłania się sam --- wystarczyło wstawić $i\theta$ do szeregu $e^x$
i~pogrupować wyrazy!
\end{tcolorbox}

%% --- Dowód, że to okrąg ---
\begin{tcolorbox}[colback=green!5, colframe=green!60!black, breakable,
  title={Dowód, że $e^{i\theta}$ kreśli okrąg jednostkowy}]

Wiemy, że $e^{i\theta} = \cos\theta + i\sin\theta$.
Na płaszczyźnie zespolonej ten punkt ma współrzędne:
\[
(\underbrace{\cos\theta}_{\text{Re}},\; \underbrace{\sin\theta}_{\text{Im}})
\]

\textbf{Pytanie:} Jaką figurę kreślą te punkty, gdy $\theta$ przebiega od $0$ do $2\pi$?

\medskip
\textbf{Odległość od zera} (moduł):
\[
|e^{i\theta}| = \sqrt{(\cos\theta)^2 + (\sin\theta)^2}
= \sqrt{\underbrace{\cos^2\theta + \sin^2\theta}_{= \; 1 \text{ (tożsamość trygonometryczna!)}}}
= \sqrt{1} = 1
\]

Odległość od zera wynosi \textbf{zawsze 1}, niezależnie od~$\theta$.

\medskip
A~co to jest zbiór punktów w~odległości dokładnie $1$ od punktu $(0,0)$?

To jest \textbf{okrąg o~promieniu~1} ze środkiem w~zerze --- \textbf{okrąg jednostkowy}!

\[
\boxed{|e^{i\theta}| = 1 \quad\text{dla każdego }\theta
\qquad\Longrightarrow\qquad
e^{i\theta}\text{ leży na okręgu jednostkowym}}
\]

\medskip
\textbf{Co więcej:}
\begin{itemize}[leftmargin=2em]
  \item Gdy $\theta = 0$: \quad $e^{i\cdot 0} = \cos 0 + i\sin 0 = 1 + 0 = 1$
        \quad(start na prawo)
  \item Gdy $\theta$ rośnie: punkt przesuwa się \textbf{w~górę} i~w~lewo
        (kierunek $\sin$ rośnie)
  \item Gdy $\theta = 2\pi$: \quad $e^{i\cdot 2\pi} = \cos 2\pi + i\sin 2\pi = 1$
        \quad(wraca na start --- pełen obrót!)
\end{itemize}

\begin{center}
\begin{tikzpicture}[scale=1.4]
  \draw[gray!30] (0,0) circle (1);
  \draw[->] (-1.5,0) -- (1.7,0) node[right] {Re};
  \draw[->] (0,-1.5) -- (0,1.7) node[above] {Im};
  % Ścieżka z strzałkami
  \draw[lejepaBlue, thick, ->, >=stealth,
    postaction={decorate, decoration={markings,
      mark=at position 0.15 with {\arrow{>}},
      mark=at position 0.40 with {\arrow{>}},
      mark=at position 0.65 with {\arrow{>}},
      mark=at position 0.90 with {\arrow{>}}
    }}]
    (1,0) arc (0:350:1);
  % Punkty
  \fill[lejepaRed] (1,0) circle (2.5pt)
    node[below right] {$\theta\!=\!0$: start};
  \fill[lejepaBlue] ({cos(90)},{sin(90)}) circle (2pt)
    node[above right] {$\theta\!=\!\frac{\pi}{2}$};
  \fill[lejepaBlue] ({cos(180)},{sin(180)}) circle (2pt)
    node[above left] {$\theta\!=\!\pi$};
  \fill[lejepaBlue] ({cos(270)},{sin(270)}) circle (2pt)
    node[below left] {$\theta\!=\!\frac{3\pi}{2}$};
  % Promień = 1
  \draw[dashed, gray, thick] (0,0) -- ({cos(45)},{sin(45)})
    node[midway, below right] {$r = 1$};
  % Etykieta
  \node[text width=4cm, align=center] at (2.8, 0)
    {$\theta$ rośnie\\$\Downarrow$\\punkt obraca się\\przeciwnie do\\zegara};
\end{tikzpicture}
\end{center}

\end{tcolorbox}

\textbf{Bonus --- najpiękniejszy wzór matematyki.}
Wstawiając $\theta = \pi$ do wzoru Eulera:
\[
e^{i\pi} = \cos\pi + i\sin\pi = -1 + 0 = -1
\]
Przenosząc na drugą stronę:
\begin{equation}
\boxed{e^{i\pi} + 1 = 0}
\label{eq:euler_identity}
\end{equation}
Ten wzór łączy pięć najważniejszych stałych matematyki
($e$, $i$, $\pi$, $1$, $0$) w~jednym równaniu.
Nazywamy go \textbf{tożsamością Eulera}
--- często uważaną za najpiękniejszy wzór w~całej matematyce.

\bigskip
\textbf{Co to znaczy?} Wyrażenie $e^{i\theta}$ to punkt na \textbf{okręgu jednostkowym}
(okrąg o~promieniu~1) na płaszczyźnie zespolonej:

\begin{center}
\begin{tikzpicture}[scale=1.6]
  % okrąg
  \draw[gray!40] (0,0) circle (1);
  \draw[->] (-1.4,0) -- (1.6,0) node[right] {Re};
  \draw[->] (0,-1.4) -- (0,1.6) node[above] {Im};
  % punkt
  \def\ang{40}
  \coordinate (P) at ({cos(\ang)},{sin(\ang)});
  \draw[thick, lejepaBlue, ->] (0,0) -- (P);
  \fill[lejepaBlue] (P) circle (1.5pt) node[above right]
    {$e^{i\theta} = \cos\theta + i\sin\theta$};
  % kąt
  \draw[lejepaRed, thick] (0.3,0) arc (0:\ang:0.3);
  \node[lejepaRed] at (0.45,0.15) {$\theta$};
  % rzuty
  \draw[dashed, gray] (P) -- ({cos(\ang)},0)
    node[below, black] {\small$\cos\theta$};
  \draw[dashed, gray] (P) -- (0,{sin(\ang)})
    node[left, black] {\small$\sin\theta$};
  % specjalne punkty
  \fill[gray] (1,0) circle (1pt) node[below right] {\small$e^{i\cdot 0}=1$};
  \fill[gray] (0,1) circle (1pt) node[above left] {\small$e^{i\pi/2}=i$};
  \fill[gray] (-1,0) circle (1pt) node[below left] {\small$e^{i\pi}=-1$};
  \fill[gray] (0,-1) circle (1pt) node[below left] {\small$e^{i\cdot 3\pi/2}=-i$};
\end{tikzpicture}
\end{center}

\textbf{Kluczowe obserwacje:}
\begin{itemize}[leftmargin=2em]
  \item Gdy $\theta$ rośnie, punkt $e^{i\theta}$ \textbf{obraca się} po okręgu
        (przeciwnie do zegara).
  \item Moduł jest zawsze $|e^{i\theta}| = \sqrt{\cos^2\theta + \sin^2\theta} = 1$ ---
        punkt nigdy nie oddala się od zera.
  \item $e^{i\theta}$ to \textbf{obrót o~kąt $\theta$} na płaszczyźnie zespolonej.
\end{itemize}

\textbf{Przykłady konkretne:}
\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{c c c c}
\toprule
$\theta$ & $\cos\theta$ & $\sin\theta$ & $e^{i\theta}$ \\
\midrule
$0$ & $1$ & $0$ & $1$ \\
$\pi/2 \approx 1{,}57$ & $0$ & $1$ & $i$ \\
$\pi \approx 3{,}14$ & $-1$ & $0$ & $-1$ \\
$2\pi \approx 6{,}28$ & $1$ & $0$ & $1$ (pełen obrót) \\
\bottomrule
\end{tabular}
\end{center}

%% ----- Element 4: Od Eulera do CF -----
\subsubsection{Element 4: Od wzoru Eulera do funkcji charakterystycznej}

Teraz zastosujmy wzór Eulera. Weźmy zmienną losową $X$
(np.\ nasze rzuty $u$) i~wpiszmy $\theta = tx$:
\[
e^{itx} = \cos(tx) + i\sin(tx)
\]

\begin{tcolorbox}[colback=gray!5, colframe=gray!60, breakable,
  title={Co oznacza $tx$ w~wyrażeniu $e^{itx}$?}]

W~wyrażeniu $e^{itx}$ mamy \textbf{dwie zmienne}:
\begin{itemize}[leftmargin=2em]
  \item $x$ --- \textbf{dana} (wartość z~naszych danych, np.\ rzut embeddingu).
        To jest konkretna liczba, np.\ $x = 1{,}3$ albo $x = -0{,}7$.
  \item $t$ --- \textbf{parametr}, który my wybieramy.
        Można go rozumieć jako ``częstotliwość skanowania''.
\end{itemize}

\textbf{Co robi iloczyn $tx$?} Określa \textbf{kąt obrotu} na okręgu jednostkowym.

Przypomnijmy: $e^{i\theta}$ to punkt na okręgu pod kątem $\theta$.
Tutaj $\theta = tx$, więc:
\begin{itemize}[leftmargin=2em]
  \item Gdy $t$ jest \textbf{małe} (np.\ $t = 0{,}1$),
        kąt $tx$ zmienia się \textbf{powoli} gdy $x$ rośnie ---
        nawet duże różnice w~$x$ dają małe różnice kąta.
  \item Gdy $t$ jest \textbf{duże} (np.\ $t = 10$),
        kąt $tx$ zmienia się \textbf{szybko} ---
        małe różnice w~$x$ powodują duże obroty na okręgu.
\end{itemize}

\textbf{Przykład liczbowy} --- dwa punkty danych $x_1 = 1{,}0$ i~$x_2 = 1{,}5$:

\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{c c c c l}
\toprule
$t$ & $tx_1$ & $tx_2$ & $tx_2 - tx_1$ & Interpretacja \\
\midrule
$0{,}5$ & $0{,}5$\,rad & $0{,}75$\,rad & $0{,}25$\,rad ($14°$)
  & wolne skanowanie --- punkty blisko siebie \\
$2$ & $2$\,rad & $3$\,rad & $1$\,rad ($57°$)
  & średnie skanowanie \\
$10$ & $10$\,rad & $15$\,rad & $5$\,rad ($286°$)
  & szybkie skanowanie --- punkty prawie po drugiej stronie \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Analogia radiowa:} $t$ to ``częstotliwość stacji radiowej'', na którą się stroimy.
Różne częstotliwości $t$ wychwytują różne cechy rozkładu,
tak jak różne stacje radiowe nadają różną muzykę.
CF~$\varphi(t)$ mówi nam ``co słychać na częstotliwości $t$''.
\end{tcolorbox}

\textbf{Wyobraź sobie:} Masz $N$ wartości $x_1, x_2, \ldots, x_N$ (np.\ rzuty embeddingów).
Dla ustalonego $t$, każda wartość $x_j$ daje punkt na okręgu:
\[
x_j \;\longrightarrow\; e^{itx_j} = \cos(tx_j) + i\sin(tx_j)
\quad\text{--- punkt na okręgu jednostkowym}
\]
Mamy więc $N$ punktów na okręgu. Ich \textbf{średnia} (środek ciężkości)
to przybliżenie funkcji charakterystycznej:

\begin{tcolorbox}[colback=orange!5, colframe=orange!70!black, breakable,
  title={Intuicja: CF to średnia punktów na okręgu}]

\begin{center}
\begin{tikzpicture}[scale=1.3]
  % okrąg
  \draw[gray!30] (0,0) circle (1);
  \draw[->] (-1.4,0) -- (1.6,0) node[right] {Re};
  \draw[->] (0,-1.4) -- (0,1.6) node[above] {Im};
  % punkty -- symulacja "blisko Gaussa"
  \foreach \ang/\lab in {15/1, 55/2, 130/3, 200/4, 280/5, 340/6} {
    \fill[lejepaBlue!70] ({cos(\ang)},{sin(\ang)}) circle (2pt);
  }
  % średnia ~blisko (0.1, 0.05)
  \fill[lejepaRed] (0.12,0.05) circle (3pt)
    node[above right] {$\overline{e^{itx_j}}$ = CF};
  \draw[lejepaRed, thick, ->] (0,0) -- (0.12,0.05);
  \node[below, text width=6cm, align=center] at (0,-1.7)
    {$N=6$ punktów (niebieskie) i~ich średnia (czerwona)};
\end{tikzpicture}
\end{center}

Jeśli dane $x_j$ są ``ładnie rozłożone'' (bliskie Gaussowi),
punkty rozkładają się \textbf{równomiernie} po okręgu i~ich średnia
jest bliska zeru. Jeśli dane mają strukturę (nie-Gaussowską),
punkty \textbf{skupiają się} w~jednym miejscu i~średnia jest daleko od zera.

To właśnie mierzy funkcja charakterystyczna!
\end{tcolorbox}

%% =============================================================
%% ----- Element 5: Momenty rozkładu --- teoria od podstaw -----
%% =============================================================
\subsubsection{Element 5: Momenty rozkładu --- czym są i~do czego służą}

Zanim wyjaśnimy, dlaczego w~SIGReg używamy akurat $e^{itX}$,
musimy poznać prostsze narzędzie, z~którego CF wyrasta --- \textbf{momenty rozkładu}.

\begin{tcolorbox}[colback=violet!5, colframe=violet!70!black, breakable,
  title={Czym są ``momenty'' w~matematyce?}]

Słowo \textbf{moment} w~matematyce pochodzi z~\textbf{fizyki} ---
z~mechaniki Archimedesa i~dźwigni.

\textbf{Fizyczny moment siły} to siła $\times$ ramię (odległość od punktu obrotu):
\[
\text{moment} = F \times d
\]
Jeśli masz belkę z~ciężarkami, to moment mówi ci,
\textit{jak mocno} ciężarki ``ciągną'' belkę, żeby się obróciła.
Im dalej ciężarek od środka ($d$ duże) i~im cięższy ($F$ duże),
tym większy moment.

\medskip
\textbf{W~matematyce i~statystyce} przenosimy tę ideę na rozkłady prawdopodobieństwa.
Zamiast ciężarków na belce mamy \textit{punkty danych na osi liczbowej},
a~zamiast masy --- \textit{prawdopodobieństwo}:

\begin{center}
\begin{tikzpicture}[scale=0.85]
  % belka
  \draw[thick] (-5,0) -- (5.5,0);
  % punkt obrotu (trójkąt)
  \draw[thick, fill=gray!30] (-0.2,-0.45) -- (0.2,-0.45) -- (0,-0.08) -- cycle;
  \node[below] at (0,-0.55) {\small punkt obrotu ($\mu$)};
  % ciężarki = dane
  \fill[lejepaBlue] (-4,0) circle (3.5pt);
  \node[above] at (-4, 0.3) {$x_1$};
  \fill[lejepaBlue] (-1.5,0) circle (3pt);
  \node[above] at (-1.5, 0.3) {$x_2$};
  \fill[lejepaBlue] (2.5,0) circle (4pt);
  \node[above] at (2.5, 0.3) {$x_3$};
  \fill[lejepaBlue] (4.5,0) circle (3.5pt);
  \node[above] at (4.5, 0.3) {$x_4$};
  % odległość x1 - mu (na lewo od mu)
  \draw[<->, lejepaRed, thick] (-4,-1.2) -- (0,-1.2);
  \node[lejepaRed, below] at (-2,-1.2) {\small$x_1 - \mu$ (ujemne)};
  % odległość x4 - mu (na prawo od mu)
  \draw[<->, lejepaRed, thick] (0,-1.2) -- (4.5,-1.2);
  \node[lejepaRed, below] at (2.25,-1.2) {\small$x_4 - \mu$ (dodatnie)};
  % pionowe linie pomocnicze
  \draw[gray, thin, dashed] (0,0) -- (0,-1.2);
  \draw[gray, thin, dashed] (-4,0) -- (-4,-1.2);
  \draw[gray, thin, dashed] (4.5,0) -- (4.5,-1.2);
\end{tikzpicture}
\end{center}

\textbf{Matematyczny moment $n$-tego rzędu} to ``siła'',
z~jaką dane ciągną na $n$-tą potęgę odchylenia od środka:
\begin{equation}
\underbrace{\mathbb{E}[(X - \mu)^n]}_{\text{$n$-ty moment centralny}}
= \int_{-\infty}^{\infty} (x - \mu)^n \, p(x)\, dx
\label{eq:moment_central}
\end{equation}

\textbf{Skąd tu się bierze całka?}
Przypomnijmy definicję $\mathbb{E}$ z~ramki wyżej:
dla rozkładu ciągłego z~gęstością $p(x)$, wartość oczekiwana
\textit{dowolnego} wyrażenia $f(X)$ to:
\[
\mathbb{E}[f(X)] = \int_{-\infty}^{\infty} f(x) \, p(x)\, dx
\]
W~naszym przypadku $f(x) = (x - \mu)^n$, więc wstawiamy do wzoru
i~dostajemy równanie~(\ref{eq:moment_central}).

\textbf{Co ta całka robi geometrycznie?}
Najłatwiej zobaczyć to dla $n = 2$ (wariancji):

\begin{center}
\begin{tikzpicture}[scale=1.0, >=stealth]
  % Gauss
  \draw[->] (-4,0) -- (4.5,0) node[right] {$x$};
  \draw[->] (-4,0) -- (-4,2.5) node[above] {$p(x)$};
  \draw[thick, lejepaBlue, domain=-3.8:3.8, samples=100]
    plot (\x, {2.0*exp(-0.5*\x*\x)});
  % Mu
  \draw[dashed, gray] (0,0) -- (0,2.0);
  \node[below] at (0,-0.15) {$\mu$};
  % Punkt x_0
  \draw[thick, lejepaRed, dashed] (2.2,0) -- (2.2,{2.0*exp(-0.5*2.2*2.2)});
  \fill[lejepaRed] (2.2,0) circle (2pt);
  \node[below, lejepaRed] at (2.2,-0.15) {$x_0$};
  % Odchylenie
  \draw[<->, lejepaRed, thick] (0,-0.6) -- (2.2,-0.6);
  \node[below, lejepaRed] at (1.1,-0.6) {\small$x_0 - \mu$};
  % Prostokąt (x-mu)^2 * p(x)
  \fill[lejepaRed!20] (2.0,0) rectangle (2.4,{(2.2*2.2)*2.0*exp(-0.5*2.2*2.2)*0.15});
  % Strzałka do prostokąta
  \draw[->, thick] (3.5,1.2) -- (2.5,{(2.2*2.2)*2.0*exp(-0.5*2.2*2.2)*0.15 + 0.05});
  \node[right, text width=3.5cm, font=\small] at (3.0,1.5)
    {$(x_0 - \mu)^2 \cdot p(x_0)$\\= wkład punktu $x_0$\\do wariancji};
  % Zamalowane pole pod krzywą (wariancja = suma wszystkich wkładów)
  \fill[lejepaBlue!10, domain=-3.5:3.5, samples=80]
    plot (\x, {0.15*\x*\x*2.0*exp(-0.5*\x*\x)}) -- (3.5,0) -- (-3.5,0) -- cycle;
  \draw[thick, lejepaGreen!70!black, domain=-3.5:3.5, samples=80]
    plot (\x, {0.15*\x*\x*2.0*exp(-0.5*\x*\x)});
  \node[lejepaGreen!70!black, above] at (-2.3,0.55) {\small$(x\!-\!\mu)^2 \cdot p(x)$};
\end{tikzpicture}
\end{center}

\begin{tcolorbox}[colback=orange!5, colframe=orange!70!black, breakable,
  title={Jak czytać ten wykres?}]

\textbf{Niebieska krzywa} --- gęstość $p(x)$, czyli rozkład Gaussa.
Mówi, jak prawdopodobne są poszczególne wartości $x$.

\textbf{Zielona krzywa} --- iloczyn $(x - \mu)^2 \cdot p(x)$.
To jest funkcja ``pod całką'' we wzorze~(\ref{eq:moment_central}) dla $n = 2$.
Każdy punkt $x$ wnosi wkład:
\[
\underbrace{(x - \mu)^2}_{\substack{\text{jak daleko}\\\text{od średniej?}}}
\;\times\;
\underbrace{p(x)}_{\substack{\text{jak prawdo-}\\\text{podobny?}}}
\]

\textbf{Pole pod zieloną krzywą} (jasnoniebieskie) $=$ wartość całki $=$ \textbf{wariancja}.

\medskip
\textbf{Intuicja:} Punkt daleko od $\mu$ ma duże $(x - \mu)^2$,
ale małe $p(x)$ (mało prawdopodobny).
Punkt blisko $\mu$ ma małe $(x - \mu)^2$,
ale duże $p(x)$ (bardzo prawdopodobny).
Całka \textbf{równoważy} te dwa efekty --- to średnia ważona prawdopodobieństwem.
\end{tcolorbox}

Im wyższy rząd $n$, tym \textit{subtelniejszą} cechę kształtu rozkładu opisuje moment
--- dokładnie tak, jak w~fizyce momenty wyższych rzędów opisują coraz subtelniejsze
aspekty rozkładu masy.

\end{tcolorbox}

\begin{tcolorbox}[colback=yellow!5, colframe=yellow!70!black, breakable,
  title={Gdzie używa się momentów?}]

Momenty to \textbf{fundamentalne} pojęcie --- pojawiają się wszędzie:

\begin{itemize}[leftmargin=2em]
  \item \textbf{Statystyka:} Średnia i~wariancja (1.\ i~2.\ moment) to podstawowe
        miary opisujące dane. Każdy kurs statystyki zaczyna od nich.
  \item \textbf{Teoria prawdopodobieństwa:} Momenty definiują rozkłady.
        Rozkład Gaussa jest jedynym rozkładem, którego \textit{wszystkie} momenty
        wyższe niż 2.\ mają prostą formę.
  \item \textbf{Fizyka:} Moment bezwładności ($\mathbb{E}[r^2]$) ---
        jak trudno obrócić obiekt. Moment dipolowy --- rozkład ładunków.
  \item \textbf{Przetwarzanie obrazów:} Momenty Hu --- niezmienniki
        opisujące kształt obiektów na obrazie (obrót, skala).
  \item \textbf{Machine learning:} Adam optimizer śledzi 1.\ moment (średnią)
        i~2.\ moment (wariancję) gradientów --- to dlatego nazywa się
        \textit{Adaptive \textbf{Moment} Estimation}!
  \item \textbf{Finanse:} Skośność (3.\ moment) mierzy ryzyko ekstremalnych strat;
        kurtoza (4.\ moment) mierzy ``grubość ogonów'' rozkładu stóp zwrotu.
\end{itemize}
\end{tcolorbox}

\textbf{Podsumowując:} Moment $n$-tego rzędu to po prostu
$\mathbb{E}[X^n]$ (moment zwykły) lub $\mathbb{E}[(X-\mu)^n]$ (moment centralny).
To \textit{jedna liczba}, która opisuje pewną cechę kształtu rozkładu.
Im więcej momentów znamy, tym lepiej ``widzimy'' rozkład ---
ale jak zobaczymy za chwilę, nawet nieskończenie wiele momentów
nie zawsze wystarcza, żeby odtworzyć rozkład jednoznacznie.

Zobaczmy teraz konkretnie, co mierzą pierwsze cztery momenty:

\begin{tcolorbox}[colback=gray!5, colframe=gray!60, breakable,
  title={Momenty od kuchni --- na przykładzie}]

\textbf{Wyobraź sobie:} Masz zbiór $N$ liczb, np.\ wyniki egzaminu
100~studentów (w~procentach):
\[
x_1 = 45, \quad x_2 = 72, \quad x_3 = 68, \quad \ldots, \quad x_{100} = 81
\]

Pytanie: jak \textbf{opisać} ten zbiór jedną lub kilkoma liczbami?

\end{tcolorbox}

\begin{tcolorbox}[colback=blue!5, colframe=blue!60!black, breakable,
  title={Co oznacza symbol $\mathbb{E}$?}]

Symbol $\mathbb{E}$ (czytamy: ``E'' lub ``wartość oczekiwana'') to standardowy
zapis w~statystyce i~teorii prawdopodobieństwa na \textbf{średnią}.

\[
\mathbb{E}[X] \;=\; \text{``średnia wartość zmiennej losowej } X\text{''}
\]

\textbf{Skąd ta litera?} Od angielskiego \textit{\textbf{E}xpected value}
(wartość oczekiwana). Podwójne kreski ($\mathbb{E}$, tzw.\ \textit{blackboard bold})
to konwencja --- odróżnia symbol matematyczny od zwykłej litery~$E$.

\textbf{Dwa przypadki:}
\begin{itemize}[leftmargin=2em]
  \item \textbf{Dane dyskretne} (skończona lista liczb $x_1, \ldots, x_N$):
    \[
    \mathbb{E}[X] = \frac{1}{N}\sum_{j=1}^{N} x_j
    = \frac{x_1 + x_2 + \cdots + x_N}{N}
    \qquad\text{(zwykła średnia arytmetyczna)}
    \]
  \item \textbf{Rozkład ciągły} z~gęstością $p(x)$:
    \[
    \mathbb{E}[X] = \int_{-\infty}^{\infty} x \, p(x)\, dx
    \qquad\text{(średnia ważona, wagi $= $ prawdopodobieństwo)}
    \]
\end{itemize}

\textbf{Można wstawić dowolne wyrażenie} w~nawiasy kwadratowe:
\begin{itemize}[leftmargin=2em]
  \item $\mathbb{E}[X]$ --- średnia wartość $X$
  \item $\mathbb{E}[X^2]$ --- średnia wartość $X^2$ (kwadratów)
  \item $\mathbb{E}[(X - \mu)^2]$ --- średnia wartość odchyleń od średniej
        (= wariancja!)
  \item $\mathbb{E}[e^{itX}]$ --- średnia wartość $e^{itX}$
        (= funkcja charakterystyczna!)
\end{itemize}
\end{tcolorbox}

\medskip
\textbf{Moment 1.: Średnia --- ``gdzie jest środek?''}

Pierwszy moment to po prostu \textbf{średnia} (wartość oczekiwana):
\[
\mu = \mathbb{E}[X] = \frac{1}{N}\sum_{j=1}^{N} x_j
\]

\textbf{Przykład:} Egzamin z~5~ocenami: $55, 60, 70, 75, 90$.
\[
\mu = \frac{55 + 60 + 70 + 75 + 90}{5} = \frac{350}{5} = 70
\]
Średnia mówi: ``wyniki skupiają się wokół $70$''.

\begin{center}
\begin{tikzpicture}[scale=0.065]
  \draw[->] (40,0) -- (100,0) node[right] {\small wynik};
  \foreach \x in {55,60,70,75,90} {
    \fill[lejepaBlue] (\x,0) circle (6pt);
    \node[below] at (\x,-1.5) {\tiny\x};
  }
  \draw[thick, lejepaRed, ->] (70,6) -- (70,1.5);
  \node[above, lejepaRed] at (70,6) {\small $\mu = 70$};
\end{tikzpicture}
\end{center}

\textbf{Ale to nie wystarczy!} Dwa zbiory mogą mieć tę samą średnią,
ale wyglądać zupełnie inaczej:
\begin{itemize}[leftmargin=2em]
  \item Zbiór A: $68, 69, 70, 71, 72$ --- średnia $= 70$, \textbf{ciasno skupione}
  \item Zbiór B: $10, 40, 70, 100, 130$ --- średnia $= 70$, \textbf{bardzo rozrzucone}
\end{itemize}
Potrzebujemy czegoś więcej.

\bigskip
\textbf{Moment 2.: Wariancja --- ``jak bardzo rozrzucone?''}

Drugi moment (centralny) to \textbf{wariancja} --- mierzy,
jak daleko wartości odchylają się od średniej:
\[
\sigma^2 = \mathrm{Var}(X) = \mathbb{E}\big[(X - \mu)^2\big]
= \frac{1}{N}\sum_{j=1}^{N} (x_j - \mu)^2
\]

\textbf{Krok po kroku} dla zbioru A: $68, 69, 70, 71, 72$ (średnia $\mu = 70$):
\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{c c c c}
\toprule
$x_j$ & $x_j - \mu$ & $(x_j - \mu)^2$ & Komentarz \\
\midrule
$68$ & $-2$ & $4$ & 2~pkt poniżej średniej \\
$69$ & $-1$ & $1$ &  \\
$70$ & $0$ & $0$ & dokładnie na średniej \\
$71$ & $+1$ & $1$ &  \\
$72$ & $+2$ & $4$ &  \\
\midrule
& & $\sigma^2 = \frac{4+1+0+1+4}{5} = \mathbf{2{,}0}$ & \textbf{mała wariancja} \\
\bottomrule
\end{tabular}
\end{center}

Dla zbioru B: $10, 40, 70, 100, 130$ (średnia $\mu = 70$):
\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{c c c}
\toprule
$x_j$ & $x_j - \mu$ & $(x_j - \mu)^2$ \\
\midrule
$10$ & $-60$ & $3600$ \\
$40$ & $-30$ & $900$ \\
$70$ & $0$ & $0$ \\
$100$ & $+30$ & $900$ \\
$130$ & $+60$ & $3600$ \\
\midrule
& & $\sigma^2 = \frac{3600+900+0+900+3600}{5} = \mathbf{1800}$ \\
\bottomrule
\end{tabular}
\end{center}

Wariancja $2$ vs $1800$ --- teraz widzimy różnicę!

\textbf{Odchylenie standardowe} $\sigma = \sqrt{\sigma^2}$ ma te same jednostki co dane
(np.\ $\sigma_A = \sqrt{2} \approx 1{,}4$ punktu, $\sigma_B = \sqrt{1800} \approx 42$ punkty).

\begin{tcolorbox}[colback=blue!5, colframe=blue!60!black]
\textbf{Dlaczego podnosimy do kwadratu?} Bo odchylenia $x_j - \mu$ mogą być
ujemne (poniżej średniej) lub dodatnie (powyżej). Gdybyśmy je po prostu
zsumowali, ujemne i~dodatnie by się \textit{zniosły} i~dały~$0$.
Kwadrat $(x_j - \mu)^2$ jest zawsze $\geq 0$, więc duże odchylenia
się nie kasują.
\end{tcolorbox}

\bigskip
\textbf{Moment 3.: Skośność --- ``czy rozkład jest symetryczny?''}

Trzeci moment (centralny, standaryzowany) to \textbf{skośność} (ang.\ \textit{skewness}):
\[
\gamma_1 = \mathbb{E}\!\left[\left(\frac{X - \mu}{\sigma}\right)^{\!3}\right]
\]

\begin{itemize}[leftmargin=2em]
  \item $\gamma_1 = 0$ --- rozkład jest \textbf{symetryczny} (np.\ Gauss)
  \item $\gamma_1 > 0$ --- rozkład ma \textbf{długi ogon w~prawo}
        (mało osób zarabia bardzo dużo)
  \item $\gamma_1 < 0$ --- rozkład ma \textbf{długi ogon w~lewo}
\end{itemize}

\begin{center}
\begin{tikzpicture}[scale=0.8, >=stealth]
  % Symetryczny
  \begin{scope}[shift={(-4.5,0)}]
    \draw[->] (-2.2,0) -- (2.5,0);
    \draw[thick, lejepaBlue, domain=-2:2, samples=50]
      plot (\x, {1.5*exp(-\x*\x)});
    \node[below] at (0,-0.3) {\small$\gamma_1 = 0$};
    \node[above] at (0,1.7) {\small symetryczny};
  \end{scope}
  % Prawo-skośny
  \begin{scope}[shift={(0,0)}]
    \draw[->] (-1.2,0) -- (3.5,0);
    \draw[thick, lejepaRed, domain=0.01:3.2, samples=50]
      plot (\x, {2.5*\x*exp(-\x)});
    \node[below] at (1,-0.3) {\small$\gamma_1 > 0$};
    \node[above] at (1,1.7) {\small ogon w~prawo};
  \end{scope}
  % Lewo-skośny
  \begin{scope}[shift={(5.5,0)}]
    \draw[->] (-3.5,0) -- (1.2,0);
    \draw[thick, lejepaGreen, domain=-3.2:-0.01, samples=50]
      plot (\x, {-2.5*\x*exp(\x)});
    \node[below] at (-1,-0.3) {\small$\gamma_1 < 0$};
    \node[above] at (-1,1.7) {\small ogon w~lewo};
  \end{scope}
\end{tikzpicture}
\end{center}

\textbf{Dlaczego potęga 3?}
Potęga nieparzysta zachowuje znak: $(x - \mu)^3$ jest ujemne gdy $x < \mu$
i~dodatnie gdy $x > \mu$. Jeśli jest więcej wartości daleko po prawej stronie,
suma wychodzi dodatnia $\Rightarrow$ skośność $> 0$.

\bigskip
\textbf{Moment 4.: Kurtoza --- ``jak grube ogony?''}

Czwarty moment (centralny, standaryzowany) to \textbf{kurtoza} (ang.\ \textit{kurtosis}):
\[
\gamma_2 = \mathbb{E}\!\left[\left(\frac{X - \mu}{\sigma}\right)^{\!4}\right] - 3
\]
(Odejmujemy $3$, bo rozkład Gaussa ma kurtosis $= 3$; po odjęciu Gauss ma $\gamma_2 = 0$.)

\begin{itemize}[leftmargin=2em]
  \item $\gamma_2 = 0$ --- ogony jak Gauss (rozkład \textbf{mezokurtyczny})
  \item $\gamma_2 > 0$ --- \textbf{grubsze} ogony niż Gauss (więcej wartości ekstremalnych)
  \item $\gamma_2 < 0$ --- \textbf{cieńsze} ogony niż Gauss (mniej wartości ekstremalnych)
\end{itemize}

\begin{center}
\begin{tikzpicture}[scale=0.85, >=stealth]
  % --- Platykurtyczny (cienkie ogony) ---
  \begin{scope}[shift={(-5,0)}]
    \draw[->] (-3.2,0) -- (3.5,0);
    \draw[->] (0,-0.1) -- (0,2.5);
    % Uniform-like (szeroki, płaski)
    \draw[thick, lejepaGreen, domain=-2.2:2.2, samples=80]
      plot (\x, {2.0*exp(-0.5*\x*\x/(1.8*1.8))});
    % Gauss reference (cienka linia)
    \draw[gray, dashed, domain=-3:3, samples=80]
      plot (\x, {1.8*exp(-0.5*\x*\x)});
    \node[below, text width=3cm, align=center] at (0,-0.5)
      {\small $\gamma_2 < 0$\\\footnotesize\textbf{platykurtyczny}\\\footnotesize(cienkie ogony,\\\footnotesize płaski szczyt)};
  \end{scope}
  % --- Gauss (mezokurtyczny) ---
  \begin{scope}[shift={(0,0)}]
    \draw[->] (-3.2,0) -- (3.5,0);
    \draw[->] (0,-0.1) -- (0,2.5);
    \draw[thick, lejepaBlue, domain=-3:3, samples=80]
      plot (\x, {1.8*exp(-0.5*\x*\x)});
    \node[below, text width=3cm, align=center] at (0,-0.5)
      {\small $\gamma_2 = 0$\\\footnotesize\textbf{mezokurtyczny}\\\footnotesize(Gauss --- punkt\\\footnotesize odniesienia)};
  \end{scope}
  % --- Leptokurtyczny (grube ogony) ---
  \begin{scope}[shift={(5,0)}]
    \draw[->] (-3.2,0) -- (3.5,0);
    \draw[->] (0,-0.1) -- (0,2.5);
    % Laplace-like (ostry szczyt, grube ogony)
    \draw[thick, lejepaRed, domain=-3:3, samples=80]
      plot (\x, {2.2*exp(-1.8*abs(\x))});
    % Gauss reference
    \draw[gray, dashed, domain=-3:3, samples=80]
      plot (\x, {1.8*exp(-0.5*\x*\x)});
    \node[below, text width=3cm, align=center] at (0,-0.5)
      {\small $\gamma_2 > 0$\\\footnotesize\textbf{leptokurtyczny}\\\footnotesize(grube ogony,\\\footnotesize ostry szczyt)};
  \end{scope}
\end{tikzpicture}
\end{center}

\begin{tcolorbox}[colback=orange!5, colframe=orange!70!black, breakable,
  title={Jak czytać ten rysunek?}]

Na każdym wykresie szara przerywana linia to \textbf{Gauss} (punkt odniesienia, $\gamma_2 = 0$).
Kolorowa linia to porównywany rozkład.

\textbf{Lewy wykres} (zielony, $\gamma_2 < 0$):
Rozkład jest \textbf{szerszy i~bardziej płaski} niż Gauss --- wartości
rozkładają się bardziej równomiernie, mniej ekstremalnych wyników.
Przykład: rozkład jednostajny (``każda wartość jednakowo prawdopodobna'').

\textbf{Środkowy wykres} (niebieski, $\gamma_2 = 0$):
To sam \textbf{Gauss} $\mathcal{N}(0,1)$ --- ``złoty środek''.

\textbf{Prawy wykres} (czerwony, $\gamma_2 > 0$):
Rozkład ma \textbf{ostrzejszy szczyt} i~\textbf{grubsze ogony} niż Gauss ---
większość wartości skupia się blisko średniej,
ale \textit{zdarzają się} wartości bardzo dalekie od środka.
Przykład: rozkład Laplace'a, rozkład $t$-Studenta z~małą liczbą stopni swobody.

\medskip
\textbf{Dlaczego to ważne?}
W~finansach: grube ogony ($\gamma_2 > 0$) oznaczają, że kryzysy (duże straty)
zdarzają się \textit{częściej}, niż przewidywałby Gauss.
Kryzys 2008~r.\ był ``wydarzeniem o~25~sigma'' według modelu Gaussowskiego ---
czyli praktycznie niemożliwym. A~jednak się zdarzył,
bo rynki mają $\gamma_2 \gg 0$.
\end{tcolorbox}

\textbf{Dlaczego potęga 4?}
Potęga parzysta, więc $(x - \mu)^4$ jest zawsze $\geq 0$ (jak wariancja).
Ale potęga $4$ wzmacnia \textbf{duże odchylenia} znacznie bardziej niż kwadrat:
jeśli punkt jest $3\sigma$ od średniej, to $(3\sigma)^2 = 9\sigma^2$,
ale $(3\sigma)^4 = 81\sigma^4$ --- dziewięciokrotnie większy wkład!
Dlatego kurtoza jest szczególnie wrażliwa na \textit{wartości ekstremalne} (ogony).

\bigskip
\textbf{Ogólna definicja: $n$-ty moment}

\begin{tcolorbox}[colback=lejepaGreen!8, colframe=lejepaGreen!80, breakable,
  title={Definicja: Momenty rozkładu}]

Dla zmiennej losowej $X$:
\begin{itemize}[leftmargin=2em]
  \item \textbf{$n$-ty moment (zwykły):}
    $m_n = \mathbb{E}[X^n] = \int_{-\infty}^{\infty} x^n \, p(x)\,dx$
  \item \textbf{$n$-ty moment centralny} (wokół średniej):
    $\mu_n = \mathbb{E}[(X - \mu)^n]$
\end{itemize}

\medskip
Pierwsze cztery momenty centralne mają nazwy:

\begin{center}
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{c l l l}
\toprule
$n$ & \textbf{Nazwa} & \textbf{Wzór} & \textbf{Co mierzy} \\
\midrule
$1$ & Średnia & $\mu = \mathbb{E}[X]$ & położenie (``gdzie jest środek'') \\
$2$ & Wariancja & $\sigma^2 = \mathbb{E}[(X\!-\!\mu)^2]$ & rozrzut (``jak szeroki'') \\
$3$ & Skośność & $\gamma_1 = \mathbb{E}[((X\!-\!\mu)/\sigma)^3]$ & asymetria (``który ogon dłuższy'') \\
$4$ & Kurtoza & $\gamma_2 = \mathbb{E}[((X\!-\!\mu)/\sigma)^4] - 3$ & grubość ogonów \\
\bottomrule
\end{tabular}
\end{center}

Każdy kolejny moment opisuje coraz \textit{subtelniejszą} cechę kształtu rozkładu.
Dla $\mathcal{N}(0,1)$: $\mu = 0$, $\sigma^2 = 1$, $\gamma_1 = 0$, $\gamma_2 = 0$.
\end{tcolorbox}

\begin{keyinsight}[Momenty to ``pytania'' zadawane rozkładowi]
Można myśleć o~momentach jak o~coraz trudniejszych pytaniach:
\begin{enumerate}[leftmargin=3.5em, labelsep=0.5em]
  \item[\textbf{1.}] Gdzie jest środek? $\longrightarrow$ średnia
  \item[\textbf{2.}] Jak bardzo rozrzucony? $\longrightarrow$ wariancja
  \item[\textbf{3.}] Czy symetryczny? $\longrightarrow$ skośność
  \item[\textbf{4.}] Czy ma grube ogony? $\longrightarrow$ kurtoza
  \item[\textbf{5, 6, \ldots}] Coraz subtelniejsze cechy kształtu\ldots
\end{enumerate}
Znając \textit{wszystkie} momenty (nieskończenie wiele), chcielibyśmy
odtworzyć cały rozkład. Czy to jest możliwe? O~tym za chwilę.
\end{keyinsight}

%% ----- Element 6: Dlaczego akurat e^{itX}? -----
\subsubsection{Element 6: Dlaczego akurat $e^{itX}$? --- skąd ten wzór}

\begin{tcolorbox}[colback=red!5, colframe=red!60!black, breakable,
  title={Pytanie: Dlaczego $e^{itX}$, a~nie coś prostszego?}]

Poznaliśmy momenty --- średnią, wariancję, skośność, kurtozę\ldots
Moglibyśmy próbować opisać rozkład
\textit{właśnie} za pomocą momentów. Ale czy to wystarczy?

Prześledźmy trzy próby opisu rozkładu --- od prostych do~CF.
\end{tcolorbox}

\textbf{Próba 1: Momenty --- $\mathbb{E}[X]$, $\mathbb{E}[X^2]$, $\mathbb{E}[X^3]$, \ldots}

Najprostszy pomysł: opisujemy rozkład przez jego momenty
(tę samą listę, którą właśnie poznaliśmy).
Jeśli znamy \textit{wszystkie} momenty: $m_1, m_2, m_3, \ldots$ ---
to czy jednoznacznie wiemy, jaki to rozkład?

\textbf{Problem: NIE!} Istnieją rozkłady, które mają \textit{dokładnie te same}
wszystkie momenty, ale są \textbf{różne}!
(Znany kontrprzykład: rozkład lognormalny i~pewne inne rozkłady mają
identyczne momenty --- to tzw.\ \textit{problem momentów Stieltiesa}.)

Momenty \textbf{nie} identyfikują jednoznacznie rozkładu.

\medskip
\textbf{Próba 2: ``Generowanie'' momentów --- $\mathbb{E}[e^{tX}]$}

Momenty jako lista ($m_1, m_2, m_3, \ldots$) nie identyfikują rozkładu jednoznacznie.
Ale co, gdybyśmy \textit{zapakowali je wszystkie w~jedną funkcję}?

\begin{tcolorbox}[colback=violet!5, colframe=violet!70!black, breakable,
  title={Dlaczego akurat $e^{tX}$?}]

\textbf{Motywacja:} Mamy nieskończenie wiele momentów:
$\mathbb{E}[X],\; \mathbb{E}[X^2],\; \mathbb{E}[X^3],\; \ldots$

Chcemy jednej funkcji, która \textit{ukryje w~sobie} wszystkie te momenty.
Szukamy więc funkcji $f(x)$, której rozwinięcie
w~szereg Taylora zawiera potęgi $x^n$ --- bo $\mathbb{E}[X^n]$ to $n$-ty moment.

Ale czym jest szereg Taylora? Przypomnijmy ogólny wzór.

\medskip
\textbf{Ogólny wzór Taylora} (wokół punktu $x = 0$, tzw.\ szereg Maclaurina):
\begin{equation}
f(x) = f(0) + f'(0)\,x + \frac{f''(0)}{2!}\,x^2
+ \frac{f'''(0)}{3!}\,x^3 + \cdots
= \sum_{n=0}^{\infty} \frac{f^{(n)}(0)}{n!}\,x^n
\label{eq:taylor}
\end{equation}
gdzie $f^{(n)}(0)$ to $n$-ta pochodna funkcji $f$ w~punkcie $0$.

\textbf{Słownie:} Każdą ``gładką'' funkcję można zapisać jako
nieskończoną sumę potęg $x^n$, a~współczynniki przy potęgach
zależą od pochodnych $f$ w~zerze.

\medskip
Zobaczmy, jak wygląda to rozwinięcie dla \textbf{kilku znanych funkcji}:

\medskip
\textbf{Przykład A:} $f(x) = \frac{1}{1-x}$ \quad(szereg geometryczny)
\[
\frac{1}{1-x} = 1 + x + x^2 + x^3 + x^4 + \cdots = \sum_{n=0}^{\infty} x^n
\]
Współczynniki: $1, 1, 1, 1, \ldots$ --- \textit{wszystkie} potęgi są,
ale współczynniki to same jedynki (nie ma $1/n!$).

\textbf{Przykład B:} $f(x) = \sin(x)$
\[
\sin(x) = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \cdots
\]
\textbf{Brakuje} potęg parzystych ($x^0, x^2, x^4, \ldots$) ---
nie wyciągniemy z~tego $\mathbb{E}[X^2]$ (wariancji)!

\textbf{Przykład C:} $f(x) = \cos(x)$
\[
\cos(x) = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!} + \cdots
\]
\textbf{Brakuje} potęg nieparzystych ($x^1, x^3, x^5, \ldots$) ---
nie wyciągniemy z~tego $\mathbb{E}[X]$ (średniej)!

\textbf{Przykład D:} $f(x) = \ln(1+x)$
\[
\ln(1+x) = x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} + \cdots
\]
Są wszystkie potęgi, ale ze \textbf{znakami naprzemiennymi} i~współczynnikami $1/n$.
Problem: różniczkowanie dałoby skomplikowane kombinacje momentów.

\medskip
\textbf{Przykład E:} $f(x) = e^x$
\[
e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \frac{x^4}{4!} + \cdots
= \sum_{n=0}^{\infty} \frac{x^n}{n!}
\]

\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{l c c c}
\toprule
\textbf{Funkcja} & \textbf{Wszystkie potęgi $x^n$?}
  & \textbf{Proste współczynniki?} & \textbf{Nadaje się?} \\
\midrule
$\frac{1}{1-x}$ & tak & jedynki (brak $1/n!$) & trudno odczytać momenty \\
$\sin(x)$ & \textbf{nie} (brak parzystych) & --- & nie \\
$\cos(x)$ & \textbf{nie} (brak nieparzystych) & --- & nie \\
$\ln(1+x)$ & tak & skomplikowane ($1/n$, znaki) & niewygodnie \\
$e^x$ & \textbf{tak} & \textbf{tak} ($1/n!$) & \textbf{idealnie!} \\
\bottomrule
\end{tabular}
\end{center}

Tylko $e^x$ ma \textbf{wszystkie potęgi} $x^n$ ze współczynnikami $1/n!$,
które dokładnie ``kasują się'' przy $n$-krotnym różniczkowaniu
(bo $\frac{d^n}{dx^n}\frac{x^n}{n!} = 1$).
Dlatego wystarczy wstawić $x = tX$:
\[
e^{tX} = 1 + tX + \frac{(tX)^2}{2!} + \frac{(tX)^3}{3!} + \cdots
= \sum_{n=0}^{\infty} \frac{t^n X^n}{n!}
\]

Teraz bierzemy wartość oczekiwaną obu stron
(średnią po wszystkich danych / rozkładzie):
\begin{align*}
\mathbb{E}[e^{tX}]
&= \mathbb{E}\!\left[1 + tX + \frac{t^2 X^2}{2!} + \frac{t^3 X^3}{3!} + \cdots\right] \\[4pt]
&= 1 + t\underbrace{\mathbb{E}[X]}_{m_1}
+ \frac{t^2}{2!}\underbrace{\mathbb{E}[X^2]}_{m_2}
+ \frac{t^3}{3!}\underbrace{\mathbb{E}[X^3]}_{m_3} + \cdots
\end{align*}

\textbf{Wynik:} Jedna funkcja $M(t) = \mathbb{E}[e^{tX}]$ koduje
\textit{wszystkie momenty naraz}!
Żeby odczytać $n$-ty moment, wystarczy wziąć $n$-tą pochodną po $t$
w~punkcie $t = 0$:
\[
m_n = \mathbb{E}[X^n] = \left.\frac{d^n M(t)}{dt^n}\right|_{t=0}
\]

\textbf{Dlaczego pochodna ``wyciąga'' moment?} Pokażmy to na konkretnym przykładzie.

Zapiszmy $M(t)$ jako szereg (z~powyższego rozwinięcia):
\[
M(t) = \underbrace{1}_{\text{wyraz }0}
+ \underbrace{m_1}_{\text{wyraz }1}\!\cdot t
+ \underbrace{\frac{m_2}{2!}}_{\text{wyraz }2}\!\cdot t^2
+ \underbrace{\frac{m_3}{3!}}_{\text{wyraz }3}\!\cdot t^3
+ \underbrace{\frac{m_4}{4!}}_{\text{wyraz }4}\!\cdot t^4 + \cdots
\]

\textbf{Chcemy wyciągnąć $m_2$} (2.\ moment = wariancja).
Różniczkujemy po $t$ dwa razy:

\medskip
\textbf{Pierwsza pochodna} $\frac{dM}{dt}$
--- przypomnienie: $\frac{d}{dt} t^n = n\,t^{n-1}$:
\[
M'(t) = 0 + m_1 + \frac{m_2}{2!}\cdot 2t + \frac{m_3}{3!}\cdot 3t^2
+ \frac{m_4}{4!}\cdot 4t^3 + \cdots
\]

\textbf{Druga pochodna} $\frac{d^2M}{dt^2}$:
\[
M''(t) = 0 + 0 + \frac{m_2}{\cancel{2!}}\cdot \cancel{2} + \frac{m_3}{3!}\cdot 3\cdot 2\,t
+ \frac{m_4}{4!}\cdot 4\cdot 3\,t^2 + \cdots
\]

\textbf{Wstawiamy $t = 0$} --- wszystkie wyrazy z~$t, t^2, t^3, \ldots$ znikają!
\[
M''(0) = m_2 + 0 + 0 + \cdots = m_2 \qquad\checkmark
\]

\begin{tcolorbox}[colback=green!5, colframe=green!60!black]
\textbf{Co się tu stało?}
Każdy wyraz szeregu to $\frac{m_n}{n!}\,t^n$.
Gdy różniczkujemy $n$~razy, z~$t^n$ powstaje $n! \cdot t^0 = n!$,
które \textbf{kasuje} $n!$ w~mianowniku:
\[
\frac{d^n}{dt^n}\left(\frac{m_n}{n!}\,t^n\right) = \frac{m_n}{\cancel{n!}} \cdot \cancel{n!} = m_n
\]
Wstawienie $t = 0$ zabija wszystkie \textit{wyższe} wyrazy (bo mają $t^1, t^2, \ldots$).
Zostaje \textbf{dokładnie} $m_n$.

To dlatego współczynniki $1/n!$ w~$e^x$ są kluczowe ---
są ``odwrotnością'' tego, co robi $n$-krotne różniczkowanie!
\end{tcolorbox}

\textbf{Dlatego} ta funkcja nazywa się \textbf{funkcją tworzącą momenty}
(ang.\ \textit{moment generating function}, MGF) ---
dosłownie ``generuje'' momenty przez różniczkowanie.

\medskip
\textbf{Dlaczego $e^x$, a~nie inna funkcja?}
Bo $e^x$ jest \textit{jedyną} funkcją, która spełnia jednocześnie:
\begin{enumerate}[leftmargin=2em]
  \item Zawiera wszystkie potęgi $x^n$ w~szeregu Taylora.
  \item Ma proste współczynniki ($1/n!$), które łatwo odwrócić
        różniczkowaniem.
  \item Jest swoją własną pochodną: $(e^x)' = e^x$ ---
        dlatego różniczkowanie po $t$ ``wyciąga'' kolejne momenty.
\end{enumerate}
Żadna inna funkcja nie ma tych trzech własności jednocześnie.
\end{tcolorbox}

\textbf{Problem z~MGF:} Ta całka $\int e^{tx} p(x)\,dx$ może \textbf{nie istnieć}
(być nieskończona)! Np.\ dla rozkładu Cauchy'ego $e^{tx}$ rośnie tak szybko,
że całka wybucha do $\infty$ dla każdego $t \neq 0$.

Potrzebujemy czegoś, co ma te same zalety co $e^{tX}$
(wszystkie momenty w~jednej funkcji), ale \textbf{zawsze istnieje}.

\medskip
\textbf{Próba 3 (sukces!): Wstawiamy $i$ --- $\mathbb{E}[e^{itX}]$}

Rozwiązanie: zamiast $e^{tX}$ (rzeczywista eksponenta, która rośnie do~$\infty$),
bierzemy $e^{itX}$ (zespolona eksponenta = \textbf{obrót po okręgu}).

Dlaczego to pomaga? Zobaczmy szereg Taylora.
W~MGF mieliśmy $e^{tX}$ z~podstawieniem $x = tX$:
\[
e^{tX} = 1 + tX + \frac{t^2 X^2}{2!} + \frac{t^3 X^3}{3!} + \cdots
\]
Teraz robimy to samo, ale z~$x = itX$ (wstawiamy $i$):
\[
e^{itX} = 1 + (it)X + \frac{(it)^2 X^2}{2!} + \frac{(it)^3 X^3}{3!}
+ \frac{(it)^4 X^4}{4!} + \cdots
\]

Obliczamy potęgi $(it)^n$, korzystając z~cyklu potęg $i$
(z~Elementu~3: $i^0\!=\!1,\; i^1\!=\!i,\; i^2\!=\!-1,\; i^3\!=\!-i,\; i^4\!=\!1,\;\ldots$):
\begin{align*}
e^{itX} &= 1 + it\,X + \frac{(it)^2}{2!}X^2 + \frac{(it)^3}{3!}X^3
+ \frac{(it)^4}{4!}X^4 + \cdots \\[4pt]
&= 1 + it\,X
\underbrace{- \frac{t^2}{2!}X^2}_{i^2 = -1}
\underbrace{- \frac{it^3}{3!}X^3}_{i^3 = -i}
\underbrace{+ \frac{t^4}{4!}X^4}_{i^4 = +1}
+ \cdots
\end{align*}

Teraz bierzemy wartość oczekiwaną $\mathbb{E}[\ldots]$:
\[
\varphi(t) = \mathbb{E}[e^{itX}]
= 1 + it\,m_1 - \frac{t^2}{2!}\,m_2 - \frac{it^3}{3!}\,m_3
+ \frac{t^4}{4!}\,m_4 + \cdots
\]
gdzie $m_n = \mathbb{E}[X^n]$ to $n$-ty moment (tak jak w~MGF!).

\begin{tcolorbox}[colback=green!5, colframe=green!60!black, breakable,
  title={Porównanie: MGF vs CF --- te same momenty, inne współczynniki}]

\renewcommand{\arraystretch}{1.4}
\begin{tabular}{l l l}
\toprule
\textbf{Wyraz} & \textbf{MGF} $M(t) = \mathbb{E}[e^{tX}]$
  & \textbf{CF} $\varphi(t) = \mathbb{E}[e^{itX}]$ \\
\midrule
$n = 0$ & $1$ & $1$ \\
$n = 1$ & $+\;t\,m_1$ & $+\;it\,m_1$ \\
$n = 2$ & $+\;\frac{t^2}{2!}\,m_2$ & $-\;\frac{t^2}{2!}\,m_2$ \\
$n = 3$ & $+\;\frac{t^3}{3!}\,m_3$ & $-\;\frac{it^3}{3!}\,m_3$ \\
$n = 4$ & $+\;\frac{t^4}{4!}\,m_4$ & $+\;\frac{t^4}{4!}\,m_4$ \\
\bottomrule
\end{tabular}

\medskip
\textbf{Obserwacja:} CF zawiera \textit{dokładnie te same momenty} $m_n$ co MGF!
Jedyna różnica to czynniki $i^n$ (naprzemienne znaki i~czynniki $i$),
które nie zmieniają \textit{informacji}, a~jedynie ``owijają'' wartości na okrąg
zamiast pozwalać im rosnąć do~$\infty$.

\textbf{Kluczowy zysk:} Każdy wyraz CF ma moduł:
\[
\left|\frac{(it)^n}{n!}X^n\right| = \frac{|t|^n}{n!}|X|^n
\]
Nie ma tu $e^{tX}$, które \textit{eksploduje} --- czynnik $i^n$ ma moduł $|i^n| = 1$,
więc nic nie wybucha.
\end{tcolorbox}

\textbf{Podsumowując:}
\begin{itemize}[leftmargin=2em]
  \item $e^{tX}$ (MGF) rośnie \textbf{bez granic} gdy $X$ jest duże
        --- całka może nie istnieć.
  \item $e^{itX}$ (CF) $= \cos(tX) + i\sin(tX)$ --- moduł jest \textbf{zawsze~1},
        bo $|\cos\theta + i\sin\theta| = 1$. Nigdy nie wybucha!
\end{itemize}

\begin{tcolorbox}[colback=blue!5, colframe=blue!60!black]
\textbf{Kluczowa różnica:}
\[
\underbrace{|e^{tX}|}_{\text{MGF}} = e^{tX}
\;\xrightarrow{X \to \infty}\; \infty
\qquad\text{vs.}\qquad
\underbrace{|e^{itX}|}_{\text{CF}} = 1 \quad\text{zawsze!}
\]
Dodanie $i$ zamienia ``eksplozję'' w~``obrót''.
Dlatego $\mathbb{E}[e^{itX}]$ istnieje \textbf{dla każdego rozkładu} ---
nawet takiego, który nie ma średniej ani wariancji.
\end{tcolorbox}

\textbf{A~ponadto:} CF jednoznacznie identyfikuje rozkład --- można z~niej
\textit{odzyskać} gęstość $p(x)$. Gwarantuje to twierdzenie inwersji.

%% --- Twierdzenie inwersji ---
\begin{tcolorbox}[colback=violet!5, colframe=violet!70!black, breakable,
  title={Twierdzenie inwersji Lévy'ego (Paul Lévy, 1925)}]

\textbf{Twierdzenie:} Jeśli zmienna losowa $X$ ma gęstość $p(x)$
i~funkcję charakterystyczną $\varphi_X(t) = \mathbb{E}[e^{itX}]$, to:
\begin{equation}
p(x) = \frac{1}{2\pi}\int_{-\infty}^{\infty} e^{-itx}\,\varphi_X(t)\,dt
\label{eq:inversion}
\end{equation}

\textbf{Słownie:} Znając CF $\varphi_X(t)$, możemy \textbf{odtworzyć} gęstość $p(x)$
--- wystarczy scałkować $e^{-itx}\,\varphi_X(t)$ po wszystkich częstotliwościach~$t$.
\end{tcolorbox}

\textbf{Skąd ten wzór?} Wyprowadźmy go krok po kroku.

\medskip
\textbf{Krok 1.} Zapiszmy, co stoi pod całką w~(\ref{eq:inversion}).
Wstawiamy definicję CF: $\varphi_X(t) = \int e^{ity}\,p(y)\,dy$
(używamy $y$ zamiast $x$, żeby nie mylić zmiennych):
\[
\frac{1}{2\pi}\int_{-\infty}^{\infty} e^{-itx}\,
\underbrace{\varphi_X(t)}_{=\,\int e^{ity} p(y)\,dy}\,dt
= \frac{1}{2\pi}\int_{-\infty}^{\infty}\!\int_{-\infty}^{\infty}
e^{-itx}\,e^{ity}\,p(y)\,dy\,dt
\]

\textbf{Krok 2.} Łączymy wykładniki ($e^{-itx}\cdot e^{ity} = e^{it(y-x)}$):
\[
= \frac{1}{2\pi}\int_{-\infty}^{\infty}\!\int_{-\infty}^{\infty}
e^{it(y-x)}\,p(y)\,dy\,dt
\]

\textbf{Krok 3.} Zamieniamy kolejność całkowania
(najpierw po~$t$, potem po~$y$):
\[
= \int_{-\infty}^{\infty} p(y) \underbrace{\left[\frac{1}{2\pi}
\int_{-\infty}^{\infty} e^{it(y-x)}\,dt\right]}_{\text{co to jest?}}\,dy
\]

\textbf{Krok 4.} Kluczowy fakt --- \textbf{delta Diraca}:

\begin{tcolorbox}[colback=blue!5, colframe=blue!60!black]
\textbf{Tożsamość Fouriera:}
\[
\frac{1}{2\pi}\int_{-\infty}^{\infty} e^{it\alpha}\,dt = \delta(\alpha)
\]
gdzie $\delta(\alpha)$ to \textbf{delta Diraca} --- ``funkcja'', która:
\begin{itemize}[leftmargin=2em]
  \item jest równa $0$ wszędzie, gdzie $\alpha \neq 0$,
  \item jest ``nieskończenie wysoka'' w~$\alpha = 0$,
  \item ma pole pod krzywą równe~$1$: $\int_{-\infty}^{\infty}\delta(\alpha)\,d\alpha = 1$.
\end{itemize}
\textbf{Kluczowa własność} (``sito''):
$\int f(\alpha)\,\delta(\alpha - a)\,d\alpha = f(a)$

Delta ``wyciąga'' wartość funkcji w~jednym punkcie.
\end{tcolorbox}

W~naszym przypadku $\alpha = y - x$:
\[
\frac{1}{2\pi}\int_{-\infty}^{\infty} e^{it(y-x)}\,dt = \delta(y - x)
\]

\textbf{Krok 5.} Wstawiamy deltę:
\[
\int_{-\infty}^{\infty} p(y)\,\delta(y - x)\,dy
= p(x) \qquad\checkmark
\]
Delta ``wybrała'' z~całki dokładnie $y = x$ i~zostawiła $p(x)$.

\begin{tcolorbox}[colback=green!5, colframe=green!60!black]
\textbf{Podsumowanie wyprowadzenia:}
\[
\frac{1}{2\pi}\int e^{-itx}\,\varphi_X(t)\,dt
\;\xrightarrow{\text{wstaw definicję CF}}\;
\int p(y)\underbrace{\frac{1}{2\pi}\int e^{it(y-x)}dt}_{\delta(y-x)}\,dy
\;\xrightarrow{\text{delta Diraca}}\;
p(x)
\]
\end{tcolorbox}

\medskip
\textbf{Dlaczego to jest ważne?}

\begin{tcolorbox}[colback=orange!5, colframe=orange!70!black, breakable,
  title={Konsekwencje twierdzenia inwersji}]

\textbf{1. Jednoznaczność:} Jeśli dwa rozkłady mają tę samą CF,
to z~(\ref{eq:inversion}) mają tę samą gęstość:
\[
\varphi_X(t) = \varphi_Y(t) \;\;\forall\, t
\qquad\Longrightarrow\qquad
p_X(x) = p_Y(x) \;\;\forall\, x
\qquad\Longrightarrow\qquad
X \stackrel{d}{=} Y
\]
CF to \textbf{prawdziwy odcisk palca} rozkładu ---
dwa różne rozkłady nie mogą mieć tej samej CF.

\medskip
\textbf{2. CF $\leftrightarrow$ gęstość to para transformacji Fouriera:}
\[
\underbrace{p(x) \;\xrightarrow{\text{transformata Fouriera}}\; \varphi(t)}_{\text{CF: ``kodujemy'' rozkład}}
\qquad\qquad
\underbrace{\varphi(t) \;\xrightarrow{\text{odwrotna transf.\ Fouriera}}\; p(x)}_{\text{inwersja: ``dekodujemy'' rozkład}}
\]
Przejście w~jedną stronę:
$\varphi(t) = \int e^{itx}\,p(x)\,dx$ \quad(definicja CF)

Przejście w~drugą stronę:
$p(x) = \frac{1}{2\pi}\int e^{-itx}\,\varphi(t)\,dt$ \quad(tw.\ inwersji)

\medskip
\textbf{3. Analogia z~muzyką:}
\begin{itemize}[leftmargin=2em]
  \item $p(x)$ to ``przebieg czasowy'' dźwięku (amplituda w~czasie).
  \item $\varphi(t)$ to ``widmo częstotliwościowe'' (jakie częstotliwości zawiera dźwięk).
  \item Transformata Fouriera: nagranie $\to$ widmo.
  \item Odwrotna transformata: widmo $\to$ nagranie.
  \item Te dwie reprezentacje zawierają \textbf{tę samą informację} ---
        można przechodzić między nimi bez straty.
\end{itemize}
\end{tcolorbox}

To jest silniejsze niż momenty --- CF to \textit{prawdziwy} odcisk palca rozkładu.

\begin{tcolorbox}[colback=yellow!5, colframe=yellow!70!black,
  title={Podsumowanie: 3 próby opisu rozkładu}]
\begin{center}
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{l c c c}
\toprule
\textbf{Metoda} & \textbf{Wzór} & \textbf{Jednoznaczna?} & \textbf{Zawsze istnieje?} \\
\midrule
Momenty & $\mathbb{E}[X^n]$ & Nie zawsze & Nie zawsze \\
MGF & $\mathbb{E}[e^{tX}]$ & Tak (jeśli istnieje) & \textbf{Nie} \\
CF & $\mathbb{E}[e^{itX}]$ & \textbf{Tak} & \textbf{Tak} \\
\bottomrule
\end{tabular}
\end{center}
CF wygrywa w~obu kategoriach --- dlatego jest \textbf{fundamentalnym}
narzędziem teorii prawdopodobieństwa.
\end{tcolorbox}

\medskip
\textbf{Historycznie:} Funkcję charakterystyczną wprowadził
\textbf{Paul Lévy} w~latach 1920-tych, ale idea sięga
\textbf{transformaty Fouriera} (Joseph Fourier, 1822).
CF to po prostu transformata Fouriera gęstości prawdopodobieństwa!
Jeśli znasz przetwarzanie sygnałów --- CF robi z~rozkładem dokładnie to,
co FFT robi z~sygnałem: zamienia go z~``dziedziny wartości''
na ``dziedzinę częstotliwości''.

%% ----- Element 7: Formalna definicja CF -----
\subsubsection{Element 7: Formalna definicja funkcji charakterystycznej}

Teraz możemy podać formalną definicję --- po powyższych wyjaśnieniach
powinna być już zrozumiała:

\begin{tcolorbox}[
  colback=lejepaGreen!8,
  colframe=lejepaGreen!80,
  fonttitle=\bfseries,
  title={Definicja: Funkcja charakterystyczna (CF)},
  breakable,
]

Dla zmiennej losowej $X$ o~rozkładzie $p(x)$:
\begin{equation}
\varphi_X(t) = \mathbb{E}\big[e^{itX}\big]
= \int_{-\infty}^{\infty} e^{itx}\,p(x)\,dx
\label{eq:cf_def}
\end{equation}

Rozbijmy ten wzór na elementy:
\begin{center}
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{c p{10cm}}
\toprule
\textbf{Symbol} & \textbf{Znaczenie} \\
\midrule
$\varphi_X(t)$ & wynik: jedna liczba zespolona dla każdej częstotliwości $t$ \\
$\mathbb{E}[\cdot]$ & wartość oczekiwana (``średnia'') \\
$e^{itx}$ & punkt na okręgu jednostkowym (wzór Eulera: $\cos(tx) + i\sin(tx)$) \\
$t$ & parametr --- ``częstotliwość'', po której skanujemy rozkład \\
$p(x)$ & gęstość prawdopodobieństwa rozkładu $X$ \\
$\int_{-\infty}^{\infty} \ldots\, dx$ & sumujemy po wszystkich możliwych wartościach $x$ \\
\bottomrule
\end{tabular}
\end{center}

\medskip
\textbf{Słownie:} ``Dla każdej częstotliwości $t$, bierzemy średnią ważoną
punktów na okręgu $e^{itx}$, gdzie wagami jest prawdopodobieństwo $p(x)$.''

\end{tcolorbox}

\begin{tcolorbox}[colback=orange!5, colframe=orange!70!black, breakable,
  title={Co oznacza ``częstotliwość'' $t$ w~funkcji charakterystycznej?}]

W~codziennym życiu \textbf{częstotliwość} to ile razy coś się powtarza
w~jednostce czasu --- np.\ fala dźwiękowa 440~Hz to 440~drgań na sekundę.

W~CF rola $t$ jest analogiczna, ale zamiast czasu mamy \textbf{oś wartości} $x$.
Przypomnijmy: $e^{itx} = \cos(tx) + i\sin(tx)$.

\textbf{Co robi $t$?} Kontroluje, \textit{jak szybko} funkcje $\cos$ i~$\sin$
oscylują, gdy przesuwamy się po osi~$x$:

\begin{center}
\begin{tikzpicture}[scale=0.75, >=stealth]
  % t = 0.5 (wolna oscylacja)
  \begin{scope}[shift={(-5.5,0)}]
    \draw[->] (-3.2,0) -- (3.5,0) node[right] {\small$x$};
    \draw[->] (0,-1.4) -- (0,1.6);
    \draw[thick, lejepaBlue, domain=-3:3, samples=80]
      plot (\x, {cos(0.5*\x r)});
    \node[above] at (0,1.6) {\small$\cos(0{,}5\cdot x)$};
    \node[below] at (0,-1.7) {\small$t = 0{,}5$ (wolna)};
  \end{scope}
  % t = 2 (średnia oscylacja)
  \begin{scope}[shift={(0,0)}]
    \draw[->] (-3.2,0) -- (3.5,0) node[right] {\small$x$};
    \draw[->] (0,-1.4) -- (0,1.6);
    \draw[thick, lejepaRed, domain=-3:3, samples=80]
      plot (\x, {cos(2*\x r)});
    \node[above] at (0,1.6) {\small$\cos(2x)$};
    \node[below] at (0,-1.7) {\small$t = 2$ (średnia)};
  \end{scope}
  % t = 6 (szybka oscylacja)
  \begin{scope}[shift={(5.5,0)}]
    \draw[->] (-3.2,0) -- (3.5,0) node[right] {\small$x$};
    \draw[->] (0,-1.4) -- (0,1.6);
    \draw[thick, lejepaGreen!70!black, domain=-3:3, samples=120]
      plot (\x, {cos(6*\x r)});
    \node[above] at (0,1.6) {\small$\cos(6x)$};
    \node[below] at (0,-1.7) {\small$t = 6$ (szybka)};
  \end{scope}
\end{tikzpicture}
\end{center}

\begin{itemize}[leftmargin=2em]
  \item \textbf{Małe $t$} (lewa fala) --- wolna oscylacja, szerokie ``góry i~doliny''.
    Takie $t$ wyczuwa \textbf{grube cechy} rozkładu
    (np.\ ``czy dane są skupione, czy rozrzucone?'').
  \item \textbf{Średnie $t$} (środkowa fala) --- umiarkowana oscylacja.
    Wyczuwa \textbf{średnie cechy} (np.\ ``czy rozkład jest symetryczny?'').
  \item \textbf{Duże $t$} (prawa fala) --- szybka oscylacja, gęste ``fale''.
    Wyczuwa \textbf{drobne szczegóły} rozkładu
    (np.\ ``czy ma ostre krawędzie?'').
\end{itemize}

\medskip
\textbf{Analogia:} Wyobraź sobie, że badasz teren wzrokiem:
\begin{itemize}[leftmargin=2em]
  \item \textbf{Małe $t$} $=$ patrzysz z~daleka (z~samolotu) ---
    widzisz góry i~doliny, ale nie drzewa.
  \item \textbf{Średnie $t$} $=$ patrzysz z~wieży ---
    widzisz domy, drogi, lasy.
  \item \textbf{Duże $t$} $=$ patrzysz przez lupę ---
    widzisz drobne kamyki, ale tracisz obraz całości.
\end{itemize}

\textbf{CF skanuje rozkład na wszystkich ``powiększeniach'' naraz} ---
dla każdego $t$ dostajemy jedną liczbę $\varphi(t)$,
a~razem tworzą one kompletny portret rozkładu.

\medskip
\textbf{Uwaga:} $t$ \textit{nie jest} częstotliwością w~sensie fizycznym (Hz).
To parametr matematyczny, ale zachowuje się analogicznie:
\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{l l}
\toprule
\textbf{Akustyka} & \textbf{Funkcja charakterystyczna} \\
\midrule
częstotliwość dźwięku [Hz] & parametr $t$ \\
przebieg czasowy sygnału & gęstość $p(x)$ \\
widmo (FFT) & CF $\varphi(t)$ \\
niskie tony (bas) & małe $|t|$ --- grube cechy \\
wysokie tony (sopran) & duże $|t|$ --- drobne szczegóły \\
\bottomrule
\end{tabular}
\end{center}
\end{tcolorbox}

\textbf{Kluczowe własności CF} (dlaczego jest tak użyteczna):
\begin{enumerate}[leftmargin=2em]
  \item \textbf{Jednoznaczność:} Dwa rozkłady są identyczne
        $\iff$ ich CF-y są identyczne.
        Dlatego porównując CF-y, porównujemy \textit{całe} rozkłady ---
        nie tylko średnią czy wariancję, ale cały kształt.
  \item \textbf{Zawsze istnieje:} W~przeciwieństwie do momentów (średnia, wariancja, \ldots),
        CF istnieje dla \textit{każdego} rozkładu --- nawet takiego,
        który nie ma skończonej wariancji.
  \item \textbf{Gładkość:} CF jest ciągłą, różniczkowalną funkcją parametru $t$ ---
        idealna do optymalizacji gradientowej.
\end{enumerate}

\textbf{Analogia:} Pomyśl o~strojeniu gitary. Uderzasz w~strunę i~słyszysz dźwięk ---
mieszaninę częstotliwości. Funkcja charakterystyczna robi to samo z~rozkładem
prawdopodobieństwa: ``rozkłada'' go na częstotliwości.
Różne rozkłady dają różne ``dźwięki''.

\begin{tcolorbox}[colback=yellow!5, colframe=yellow!70!black, breakable,
  title={Przykład: CF dla rzutu monetą}]

Żeby użyć CF, musimy przypisać wynikowi rzutu monetą \textbf{liczbę} ---
bo $e^{itX}$ wymaga, żeby $X$ było liczbą, a~nie napisem ``orzeł''/``reszka''.

\textbf{Jak przypisać?} Wybieramy \textit{dowolne} dwie liczby ---
ale $-1$ i~$+1$ to naturalny wybór, bo:
\begin{itemize}[leftmargin=2em]
  \item Są \textbf{symetryczne} wokół zera (moneta jest ``fair'').
  \item Średnia $\mathbb{E}[X] = \frac{1}{2}(-1) + \frac{1}{2}(+1) = 0$
        --- rozkład wycentrowany.
  \item Moglibyśmy wybrać np.\ $0$ i~$1$ (jak w~informatyce),
        ale $-1$ i~$+1$ lepiej oddają symetrię monety.
\end{itemize}

Zatem: $X = -1$ (orzeł) z~prawdopodobieństwem $\frac{1}{2}$,
$X = +1$ (reszka) z~prawdopodobieństwem $\frac{1}{2}$.
\begin{align*}
\varphi_X(t) &= \mathbb{E}[e^{itX}]
= \tfrac{1}{2}\,e^{it\cdot(-1)} + \tfrac{1}{2}\,e^{it\cdot(+1)} \\
&= \tfrac{1}{2}\,e^{-it} + \tfrac{1}{2}\,e^{it}
= \tfrac{1}{2}\big(\underbrace{e^{-it} + e^{it}}_{= 2\cos t}\big) = \cos t
\end{align*}
CF rzutu monetą (z~kodowaniem $-1/+1$) to po prostu $\cos t$ --- czysta oscylacja!

\medskip
\textbf{A~gdybyśmy wybrali $0$ i~$1$?} Zobaczmy:

$X = 0$ (orzeł) z~prawdopodobieństwem $\frac{1}{2}$,
$X = 1$ (reszka) z~prawdopodobieństwem $\frac{1}{2}$.
\begin{align*}
\varphi_X(t) &= \mathbb{E}[e^{itX}]
= \tfrac{1}{2}\,e^{it\cdot 0} + \tfrac{1}{2}\,e^{it\cdot 1} \\
&= \tfrac{1}{2}\cdot\underbrace{e^{0}}_{=\,1} + \tfrac{1}{2}\,e^{it}
= \tfrac{1}{2} + \tfrac{1}{2}\,e^{it}
= \tfrac{1}{2}\big(1 + e^{it}\big)
\end{align*}

Korzystając ze wzoru Eulera: $e^{it} = \cos t + i\sin t$:
\[
\varphi_X(t) = \tfrac{1}{2}(1 + \cos t + i\sin t)
= \underbrace{\tfrac{1}{2}(1 + \cos t)}_{\text{część rzeczywista}}
+ \underbrace{\tfrac{1}{2}\sin t}_{\text{część urojona}}\cdot i
\]

\medskip
\textbf{Porównanie dwóch kodowań:}
\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{l c c}
\toprule
& \textbf{Kodowanie $-1/+1$} & \textbf{Kodowanie $0/1$} \\
\midrule
CF & $\cos t$ & $\frac{1}{2}(1 + e^{it})$ \\
Postać & czysto rzeczywista & zespolona (ma część urojoną!) \\
$\varphi(0)$ & $1$ & $1$ \\
Symetria & symetryczna ($\varphi(t) = \varphi(-t)$) & niesymetryczna \\
\bottomrule
\end{tabular}
\end{center}

CF z~kodowaniem $-1/+1$ jest prostsza (czysto rzeczywista, symetryczna),
bo rozkład jest wycentrowany w~zerze ($\mathbb{E}[X] = 0$).
CF z~kodowaniem $0/1$ jest zespolona, bo rozkład ma niezerową
średnią ($\mathbb{E}[X] = \frac{1}{2}$).

Oba kodowania opisują ten sam eksperyment (rzut monetą) ---
różnią się tylko ``przesunięciem'' na osi liczbowej.

\medskip
\textbf{Uwaga:} Obie CF oscylują! W~kodowaniu $-1/+1$ mamy $\cos t$,
w~kodowaniu $0/1$ mamy $\frac{1}{2}(1 + \cos t)$ i~$\frac{1}{2}\sin t$ ---
wszędzie pojawiają się $\cos$ i~$\sin$, czyli oscylacje.
To nie przypadek: \textbf{rozkład dyskretny} (skończona liczba wartości,
tu: dwie) \textit{zawsze} daje oscylującą CF.
Dla porównania, Gauss $\mathcal{N}(0,1)$ --- rozkład ciągły ---
ma CF $e^{-t^2/2}$, która \textit{nie} oscyluje, tylko gładko opada do zera.
\end{tcolorbox}

%% ----- Element 8: CF standardowego Gaussa -----
\subsubsection{Element 8: CF standardowego Gaussa --- cel, do którego dążymy}

Jeśli $X \sim \mathcal{N}(0,1)$, to jego funkcja charakterystyczna ma piękną, prostą postać:
\begin{equation}
\varphi_{\mathcal{N}(0,1)}(t) = e^{-t^2/2}
\label{eq:cf_gauss}
\end{equation}

\textbf{Skąd ten wzór?} Wstawiamy gęstość Gaussa $p(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}$
do definicji CF:
\[
\varphi(t) = \int_{-\infty}^{\infty} e^{itx} \cdot \frac{1}{\sqrt{2\pi}}e^{-x^2/2}\,dx
= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{itx - x^2/2}\,dx
\]
Wykładnik $itx - x^2/2$ można uzupełnić do kwadratu (``completing the square'').
Pokażemy to krok po kroku:

\medskip
\textbf{Krok 1.} Wyciągamy $-\frac{1}{2}$ przed wyrażenie z~$x$:
\[
itx - \frac{x^2}{2}
= -\frac{1}{2}\!\left(x^2 - 2it\,x\right)
\]
(sprawdzenie: $-\frac{1}{2}\cdot x^2 = -\frac{x^2}{2}$ \checkmark\quad
$-\frac{1}{2}\cdot(-2it\,x) = +itx$ \checkmark)

\medskip
\textbf{Krok 2.} Uzupełniamy do kwadratu wewnątrz nawiasu.
Chcemy zapisać $x^2 - 2it\,x$ jako $(x - \text{coś})^2 - \text{reszta}$.

Przypomnijmy wzór skróconego mnożenia:
\[
(x - a)^2 = x^2 - 2ax + a^2
\]
Porównujemy z~naszym wyrażeniem $x^2 - 2it\,x$:
\[
\underbrace{x^2 - 2\,it\,x}_{\text{nasze}} \quad\longleftrightarrow\quad
\underbrace{x^2 - 2\,a\,x + a^2}_{(x-a)^2}
\qquad\Rightarrow\quad a = it
\]
Zatem:
\[
x^2 - 2it\,x = \underbrace{(x - it)^2}_{= x^2 - 2it\,x + (it)^2} - (it)^2
\]
Musimy odjąć $(it)^2$, bo $(x-it)^2$ zawiera dodatkowy składnik $(it)^2$,
którego nie było w~oryginale.

\medskip
\textbf{Krok 3.} Obliczamy $(it)^2$:
\[
(it)^2 = i^2 \cdot t^2 = (-1)\cdot t^2 = -t^2
\]
(bo $i = \sqrt{-1}$, więc $i^2 = -1$).

\medskip
\textbf{Krok 4.} Wstawiamy z~powrotem:
\[
x^2 - 2it\,x = (x - it)^2 - (-t^2) = (x-it)^2 + t^2
\]
A~z~czynnikiem $-\frac{1}{2}$ z~Kroku~1:
\[
-\frac{1}{2}\!\left(x^2 - 2it\,x\right)
= -\frac{1}{2}\!\left[(x - it)^2 + t^2\right]
= -\frac{1}{2}(x - it)^2 - \frac{t^2}{2}
\]

\begin{tcolorbox}[colback=blue!5, colframe=blue!50!black, title={Wynik: uzupełnienie do kwadratu}]
\[
\boxed{itx - \frac{x^2}{2} = -\frac{1}{2}(x - it)^2 - \frac{t^2}{2}}
\]
\end{tcolorbox}
Czynnik $e^{-t^2/2}$ nie zależy od~$x$, więc wychodzi przed całkę,
a~pozostała całka $\int e^{-(x-it)^2/2}\,dx = \sqrt{2\pi}$ (Gaussowska!),
skraca się z~$1/\sqrt{2\pi}$:
\[
\varphi(t) = e^{-t^2/2} \cdot \underbrace{\frac{1}{\sqrt{2\pi}}\int e^{-(x-it)^2/2}\,dx}_{=\,1} = e^{-t^2/2}
\]

\textbf{Intuicja:} $e^{-t^2/2}$ to ``dzwonek'' --- szybko maleje dla dużych $|t|$.
To znaczy, że Gauss $\mathcal{N}(0,1)$ jest ``gładki'' i~nie ma ostrych krawędzi
(wysokie częstotliwości są tłumione).

Zobaczmy, jak wyglądają CF różnych rozkładów na jednym wykresie:

\begin{center}
\begin{tikzpicture}[scale=0.95, >=stealth]
  % Osie
  \draw[->] (-5.5,0) -- (5.8,0) node[right] {$t$ (częstotliwość)};
  \draw[->] (0,-1.8) -- (0,2.5) node[above] {$\varphi(t)$};
  % Skala na osi Y
  \node[left] at (0,2) {\small$1$};
  \draw[gray, thin] (-0.1,2) -- (0.1,2);
  \node[left] at (0,-1.2) {\small$-0{,}6$};
  % Skala na osi X
  \foreach \x in {-4,-2,2,4} {
    \draw[gray, thin] (\x,-0.1) -- (\x,0.1);
    \node[below] at (\x,-0.15) {\small$\x$};
  }
  % --- CF Gaussa: e^{-t^2/2} --- (zawsze >= 0, gładki dzwonek)
  \draw[very thick, lejepaBlue, domain=-5.3:5.3, samples=100]
    plot (\x, {2*exp(-0.5*\x*\x)});
  \node[lejepaBlue, right] at (2.5,1.4)
    {$e^{-t^2/2}$ --- \textbf{Gauss} $\mathcal{N}(0,1)$};
  % --- CF monety (-1/+1): cos(t) --- (oscyluje)
  \draw[very thick, lejepaRed, domain=-5.3:5.3, samples=120]
    plot (\x, {2*cos(\x r)});
  \node[lejepaRed, right, text width=4cm] at (2.5,-1.5)
    {$\cos t$ --- \textbf{moneta} $\{-1,+1\}$};
  % --- CF rozkładu jednostajnego [-1,1]: sin(t)/t ---
  \draw[very thick, lejepaGreen!70!black, domain=-5.3:-0.05, samples=100]
    plot (\x, {2*sin(\x r)/\x});
  \draw[very thick, lejepaGreen!70!black, domain=0.05:5.3, samples=100]
    plot (\x, {2*sin(\x r)/\x});
  \fill[lejepaGreen!70!black] (0,2) circle (2pt); % wartosc w 0
  \node[lejepaGreen!70!black, left, text width=3.5cm, align=right] at (-2.5,1.5)
    {$\frac{\sin t}{t}$ --- \textbf{jednostajny} $[-1,1]$};
  % Linia y=0
  \draw[gray, thin, dashed] (-5.3,0) -- (5.3,0);
\end{tikzpicture}
\end{center}

\begin{tcolorbox}[colback=orange!5, colframe=orange!70!black, breakable,
  title={Jak czytać ten wykres?}]

Oś pozioma to \textbf{częstotliwość} $t$, oś pionowa to \textbf{wartość CF} $\varphi(t)$.
Wszystkie CF startują z~$\varphi(0) = 1$ (bo $e^{i \cdot 0 \cdot x} = 1$,
więc $\mathbb{E}[1] = 1$).

\textbf{Niebieska --- Gauss} $\mathcal{N}(0,1)$: CF $= e^{-t^2/2}$.
Gładki dzwonek, \textbf{zawsze $\geq 0$}, szybko opada do~zera.
Nie oscyluje --- bo Gauss jest ``najgładszym'' możliwym rozkładem.

\textbf{Czerwona --- moneta} $\{-1, +1\}$: CF $= \cos t$.
\textbf{Oscyluje} na zawsze (nigdy nie opada do zera!).
To dlatego, że moneta ma tylko 2~wartości --- jest ``maksymalnie niegładka''.

\textbf{Zielona --- rozkład jednostajny} $[-1,1]$: CF $= \frac{\sin t}{t}$.
Oscyluje, ale \textbf{amplituda maleje} --- pośredni przypadek
między gładkim Gaussem a~``ostrą'' monetą.
Rozkład jednostajny ma ``ostre krawędzie'' w~$x = \pm 1$,
co daje wolno gasnące oscylacje w~CF.

\medskip
\textbf{Reguła ogólna:}
\begin{itemize}[leftmargin=2em]
  \item Im \textbf{gładszy} rozkład $p(x)$, tym \textbf{szybciej} opada CF $\varphi(t)$.
  \item Im \textbf{ostrzejszy} rozkład (krawędzie, skoki), tym \textbf{wolniej} opada CF
        (i~tym więcej oscylacji).
\end{itemize}
To jest dokładna analogia z~akustyką: gładki dźwięk (flet)
ma mało harmonicznych (szybko gasnące widmo),
ostry dźwięk (perkusja) ma dużo harmonicznych (wolno gasnące widmo).
\end{tcolorbox}

\bigskip

\textbf{To jest nasz cel:} chcemy, żeby rzuty embeddingów miały CF
wyglądającą dokładnie jak $e^{-t^2/2}$ (niebieska krzywa powyżej).

%% ----- Element 9: Empiryczna CF -----
\subsubsection{Element 9: Empiryczna CF --- co mamy w~praktyce}

W~treningu nie znamy rozkładu $p(x)$ analitycznie --- mamy tylko \textbf{próbkę}:
$N$ liczb $u_1, u_2, \ldots, u_N$ (rzuty embeddingów z~batcha).

\textbf{Empiryczna funkcja charakterystyczna (ECF)} to przybliżenie prawdziwej CF
na podstawie próbki:
\begin{equation}
\hat{\varphi}_X(t) = \frac{1}{N}\sum_{j=1}^{N} e^{itu_j}
\label{eq:ecf}
\end{equation}

\textbf{Co tu robimy?}
\begin{enumerate}[leftmargin=2em]
  \item Dla każdego punktu $u_j$ obliczamy $e^{itu_j} = \cos(tu_j) + i\sin(tu_j)$
        --- to punkt na okręgu jednostkowym w~płaszczyźnie zespolonej.
  \item Uśredniamy te $N$ punktów (suma podzielona przez $N$).
  \item Wynik to jedno przybliżenie CF dla częstotliwości $t$.
  \item Powtarzamy dla różnych wartości $t$, żeby dostać całą krzywą $\hat{\varphi}(t)$.
\end{enumerate}

\textbf{Analogia:} Wyobraź sobie, że rzucasz $N$ kamyków do jeziora.
Każdy kamyk tworzy falę. ECF to \textit{średnia} tych fal --- jeśli kamyki
są rozłożone jak Gauss, średnia fal będzie wyglądała jak $e^{-t^2/2}$.
Jeśli nie --- będzie wyglądała inaczej.

\bigskip

Im więcej punktów ($N$ większe), tym ECF jest bliższa prawdziwej CF
(prawo wielkich liczb: średnia z~próbki zbliża się do wartości oczekiwanej).

%% ----- Element 10: Porównanie --- odległość między CF-ami -----
\subsubsection{Element 10: Porównanie --- jak mierzymy różnicę?}

Mamy dwie krzywe:
\begin{itemize}[leftmargin=2em]
  \item $\hat{\varphi}_X(t)$ --- to, co mamy (z~danych),
  \item $\varphi_{\mathcal{N}(0,1)}(t) = e^{-t^2/2}$ --- to, co chcemy.
\end{itemize}

Mierzymy, jak bardzo się różnią --- kwadrat różnicy, scałkowany po wszystkich
częstotliwościach $t$:
\[
\int_{-\infty}^{\infty}
\left|\hat{\varphi}_X(t) - e^{-t^2/2}\right|^2 \, dt
\]

\textbf{Dlaczego kwadrat $|\cdot|^2$?}
Ponieważ $\hat{\varphi}$ i~$\varphi$ to liczby zespolone, ``zwykłe'' odejmowanie
może dawać wartości ujemne i~dodatnie, które by się znosiły.
Kwadrat modułu $|a - b|^2$ jest zawsze $\geq 0$ --- mierzy ``odległość'' bez znaków.

\textbf{Dlaczego całka po~$t$?}
Bo chcemy porównać krzywe \textit{na całej dziedzinie}, nie tylko w~jednym punkcie.
Gdybyśmy sprawdzili tylko $t = 0$, obie CF dają $1$ (zawsze!) --- nic byśmy nie wykryli.

%% ----- Element 11: Waga $w(t)$ -----
\subsubsection{Element 11: Waga $w(t)$ --- dlaczego nie wszystkie częstotliwości są równie ważne}

Jest jeden problem: całka $\int_{-\infty}^{\infty} |\cdot|^2\,dt$ po \textit{wszystkich}
częstotliwościach może nie istnieć (być nieskończona).
Poza tym, bardzo wysokie częstotliwości ($|t| \gg 1$) niosą mało informacji,
bo obie CF i~tak są tam bliskie zeru.

Rozwiązanie: mnożymy przez \textbf{wagę} $w(t)$, która tłumi wysokie częstotliwości:
\begin{equation}
w(t) = e^{-t^2/\sigma^2}
\label{eq:weight}
\end{equation}

To Gaussowski ``filtr'' --- dla małych $|t|$ waga jest bliska $1$ (liczymy normalnie),
dla dużych $|t|$ waga spada do~$0$ (ignorujemy).
Parametr $\sigma$ kontroluje, jak szybko waga maleje (typowo $\sigma = 1$).

\begin{tcolorbox}[colback=orange!5, colframe=orange!70!black, breakable,
  title={Uwaga: $\sigma$ w~wadze to \textbf{nie} odchylenie standardowe danych!}]

Wzór $w(t) = e^{-t^2/\sigma^2}$ wygląda jak funkcja Gaussa ---
i~rzeczywiście \textit{nią jest}, ale w~\textbf{dziedzinie częstotliwości} $t$,
nie w~dziedzinie danych $x$.

\textbf{Dwa różne $\sigma$:}
\begin{itemize}[leftmargin=2em]
  \item $\sigma$ \textbf{danych} (np.\ w~$\mathcal{N}(0,\sigma^2)$) ---
    odchylenie standardowe rozkładu embeddingów.
    Mówi, jak szeroko rozrzucone są dane na osi~$x$.
  \item $\sigma$ \textbf{wagi} (tutaj, w~$w(t) = e^{-t^2/\sigma^2}$) ---
    parametr \textit{filtra}, który wybieramy my.
    Mówi, \textbf{jak szeroki zakres częstotliwości} $t$ bierzemy pod uwagę.
\end{itemize}

\textbf{Intuicja:}
\begin{itemize}[leftmargin=2em]
  \item Duże $\sigma$ wagi $\Rightarrow$ filtr opada powoli
    $\Rightarrow$ uwzględniamy też wysokie częstotliwości (więcej szczegółów).
  \item Małe $\sigma$ wagi $\Rightarrow$ filtr opada szybko
    $\Rightarrow$ patrzymy tylko na niskie częstotliwości (grube cechy).
\end{itemize}

To jest ten sam kształt krzywej ($e^{-(\cdot)^2}$), ale zastosowany
do zupełnie innej rzeczy --- raz ważymy dane,
raz ważymy częstotliwości.
\end{tcolorbox}

%% ----- Element 12: Pełny wzór EP -----
\subsubsection{Element 12: Pełny wzór testu Eppsa-Pulleya}

Składamy wszystkie elementy razem:

\begin{equation}
\boxed{
\text{EP} = N \int_{-\infty}^{\infty}
\underbrace{\left|\hat{\varphi}_X(t) - \varphi_{\mathcal{N}(0,1)}(t)\right|^2}_{\text{kwadrat różnicy CF-ów}}
\;\underbrace{w(t)}_{\text{waga}}\,dt
}
\label{eq:ep}
\end{equation}

gdzie:
\begin{center}
\renewcommand{\arraystretch}{1.8}
\begin{tabular}{p{3cm}p{4cm}p{7cm}}
\toprule
\textbf{Symbol} & \textbf{Wzór} & \textbf{Co robi?} \\
\midrule
$\hat{\varphi}_X(t)$ &
$\dfrac{1}{N}\displaystyle\sum_{j=1}^{N} e^{itu_j}$ &
ECF --- ``odcisk palca'' naszych danych (z~próbki) \\
$\varphi_{\mathcal{N}(0,1)}(t)$ &
$e^{-t^2/2}$ &
CF standardowego Gaussa --- ``odcisk palca'' celu \\
$w(t)$ &
$e^{-t^2/\sigma^2}$ &
Waga --- tłumi wysokie częstotliwości \\
$N$ &
rozmiar batcha &
Skalowanie --- większy batch $\Rightarrow$ bardziej czuły test \\
\bottomrule
\end{tabular}
\end{center}

\bigskip

\textbf{Interpretacja wyniku:}
\begin{itemize}[leftmargin=2em]
  \item $\text{EP} = 0$ $\Rightarrow$ embeddingi mają \textit{dokładnie} rozkład $\mathcal{N}(0,1)$
        w~tym kierunku (CF-y się pokrywają).
  \item $\text{EP} > 0$ $\Rightarrow$ embeddingi \textit{odstają} od Gaussa ---
        im większa wartość, tym gorsze dopasowanie.
  \item SIGReg minimalizuje EP $\Rightarrow$ ``pcha'' embeddingi w~stronę $\mathcal{N}(0,1)$.
\end{itemize}

\begin{keyinsight}[Dlaczego CF a~nie np.\ histogram?]
Histogram wymaga \textbf{sortowania} danych ($O(N \log N)$) i~nie ma gładkiego gradientu
(jest ``schodkowy''). CF jest \textbf{gładką} funkcją parametrów sieci ---
każdy $e^{itu_j}$ jest różniczkowalny po~$u_j$, a~$u_j = \mathbf{a}^\top f_\theta(\mathbf{x}_j)$
jest różniczkowalny po wagach $\theta$.
Dlatego gradient EP po~$\theta$ istnieje i~jest ograniczony --- idealne do SGD.
\end{keyinsight}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/characteristic_functions.pdf}
\caption{\textbf{Lewo}: Gęstości rzutów 1D --- izotropowy ma $\sigma^2=1$ (cel),
anizotropowy ma różne wariancje w~różnych kierunkach.
\textbf{Prawo}: Funkcje charakterystyczne i~błąd Eppsa-Pulleya ---
różnica między empiryczną CF a~docelową $e^{-t^2/2}$.}
\label{fig:cf}
\end{figure}

\subsection{SIGReg: pełna definicja}

\begin{definition}[Sketched Isotropic Gaussian Regularization]
\begin{equation}
\boxed{
\mathrm{SIGReg}_T(\mathbb{A}, \{f_\theta(\mathbf{x}_n)\}_{n=1}^N)
\triangleq \frac{1}{|\mathbb{A}|} \sum_{\mathbf{a} \in \mathbb{A}}
T\!\left(\{\mathbf{a}^\top f_\theta(\mathbf{x}_n)\}_{n=1}^N\right)
}
\label{eq:sigreg}
\end{equation}
gdzie $\mathbb{A} = \{\mathbf{a}_1, \ldots, \mathbf{a}_M\}$ to losowe kierunki jednostkowe,
$T$ to test Eppsa-Pulleya, $M \approx 1024$.
\end{definition}

\subsection{Dlaczego Epps-Pulley a nie inne testy?}

\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Test} & \textbf{Gradient} & \textbf{Stabilność} & \textbf{DDP} \\
\midrule
Momenty (Jarque-Bera) & eksplodujący & niska & tak \\
CDF (Cramér-von Mises) & wymaga sortowania & brak & nie \\
CF (Epps-Pulley) & ograniczony & wysoka & tak \\
\bottomrule
\end{tabular}
\end{center}

Epps-Pulley ma:
\begin{itemize}
  \item Ograniczony gradient: $|\partial \text{EP}/\partial z_i| \leq 4\sigma^2/N$ (Tw.~4 w artykule),
  \item Liniową złożoność: $O(N)$ pamięci i czasu,
  \item Naturalną kompatybilność z DDP: ECF to średnia $\Rightarrow$ \texttt{all\_reduce}.
\end{itemize}

