\clearpage
%% ============================================================
\section{Jak to wymusić? SIGReg}
\label{sec:sigreg}
%% ============================================================

Skoro wiemy, że embeddingi powinny mieć rozkład $\mathcal{N}(\mathbf{0}, \mathbf{I}_K)$,
potrzebujemy mechanizmu, który to wymusi podczas treningu.

\subsection{Idea: test statystyczny jako loss}

Zamiast heurystyk (whitening, stop-gradient, teacher-student), LeJEPA używa
\textbf{testu hipotez}:
\begin{equation}
H_0: P_\theta = \mathcal{N}(\mathbf{0}, \mathbf{I}_K)
\quad \text{vs.} \quad
H_1: P_\theta \neq \mathcal{N}(\mathbf{0}, \mathbf{I}_K)
\end{equation}

Problem: testowanie w $K$ wymiarach ($K = 128, 384, \ldots$) jest obliczeniowo trudne.

\subsection{Sketching: redukcja do testów 1D}

Kluczowy trik: zamiast testować w $\mathbb{R}^K$, rzutujemy na losowe kierunki $\mathbf{a} \in \mathbb{S}^{K-1}$:

\begin{equation}
\mathbf{a}^\top \mathbf{z} \sim \mathcal{N}(0, 1) \quad \forall\, \mathbf{a}
\quad \iff \quad
\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_K)
\end{equation}

To wynika z \textbf{Lematu Craméra-Wolda}: rozkład wielowymiarowy jest jednoznacznie
określony przez wszystkie swoje rzuty jednowymiarowe.

\paragraph{Dlaczego $\mathcal{N}(0,1)$ na rzutach gwarantuje izotropowość?}

Powyższy wzór ($\iff$) to potężne twierdzenie. Rozbijmy je na dwie strony:

\begin{tcolorbox}[
  colback=lejepaBlue!8,
  colframe=lejepaBlue!80,
  fonttitle=\bfseries,
  title={Strona $\Rightarrow$: jeśli $\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_K)$, to rzuty są $\mathcal{N}(0,1)$},
  breakable,
]
Jeśli embeddingi mają izotropowy rozkład Gaussa, to rzut na \textit{dowolny} kierunek
$\mathbf{a}$ daje $\mathcal{N}(0,1)$. Dlaczego?

Rzut $u = \mathbf{a}^\top \mathbf{z}$ to kombinacja liniowa składowych Gaussa,
więc sam jest gaussowski. Wystarczy obliczyć jego średnią i~wariancję:
\begin{align}
\mathbb{E}[u] &= \mathbb{E}[\mathbf{a}^\top \mathbf{z}]
= \mathbf{a}^\top \underbrace{\mathbb{E}[\mathbf{z}]}_{= \,\mathbf{0}} = 0 \nonumber \\[4pt]
\text{Var}[u] &= \text{Var}[\mathbf{a}^\top \mathbf{z}]
= \mathbf{a}^\top \underbrace{\text{Cov}(\mathbf{z})}_{= \,\mathbf{I}_K} \mathbf{a}
= \mathbf{a}^\top \mathbf{a}
= \underbrace{\|\mathbf{a}\|^2}_{= \,1} = 1 \nonumber
\end{align}
Średnia $= 0$, wariancja $= 1$, rozkład gaussowski $\Rightarrow$ $u \sim \mathcal{N}(0,1)$.

\textbf{Kluczowy moment:} wariancja wyszła $\mathbf{a}^\top \mathbf{I}_K \mathbf{a} = \|\mathbf{a}\|^2 = 1$
\textit{niezależnie od kierunku} $\mathbf{a}$.
To jest właśnie izotropowość --- macierz $\mathbf{I}_K$ traktuje wszystkie kierunki jednakowo.
\end{tcolorbox}

\begin{tcolorbox}[
  colback=lejepaRed!8,
  colframe=lejepaRed!80,
  fonttitle=\bfseries,
  title={Co by się stało, gdyby embeddingi NIE były izotropowe?},
  breakable,
]
Załóżmy, że embeddingi mają anizotropową macierz kowariancji, np.:
\[
\boldsymbol{\Sigma} = \begin{bmatrix} 4 & 0 \\ 0 & 0.25 \end{bmatrix}
\quad\text{(rozciągnięte wzdłuż $z_1$, ściśnięte wzdłuż $z_2$)}
\]

Teraz wariancja rzutu \textbf{zależy od kierunku}:
\begin{itemize}[leftmargin=2em]
  \item Kierunek $\mathbf{a} = (1, 0)$ (wzdłuż $z_1$):
        $\text{Var}[u] = \mathbf{a}^\top \boldsymbol{\Sigma}\, \mathbf{a}
        = (1,\; 0) \begin{bmatrix} 4 & 0 \\ 0 & 0.25 \end{bmatrix} \begin{bmatrix} 1 \\ 0 \end{bmatrix} = 4$
        \quad $\neq 1$ !
  \item Kierunek $\mathbf{a} = (0, 1)$ (wzdłuż $z_2$):
        $\text{Var}[u] = 0.25$
        \quad $\neq 1$ !
  \item Kierunek $\mathbf{a} = (1/\sqrt{2},\; 1/\sqrt{2})$ (po przekątnej):
        $\text{Var}[u] = 2.125$
        \quad $\neq 1$ !
\end{itemize}

Różne kierunki dają \textit{różne} wariancje $\Rightarrow$ rzuty \textit{nie} są $\mathcal{N}(0,1)$
$\Rightarrow$ test Eppsa-Pulleya to wykryje i~da duży loss.
\end{tcolorbox}

\begin{tcolorbox}[
  colback=lejepaGreen!8,
  colframe=lejepaGreen!80,
  fonttitle=\bfseries,
  title={Strona $\Leftarrow$ (Lemat Craméra-Wolda): jeśli WSZYSTKIE rzuty są $\mathcal{N}(0,1)$, to $\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_K)$},
  breakable,
]
To jest potężniejsze twierdzenie (i~trudniejsze do udowodnienia).
Mówi:

\textit{Jeśli rzut na \textbf{każdy} możliwy kierunek $\mathbf{a}$ daje
rozkład $\mathcal{N}(0,1)$, to jedynym rozkładem $K$-wymiarowym, który to spełnia,
jest $\mathcal{N}(\mathbf{0}, \mathbf{I}_K)$.}

Innymi słowy: nie ma ``podstępu'' --- nie istnieje żaden dziwny, nie-gaussowski rozkład
w~$\mathbb{R}^K$, który przypadkiem miałby wszystkie rzuty 1D gaussowskie.
Gaussowskość rzutów \textbf{jednoznacznie wymusza} gaussowskość całego rozkładu.

\textbf{Analogia:} Jeśli cień obiektu 3D jest okrągły z~\textit{każdej} strony
--- to obiekt \textit{musi} być kulą. Nie ma innego kształtu, który daje
okrągły cień ze~wszystkich kierunków.
\end{tcolorbox}

\begin{keyinsight}[Podsumowanie: dlaczego testujemy rzuty?]
\begin{enumerate}[leftmargin=2em, topsep=2pt]
  \item Chcemy: $\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_K)$ (izotropowy Gauss w~$K$ wymiarach).
  \item Lemat Craméra-Wolda mówi: wystarczy sprawdzić, że
        $\mathbf{a}^\top \mathbf{z} \sim \mathcal{N}(0,1)$ dla każdego kierunku $\mathbf{a}$.
  \item Jeśli jakikolwiek kierunek daje wariancję $\neq 1$ lub rozkład nie-gaussowski
        --- embeddingi \textit{nie} są izotropowe.
  \item SIGReg testuje $M = 1024$ losowych kierunków testem Eppsa-Pulleya
        i~minimalizuje odchylenia od $\mathcal{N}(0,1)$.
  \item Efekt: sieć jest ``pchana'' w~stronę $\mathcal{N}(\mathbf{0}, \mathbf{I}_K)$
        --- izotropowego Gaussa.
\end{enumerate}
\end{keyinsight}

%% ============================================================
\subsection{Co to jest rzut $u = \mathbf{a}^\top \mathbf{z}$?}
\label{sec:rzut}
%% ============================================================

W~powyższym wzorze pojawiły się trzy symbole: $u$, $\mathbf{a}$ i~$\mathbf{z}$.
Wyjaśnijmy \textit{dokładnie}, co każdy z~nich oznacza.

\subsubsection{$\mathbf{z}$ --- wektor embeddingu}

$\mathbf{z}$ to \textbf{wektor embeddingu} --- wynik działania sieci neuronowej (enkodera)
na jednym obrazie. Ma $K$ wymiarów (liczb):
\[
\mathbf{z} = \begin{bmatrix} z_1 \\ z_2 \\ \vdots \\ z_K \end{bmatrix}
\in \mathbb{R}^K
\]

W~LeJEPA typowo $K = 128$ (po projektorze MLP) lub $K = 384$ (surowy ViT CLS token).
Każdy obraz w~batchu daje swój wektor $\mathbf{z}_j$, więc z~batcha $N$ obrazów
dostajemy $N$ wektorów: $\mathbf{z}_1, \mathbf{z}_2, \ldots, \mathbf{z}_N$.

\textbf{Prosty przykład} ($K=3$, żebyśmy mogli to narysować):
\[
\mathbf{z} = \begin{bmatrix} 0.5 \\ -1.2 \\ 0.8 \end{bmatrix}
\quad \leftarrow \text{trzy liczby opisujące jeden obraz}
\]

\subsubsection{$\mathbf{a}$ --- losowy kierunek jednostkowy}

$\mathbf{a}$ to \textbf{losowy wektor kierunkowy} o~długości 1.
Też ma $K$ wymiarów (tyle samo co $\mathbf{z}$!):
\[
\mathbf{a} = \begin{bmatrix} a_1 \\ a_2 \\ \vdots \\ a_K \end{bmatrix}
\in \mathbb{R}^K, \qquad \|\mathbf{a}\| = \sqrt{a_1^2 + a_2^2 + \cdots + a_K^2} = 1
\]

Warunek $\|\mathbf{a}\| = 1$ oznacza, że $\mathbf{a}$ leży na \textbf{sferze jednostkowej}
$\mathbb{S}^{K-1}$. Co to takiego?

%% --- Co to jest sfera jednostkowa? ---
\paragraph{Co to jest sfera jednostkowa $\mathbb{S}^{K-1}$?}

\begin{tcolorbox}[
  colback=lejepaBlue!8,
  colframe=lejepaBlue!80,
  fonttitle=\bfseries,
  title={Definicja: sfera jednostkowa},
  breakable,
]
Sfera jednostkowa $\mathbb{S}^{K-1}$ to zbiór \textbf{wszystkich wektorów w~$\mathbb{R}^K$
o~długości dokładnie~1}:
\[
\mathbb{S}^{K-1} = \left\{\, \mathbf{a} \in \mathbb{R}^K \;:\; \|\mathbf{a}\| = 1 \,\right\}
\]

\textbf{Dlaczego $K\!-\!1$, a~nie $K$?}
Indeks górny oznacza \textit{wymiarowość samej sfery}, nie przestrzeni, w~której żyje:
\begin{itemize}[leftmargin=2em]
  \item $K=2$: $\mathbb{S}^1$ = \textbf{okrąg} --- krzywa 1-wymiarowa
        (żyje w~$\mathbb{R}^2$, ale sam okrąg to krzywa --- 1D).
  \item $K=3$: $\mathbb{S}^2$ = \textbf{sfera} --- powierzchnia 2-wymiarowa
        (żyje w~$\mathbb{R}^3$, ale sama powierzchnia kuli jest 2D).
  \item $K=128$: $\mathbb{S}^{127}$ = \textbf{hipersfera} 127-wymiarowa w~$\mathbb{R}^{128}$.
\end{itemize}

Ogólna reguła: warunek $\|\mathbf{a}\| = 1$ ``zabiera'' jeden stopień swobody,
więc sfera w~$\mathbb{R}^K$ ma wymiar $K - 1$.
\end{tcolorbox}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/unit_sphere.pdf}
\caption{\textbf{Lewo}: Dla $K=2$ sfera $\mathbb{S}^1$ to okrąg.
Każda strzałka to jeden losowy kierunek $\mathbf{a}$ --- wszystkie kończą się na okręgu
(bo $\|\mathbf{a}\|=1$).
\textbf{Środek}: Dla $K=3$ sfera $\mathbb{S}^2$ to powierzchnia kuli.
Kierunki $\mathbf{a}$ to strzałki od środka do powierzchni.
\textbf{Prawo}: Jak losujemy kierunek $\mathbf{a}$ --- trzy kroki.}
\label{fig:unit_sphere}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/unit_sphere_intuition.pdf}
\caption{\textbf{Lewo}: Zielone strzałki leżą na sferze ($\|\mathbf{a}\|=1$),
czerwone krzyżyki to wektory o~złej długości --- nie są na sferze.
\textbf{Prawo}: Dlaczego $\mathbb{S}^{K-1}$, a~nie $\mathbb{S}^K$
--- wymiarowość sfery jest o~1 mniejsza od wymiarowości przestrzeni.}
\label{fig:unit_sphere_intuition}
\end{figure}

%% --- Jak losujemy a? ---
\paragraph{Jak losujemy kierunek $\mathbf{a}$?}

Procedura jest prosta (3 kroki):
\begin{enumerate}[leftmargin=2em]
  \item \textbf{Losujemy} $K$ niezależnych liczb z~rozkładu $\mathcal{N}(0,1)$:
        \[
        \underbrace{v_1}_{\text{1 liczba}} ,\;\;
        \underbrace{v_2}_{\text{1 liczba}} ,\;\;
        \ldots ,\;\;
        \underbrace{v_K}_{\text{1 liczba}}
        \qquad\text{--- każda $v_k$ to osobna liczba z $\mathcal{N}(0,1)$}
        \]
        \textbf{Skąd się biorą te liczby?} Generuje je \textit{komputer} ---
        generator liczb pseudolosowych (w~PyTorch: \texttt{torch.randn(K)},
        w~NumPy: \texttt{np.random.randn(K)}).
        Rozkład $\mathcal{N}(0,1)$ oznacza, że:
        \begin{itemize}[leftmargin=2em, topsep=2pt]
          \item większość wylosowanych wartości będzie blisko~$0$ (np.\ między $-2$ a~$2$),
          \item wartości mogą być ujemne lub dodatnie (symetria wokół zera),
          \item bardzo duże $|v_k| > 3$ zdarzają się rzadko (poniżej $0.3\%$ szansy).
        \end{itemize}
        Te liczby są \textbf{losowe} --- za każdym razem inne.
        Nie są uczone, nie zależą od danych, nie mają gradientu.

        Razem tworzymy z~nich \textbf{wektor} o~$K$ elementach:
        $\mathbf{v} = (v_1, v_2, \ldots, v_K) \in \mathbb{R}^K$.
  \item \textbf{Obliczamy normę} (długość) tego wektora:
        $\|\mathbf{v}\| = \sqrt{v_1^2 + v_2^2 + \cdots + v_K^2}$ --- to jest \textit{jedna liczba}.
  \item \textbf{Normalizujemy} --- dzielimy każdy element wektora przez normę:
        $\mathbf{a} = \mathbf{v} \,/\, \|\mathbf{v}\|$,
        tzn.\ $a_k = v_k / \|\mathbf{v}\|$ dla $k = 1, \ldots, K$.
\end{enumerate}

\paragraph{Jak to wygląda w~praktyce? Konkretny przykład.}

W~kodzie to dosłownie \textbf{3 linijki}. Pokażmy dla $K = 5$
(w~LeJEPA będzie $K = 128$, ale zasada jest identyczna):

\begin{tcolorbox}[
  colback=black!3,
  colframe=black!50,
  fonttitle=\bfseries\ttfamily,
  title={Python / PyTorch},
  breakable,
]
\begin{verbatim}
import torch

K = 5
v = torch.randn(K)       # Krok 1: losujemy K liczb z N(0,1)
a = v / torch.norm(v)     # Kroki 2+3: normalizujemy
\end{verbatim}
\end{tcolorbox}

\textbf{Co dokładnie robi \texttt{torch.randn(5)}?}
Prosi komputer: ``daj mi 5 losowych liczb z~rozkładu Gaussa $\mathcal{N}(0,1)$''.
Za każdym uruchomieniem dostaniemy \textit{inne} liczby.

\begin{tcolorbox}[
  colback=lejepaGreen!8,
  colframe=lejepaGreen!80,
  fonttitle=\bfseries,
  title={Jak komputer generuje liczby z~rozkładu Gaussa?},
  breakable,
]
Komputer \textbf{nie} generuje Gaussa bezpośrednio. Robi to w~dwóch etapach:

\textbf{Etap~1: Liczby równomierne} $U_1, U_2 \in (0, 1)$.\\
To jest łatwe --- procesor ma wbudowany generator, który daje ``losowe'' liczby
równomiernie rozłożone między 0 a~1 (jak idealna kostka, ale ciągła).
Przykład: $U_1 = 0.374$, $U_2 = 0.821$.

\textbf{Etap~2: Transformacja Box-Mullera} --- klasyczny algorytm z~1958~roku
(G.E.P.~Box, M.E.~Muller, \textit{``A~Note on the Generation of Random Normal Deviates''},
The Annals of Mathematical Statistics).
Zamienia dwie liczby równomierne na dwie liczby gaussowskie:
\[
Z_1 = \sqrt{-2 \ln U_1} \cdot \cos(2\pi U_2), \qquad
Z_2 = \sqrt{-2 \ln U_1} \cdot \sin(2\pi U_2)
\]
Wynik: $Z_1$ i~$Z_2$ mają \textit{dokładnie} rozkład $\mathcal{N}(0,1)$.

\textbf{Dlaczego to działa?} Logarytm i~pierwiastek ``rozciągają'' równomierny rozkład
tak, że wartości blisko~$0$ stają się częste (środek dzwonu Gaussa),
a~wartości daleko od~$0$ stają się rzadkie (ogony dzwonu).
Cosinus i~sinus zapewniają symetrię (wartości ujemne i~dodatnie równie prawdopodobne).

\textbf{Przykład liczbowy:}
\[
U_1 = 0.374,\;\; U_2 = 0.821 \quad\Rightarrow\quad
Z_1 = \sqrt{-2 \ln 0.374} \cdot \cos(2\pi \cdot 0.821) = 1.40 \cdot 0.37 = 0.52
\]
Powtarzamy $K/2$ razy --- i~mamy $K$ liczb z~$\mathcal{N}(0,1)$.

\medskip
\textit{Uwaga:} W~praktyce PyTorch używa szybszego wariantu (algorytm Ziggurat),
ale idea jest ta sama: równomierne $\to$ przekształcenie $\to$ gaussowskie.
Jako użytkownik nie musisz o~tym myśleć --- \texttt{torch.randn(K)} robi to za Ciebie.
\end{tcolorbox}

Na przykład:

\medskip

\textbf{Uruchomienie 1:}
\begin{align}
\text{Krok 1:}\quad \mathbf{v} &= (\underset{\uparrow}{0.49},\;\; \underset{\uparrow}{-0.13},\;\; \underset{\uparrow}{0.65},\;\; \underset{\uparrow}{0.86},\;\; \underset{\uparrow}{-1.23})
\quad\leftarrow \text{5 losowych liczb} \nonumber \\
\text{Krok 2:}\quad \|\mathbf{v}\| &= \sqrt{0.49^2 + 0.13^2 + 0.65^2 + 0.86^2 + 1.23^2}
= \sqrt{2.83} = 1.683 \nonumber \\
\text{Krok 3:}\quad \mathbf{a} &= \mathbf{v} / 1.683
= (0.291,\;\; {-0.077},\;\; 0.386,\;\; 0.511,\;\; {-0.731}) \nonumber
\end{align}
Sprawdzenie: $\|\mathbf{a}\| = \sqrt{0.291^2 + 0.077^2 + 0.386^2 + 0.511^2 + 0.731^2} = 1.0$ \;\checkmark

\medskip

\textbf{Uruchomienie 2} (inne losowe liczby!):
\begin{align}
\text{Krok 1:}\quad \mathbf{v} &= (-1.07,\;\; 0.54,\;\; -0.31,\;\; 0.02,\;\; 1.51)
\quad\leftarrow \text{zupełnie inne 5 liczb} \nonumber \\
\text{Krok 2:}\quad \|\mathbf{v}\| &= 1.951 \nonumber \\
\text{Krok 3:}\quad \mathbf{a} &= (-0.548,\;\; 0.277,\;\; {-0.159},\;\; 0.010,\;\; 0.774) \nonumber
\end{align}

\begin{keyinsight}[Co się tu dzieje --- podsumowanie]
\begin{enumerate}[leftmargin=2em, topsep=2pt]
  \item Komputer ``rzuca kostką'' $K$ razy --- za każdym razem dostaje jedną losową
        liczbę z~$\mathcal{N}(0,1)$ (zwykle między $-2$ a~$2$).
  \item Te $K$ liczb składamy w~wektor $\mathbf{v}$.
  \item Wektor $\mathbf{v}$ ma jakąś długość (normę) --- dzielimy przez nią,
        żeby dostać wektor $\mathbf{a}$ o~długości dokładnie~1.
  \item Wynik: losowy kierunek $\mathbf{a}$ na sferze $\mathbb{S}^{K-1}$.
  \item Za każdym uruchomieniem kierunek jest \textit{inny} ---
        bo liczby $v_k$ są inne.
  \item W~SIGReg robimy to $M = 1024$ razy, dostając 1024 różnych kierunków.
\end{enumerate}
\end{keyinsight}

\begin{warningbox}[Uwaga: jaki to rozkład? Czy Gauss zawsze jest izotropowy?]
W~kroku~1 losujemy $K$ \textbf{oddzielnych, niezależnych} liczb, każdą z~\textit{jednowymiarowego}
Gaussa $\mathcal{N}(0,1)$. Każda $v_k$ to \textit{jedna} liczba (skalar).
Ale złożone razem w~wektor $\mathbf{v} = (v_1, \ldots, v_K)$ dają wektor o~$K$ wymiarach.

Ale ponieważ są niezależne --- to jest \textbf{dokładnie to samo}, co wylosowanie jednego
wektora z~$K$-wymiarowego Gaussa $\mathcal{N}(\mathbf{0}, \mathbf{I}_K)$:
\[
\underbrace{v_1 \sim \mathcal{N}(0,1), \;\; v_2 \sim \mathcal{N}(0,1), \;\; \ldots, \;\; v_K \sim \mathcal{N}(0,1)}_{\text{$K$ niezależnych losowań 1D}}
\quad\iff\quad
\underbrace{\mathbf{v} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_K)}_{\text{jedno losowanie $K$-wymiarowe}}
\]

To działa, bo macierz kowariancji $\mathbf{I}_K$ (macierz jednostkowa) oznacza:
każdy wymiar ma wariancję~1, i~wymiary są \textit{niezależne} (korelacja~= 0).

\bigskip

\textbf{Czy rozkład Gaussa zawsze jest izotropowy? NIE!}

Rozkład Gaussa wielowymiarowy $\mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$
może mieć \textit{dowolną} macierz kowariancji $\boldsymbol{\Sigma}$:
\begin{itemize}[leftmargin=2em]
  \item $\boldsymbol{\Sigma} = \mathbf{I}$ --- \textbf{izotropowy}: okręgi/sfery,
        \textit{żaden kierunek nie jest uprzywilejowany}.
  \item $\boldsymbol{\Sigma} = \text{diag}(\sigma_1^2, \ldots, \sigma_K^2)$ --- \textbf{anizotropowy}:
        elipsy/elipsoidy, różne wariancje w~różnych kierunkach.
  \item $\boldsymbol{\Sigma}$ z~elementami pozadiagonalnymi $\neq 0$ --- \textbf{skorelowany}:
        elipsy obrócone pod kątem.
\end{itemize}

\textbf{Dlaczego tu potrzebujemy izotropowego?}
Bo chcemy, żeby losowy kierunek $\mathbf{a}$ był \textbf{równomierny} na sferze ---
żaden kierunek nie może być bardziej prawdopodobny niż inny.
Gdybyśmy użyli anizotropowego Gaussa (np.\ $\sigma_1^2 = 10$, $\sigma_2^2 = 0.1$),
to po normalizacji kierunki byłyby ``skupione'' wokół osi~$z_1$
--- a~my byśmy testowali głównie ten jeden kierunek, pomijając inne.

Izotropowy Gauss $\mathcal{N}(\mathbf{0}, \mathbf{I}_K)$ gwarantuje, że po normalizacji
$\mathbf{a} = \mathbf{v}/\|\mathbf{v}\|$ dostajemy \textbf{rozkład jednostajny na sferze}
$\mathbb{S}^{K-1}$.
\end{warningbox}

\textbf{Przykład} ($K=3$):
\[
\mathbf{v} = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}
\quad\Rightarrow\quad
\|\mathbf{v}\| = \sqrt{1^2 + 1^2 + 1^2} = \sqrt{3}
\quad\Rightarrow\quad
\mathbf{a} = \frac{\mathbf{v}}{\|\mathbf{v}\|} = \begin{bmatrix} 1/\sqrt{3} \\ 1/\sqrt{3} \\ 1/\sqrt{3} \end{bmatrix}
\approx \begin{bmatrix} 0.577 \\ 0.577 \\ 0.577 \end{bmatrix}
\]
Sprawdzenie: $\|\mathbf{a}\| = \sqrt{3 \cdot (1/\sqrt{3})^2} = \sqrt{3 \cdot 1/3} = 1$ \;\checkmark

%% --- Rola a w SIGReg ---
\paragraph{Rola $\mathbf{a}$ w~SIGReg --- kiedy losujemy?}

\begin{tcolorbox}[
  colback=lejepaBlue!8,
  colframe=lejepaBlue!80,
  fonttitle=\bfseries,
  title={Kiedy losujemy kierunki $\mathbf{a}$? Raz czy wiele razy?},
  breakable,
]
W~oryginalnym LeJEPA kierunki losujemy \textbf{raz, przed rozpoczęciem treningu},
i~zostają \textbf{takie same} przez cały trening --- przez wszystkie epoki i~wszystkie batche:

\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{lcc}
\toprule
\textbf{Pytanie} & \textbf{Odpowiedź} \\
\midrule
Czy $\mathbf{a}$ zmienia się między epokami? & \textbf{Nie} --- te same kierunki \\
Czy $\mathbf{a}$ zmienia się między batchami? & \textbf{Nie} --- te same kierunki \\
Ile kierunków losujemy? & $M \approx 1024$ \\
Czy sieć ``uczy'' kierunki $\mathbf{a}$? & \textbf{Nie} --- brak gradientu \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Dlaczego to wystarczy?}
\begin{itemize}[leftmargin=2em, topsep=2pt]
  \item $M = 1024$ losowych kierunków w~$\mathbb{R}^{128}$ daje bardzo dobre pokrycie sfery
        $\mathbb{S}^{127}$ --- wystarczająco gęste, żeby wykryć anizotropię w~każdym kierunku.
  \item Lemat Craméra-Wolda teoretycznie wymaga \textit{wszystkich} kierunków
        ($\infty$), ale w~praktyce 1024 to wystarczające przybliżenie.
  \item Stałe kierunki mają zaletę: loss SIGReg jest \textbf{deterministyczny}
        (przy tym samym batchu daje tę samą wartość) --- to stabilizuje trening.
\end{itemize}

\textbf{Analogia:} Wyobraź sobie, że chcesz sprawdzić, czy piłka jest idealnie okrągła.
Mierzysz ją linijką w~1024 losowych kierunkach.
Nie musisz zmieniać kierunków przy każdym pomiarze ---
te same 1024 kierunków wystarczą, żeby wykryć każdą deformację.

\medskip

\textbf{Kod:}
\begin{verbatim}
# Przed treningiem (raz!):
A = torch.randn(M, K)                 # M=1024 wektorów, każdy K-wymiarowy
A = A / torch.norm(A, dim=1, keepdim=True)  # normalizacja

# W pętli treningowej (każdy batch):
for epoch in range(100):
    for batch in dataloader:
        z = encoder(batch)        # embeddingi [N, K]
        u = z @ A.T               # rzuty [N, M] - używamy TYCH SAMYCH A!
        loss = epps_pulley(u)      # test na każdym z M kierunków
\end{verbatim}
\end{tcolorbox}

\textbf{Geometrycznie:} $\mathbf{a}$ to po prostu \textit{kierunek w~przestrzeni} ---
strzałka wskazująca ``w~którą stronę patrzymy'' na nasze dane.

\subsubsection{$u = \mathbf{a}^\top \mathbf{z}$ --- iloczyn skalarny (rzut)}

Teraz najważniejsze: \textbf{iloczyn skalarny} $\mathbf{a}^\top \mathbf{z}$:
\begin{equation}
u = \mathbf{a}^\top \mathbf{z}
= a_1 \cdot z_1 + a_2 \cdot z_2 + \cdots + a_K \cdot z_K
= \sum_{k=1}^{K} a_k z_k
\label{eq:projection}
\end{equation}

\textbf{To jest po prostu suma iloczynów element-po-elemencie.}
Wynik to \textit{jedna liczba} $u \in \mathbb{R}$ --- nie wektor!
Z~wektora o~$K$ wymiarach zrobiliśmy jedną liczbę.

\textbf{Pełny przykład liczbowy} ($K=3$):
\begin{align}
u &= \mathbf{a}^\top \mathbf{z}
= a_1 \cdot z_1 + a_2 \cdot z_2 + a_3 \cdot z_3 \nonumber \\
&= 0.577 \cdot 0.5 + 0.577 \cdot (-1.2) + 0.577 \cdot 0.8 \nonumber \\
&= 0.289 + (-0.693) + 0.462 \nonumber \\
&= 0.058 \label{eq:projection_example}
\end{align}

Z~wektora 3D $(0.5, -1.2, 0.8)$ dostaliśmy jedną liczbę: $u = 0.058$.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/projection_example.pdf}
\caption{Przykład liczbowy: z~wektora $\mathbf{z} \in \mathbb{R}^3$ i~kierunku
$\mathbf{a} \in \mathbb{R}^3$ obliczamy jedną liczbę $u = \mathbf{a}^\top \mathbf{z}$.}
\label{fig:projection_example}
\end{figure}

\subsubsection{Geometryczna interpretacja --- rzut na prostą}

Iloczyn skalarny $u = \mathbf{a}^\top \mathbf{z}$ ma prostą interpretację geometryczną:

\textbf{$u$ to długość cienia} --- gdybyś postawił wektor $\mathbf{z}$ w~przestrzeni
i~poświecił na niego latarką z~kierunku prostopadłego do $\mathbf{a}$,
to cień $\mathbf{z}$ na osi $\mathbf{a}$ miałby długość $u$.

Formalnie: $u$ to \textbf{rzut ortogonalny} $\mathbf{z}$ na kierunek $\mathbf{a}$.
Punkt $u \cdot \mathbf{a}$ to najbliższy punkt na \textit{prostej wyznaczonej przez $\mathbf{a}$}
do~punktu~$\mathbf{z}$.

\begin{tcolorbox}[
  colback=lejepaGreen!8,
  colframe=lejepaGreen!80,
  fonttitle=\bfseries,
  title={Notacja: $\{\,t\,\mathbf{a} : t \in \mathbb{R}\,\}$ --- co to znaczy?},
  breakable,
]
Zapis $\{\,t\,\mathbf{a} : t \in \mathbb{R}\,\}$ to \textbf{notacja opisowa zbioru}
(ang.\ \textit{set-builder notation}). Czytamy ją tak:

\[
\underbrace{\{}_{\text{zbiór}}
\;\underbrace{t\,\mathbf{a}}_{\text{elementów postaci }t \cdot \mathbf{a}}
\;\underbrace{:}_{\text{takich, że}}
\;\underbrace{t \in \mathbb{R}}_{\text{$t$ jest dowolną liczbą rzeczywistą}}
\underbrace{\}}_{\text{koniec opisu}}
\]

Czyli: ``zbiór wszystkich punktów $t \cdot \mathbf{a}$, gdzie $t$ przebiega
od~$-\infty$ do~$+\infty$''. Geometrycznie to jest \textbf{prosta}
przechodząca przez początek układu współrzędnych w~kierunku $\mathbf{a}$:

\medskip
\begin{center}
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{rcl}
$t = 0$ & $\Rightarrow$ & punkt $\mathbf{0} = (0, 0, \ldots)$ --- początek układu \\
$t = 1$ & $\Rightarrow$ & punkt $\mathbf{a}$ --- sam wektor kierunkowy \\
$t = 2$ & $\Rightarrow$ & punkt $2\mathbf{a}$ --- dwa razy dalej w~tym samym kierunku \\
$t = -1$ & $\Rightarrow$ & punkt $-\mathbf{a}$ --- w~przeciwnym kierunku \\
$t = 0.5$ & $\Rightarrow$ & punkt $0.5\,\mathbf{a}$ --- w~połowie drogi do $\mathbf{a}$ \\
\end{tabular}
\end{center}

\medskip
Wszystkie te punkty leżą na jednej prostej ---
a~rzut $u$ mówi, \textit{w~którym miejscu} na tej prostej ``ląduje'' cień wektora $\mathbf{z}$.
\end{tcolorbox}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/projection_explanation.pdf}
\caption{\textbf{Lewo}: Embeddingi $\mathbf{z}_j$ w~2D.
\textbf{Środek}: Rzut na kierunek $\mathbf{a}$ --- każdy punkt ``spada'' na czerwoną prostą
(linia przerywana = prostopadła).
\textbf{Prawo}: Wynikowe wartości $u_j$ na osi liczbowej + histogram (wyjaśnienie niżej).}
\label{fig:projection_explanation}
\end{figure}

\paragraph{Po co histogram na prawym panelu?}

Na środkowym panelu zrzutowaliśmy $N = 8$ embeddingów na kierunek $\mathbf{a}$
i~dostaliśmy 8 liczb: $u_1, u_2, \ldots, u_8$.
Na prawym panelu robimy z~nimi dwie rzeczy:

\begin{enumerate}[leftmargin=2em]
  \item \textbf{Oś liczbowa} (górna część) --- pokazuje, \textit{gdzie} dokładnie
        wylądowała każda wartość $u_j$. Pomarańczowe kwadraty to poszczególne
        rzuty, podpisane liczbami (np.\ $u_3 = 0.8$, $u_5 = -1.1$).

  \item \textbf{Histogram} (dolna część) --- grupuje wartości $u_j$ w~``przedziały'' (bins)
        i~pokazuje, \textit{ile wartości} wpadło w~każdy przedział.
        Im wyższy słupek, tym więcej embeddingów ma rzut w~tym zakresie.

        \textbf{Po co?} Bo chcemy zobaczyć \textbf{kształt rozkładu} tych $N$ liczb.
        Histogram to najprostszy sposób na wizualizację rozkładu:
        \begin{itemize}[leftmargin=2em, topsep=2pt]
          \item Jeśli histogram wygląda jak \textbf{dzwonek} (dużo wartości blisko~0,
                mało daleko od~0) --- embeddingi w~tym kierunku wyglądają jak Gauss.
          \item Jeśli histogram jest \textbf{płaski}, \textbf{przesunięty} lub ma
                \textbf{dwa szczyty} --- embeddingi \textit{nie} są gaussowskie.
        \end{itemize}

  \item \textbf{Czarna krzywa} $\mathcal{N}(0,1)$ --- to nasz \textit{cel}.
        Pokazuje, jak powinien wyglądać idealny rozkład.
        Im bardziej histogram pokrywa się z~czarną krzywą, tym lepiej.
\end{enumerate}

\begin{keyinsight}[Histogram $\to$ test EP --- po co nam coś więcej niż histogram?]
Histogram to dobra \textit{wizualizacja} dla człowieka, ale zły \textit{loss} dla sieci:
\begin{itemize}[leftmargin=2em, topsep=2pt]
  \item Histogram jest ``schodkowy'' --- nie ma gładkiego gradientu
        (mała zmiana $u_j$ nie zmienia histogramu, dopóki punkt nie przeskoczy granicy binu).
  \item Wymaga sortowania danych --- $O(N \log N)$.
  \item Kształt zależy od wyboru binów (ile? jak szerokie?).
\end{itemize}
Dlatego SIGReg \textbf{nie} porównuje histogramów, lecz używa \textbf{funkcji charakterystycznych}
(sekcja~\ref{sec:sigreg}.4 dalej) --- mają gładki gradient, złożoność $O(N)$
i~nie wymagają żadnych arbitralnych wyborów.

\end{keyinsight}

\subsubsection{Od jednego kierunku do wielu --- dlaczego testujemy wiele $\mathbf{a}$?}

Jeden kierunek $\mathbf{a}$ daje nam jedną ``perspektywę'' na dane.
Ale różne kierunki mogą ujawnić różne problemy:
\begin{itemize}[leftmargin=2em]
\item Kierunek wzdłuż osi $z_1$ sprawdza, czy wariancja w~wymiarze 1 jest poprawna.
\item Kierunek pod kątem sprawdza, czy wymiary są nieskorelowane.
\item Tylko testując \textit{wiele} kierunków, możemy stwierdzić, że rozkład jest
      izotropowy w~\textit{wszystkich} wymiarach.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/projection_directions.pdf}
\caption{\textbf{Górny rząd}: Embeddingi izotropowe --- rzuty we wszystkich kierunkach
wyglądają jak $\mathcal{N}(0,1)$ (wariancja $\approx 1$).
\textbf{Dolny rząd}: Embeddingi anizotropowe (rozciągnięte wzdłuż $z_1$) ---
różne kierunki dają \textit{różne} wariancje (od $0.25$ do $4.0$).
SIGReg to wykrywa i~koryguje.}
\label{fig:projection_directions}
\end{figure}

\subsubsection{Podsumowanie: od obrazów do liczb 1D}

Cały pipeline wygląda tak:

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/projection_pipeline.pdf}
\caption{Pipeline SIGReg: batch obrazów $\to$ enkoder $\to$ embeddingi $K$-wymiarowe $\to$
rzut na losowy kierunek $\mathbf{a}$ $\to$ $N$ liczb 1D $\to$ test Eppsa-Pulleya $\to$ loss.}
\label{fig:projection_pipeline}
\end{figure}

\begin{keyinsight}[Dlaczego rzutujemy zamiast testować w~$K$ wymiarach?]
Test w~$K = 128$ wymiarach wymaga \textbf{astronomicznie dużo} danych
(tzw.\ ``klątwa wymiarowości'').
Rzut $u = \mathbf{a}^\top \mathbf{z}$ redukuje problem do 1D, gdzie test Eppsa-Pulleya
działa doskonale nawet z~małym batchem ($N = 64$).
Lemat Craméra-Wolda gwarantuje, że jeśli \textit{wszystkie} rzuty 1D
są $\mathcal{N}(0,1)$, to cały rozkład $K$-wymiarowy jest $\mathcal{N}(\mathbf{0}, \mathbf{I}_K)$.
W~praktyce losujemy $M \approx 1024$ kierunków --- to wystarczy.
\end{keyinsight}

%% ============================================================
\subsection{Test Eppsa-Pulleya na rzutach 1D}
%% ============================================================

Mamy rzut $u = \mathbf{a}^\top \mathbf{z}$ --- jedną liczbę dla każdego embeddingu.
Z~batcha $N$ embeddingów dostajemy $N$ takich liczb: $u_1, u_2, \ldots, u_N$.

\textbf{Pytanie:} czy te $N$ liczb wyglądają jak próbka z~$\mathcal{N}(0,1)$?

Żeby to sprawdzić, porównujemy dwa obiekty --- \textbf{funkcje charakterystyczne} (CF).
Zanim przejdziemy do wzorów SIGReg, wyjaśnijmy dokładnie, czym jest CF
i~skąd się bierze --- od samych podstaw.

%% ----- Element 0: Motywacja --- po co nam CF? -----
\subsubsection{Element 0: Po co nam funkcja charakterystyczna?}

Mamy $N$ liczb $u_1, \ldots, u_N$ (rzuty embeddingów).
Chcemy odpowiedzieć na pytanie: \textit{``Czy te liczby pochodzą z~rozkładu $\mathcal{N}(0,1)$?''}

Moglibyśmy narysować histogram i~``na oko'' porównać z~krzywą dzwonową ---
ale to nie jest precyzyjne i~nie nadaje się jako funkcja straty do treningu sieci.
Potrzebujemy narzędzia, które:
\begin{enumerate}[leftmargin=2em]
  \item \textbf{jednoznacznie identyfikuje} rozkład (nie tylko średnią czy wariancję,
        ale \textit{cały kształt}),
  \item daje \textbf{liczbę}, którą można zróżniczkować (gradient dla optymalizatora),
  \item działa dobrze nawet z~\textbf{małą próbką} ($N = 64$).
\end{enumerate}
Takim narzędziem jest właśnie \textbf{funkcja charakterystyczna}.

%% ----- Element 0.5: Liczby zespolone --- minimum potrzebne -----
\subsubsection{Element 0.5: Minimum o~liczbach zespolonych}

Funkcja charakterystyczna używa liczb zespolonych, więc przypomnijmy
absolutne minimum:

\begin{tcolorbox}[colback=gray!5, colframe=gray!60, breakable,
  title={Liczby zespolone --- co musimy wiedzieć}]

\textbf{Jednostka urojona:}
\[
i = \sqrt{-1}, \qquad i^2 = -1
\]
Liczba zespolona to para: $z = a + bi$, gdzie $a$ to \textbf{część rzeczywista},
$b$ to \textbf{część urojona}.

\textbf{Przykłady:}
\begin{itemize}[leftmargin=2em]
  \item $3 + 2i$ --- część rzeczywista $= 3$, część urojona $= 2$
  \item $5$ --- to też liczba zespolona (z~częścią urojoną $= 0$)
  \item $4i$ --- część rzeczywista $= 0$, część urojona $= 4$
\end{itemize}

\textbf{Geometrycznie:} Liczba zespolona to punkt na płaszczyźnie.
Oś pozioma to część rzeczywista, oś pionowa to część urojona:

\begin{center}
\begin{tikzpicture}[scale=0.9]
  \draw[->] (-1.5,0) -- (4.5,0) node[right] {Re (rzeczywista)};
  \draw[->] (0,-1.5) -- (0,3.5) node[above] {Im (urojona)};
  \fill[lejepaBlue] (3,2) circle (3pt) node[above right] {$3 + 2i$};
  \draw[dashed, gray] (3,0) node[below] {$3$} -- (3,2) -- (0,2) node[left] {$2$};
  \fill[lejepaRed] (0,0) circle (2pt);
\end{tikzpicture}
\end{center}

\textbf{Moduł} (odległość od zera):
$|a + bi| = \sqrt{a^2 + b^2}$

\end{tcolorbox}

%% ----- Element 0.6: Wzór Eulera -----
\subsubsection{Element 0.6: Wzór Eulera --- klucz do CF}

Najważniejszy wzór, na którym opiera się cała teoria CF:

\begin{tcolorbox}[colback=blue!5, colframe=blue!60!black, breakable,
  title={Wzór Eulera (Leonhard Euler, 1748)}]
\begin{equation}
e^{i\theta} = \cos\theta + i\sin\theta
\label{eq:euler}
\end{equation}
gdzie $\theta$ to dowolna liczba rzeczywista (kąt w~radianach).
\end{tcolorbox}

Skąd się bierze ten wzór? I~dlaczego kreśli okrąg?
Wyprowadźmy to krok po kroku --- dla czystej przyjemności matematyki.

%% --- Wyprowadzenie wzoru Eulera ---
\begin{tcolorbox}[colback=violet!5, colframe=violet!70!black, breakable,
  title={Wyprowadzenie wzoru Eulera z~szeregów Taylora}]

\textbf{Punkt wyjścia:} trzy funkcje, które znamy z~analizy matematycznej,
mają rozwinięcia w~\textbf{szereg Taylora} (nieskończoną sumę potęg):

\medskip
\textbf{Krok 1.} Szereg Taylora dla $e^x$ (wokół $x = 0$):
\[
e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \frac{x^4}{4!} + \frac{x^5}{5!} + \cdots
= \sum_{n=0}^{\infty} \frac{x^n}{n!}
\]
(Przypomnienie: $n! = 1 \cdot 2 \cdot 3 \cdots n$, np.\ $4! = 24$.)

\medskip
\textbf{Krok 2.} Szeregi Taylora dla $\cos$ i~$\sin$:
\[
\cos\theta = 1 - \frac{\theta^2}{2!} + \frac{\theta^4}{4!} - \frac{\theta^6}{6!} + \cdots
\qquad\text{(same \textbf{parzyste} potęgi, znaki naprzemienne)}
\]
\[
\sin\theta = \theta - \frac{\theta^3}{3!} + \frac{\theta^5}{5!} - \frac{\theta^7}{7!} + \cdots
\qquad\text{(same \textbf{nieparzyste} potęgi, znaki naprzemienne)}
\]

\medskip
\textbf{Krok 3.} Wstawiamy $x = i\theta$ do szeregu $e^x$:
\[
e^{i\theta} = 1 + (i\theta) + \frac{(i\theta)^2}{2!} + \frac{(i\theta)^3}{3!}
+ \frac{(i\theta)^4}{4!} + \frac{(i\theta)^5}{5!} + \cdots
\]

\textbf{Krok 4.} Obliczamy kolejne potęgi $i$:
\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{c c l}
\toprule
Potęga & Wartość & Komentarz \\
\midrule
$i^0$ & $1$   & (każda liczba do potęgi $0$ to $1$) \\
$i^1$ & $i$   &  \\
$i^2$ & $-1$  & (definicja $i$) \\
$i^3$ & $-i$  & ($i^2 \cdot i = -1 \cdot i$) \\
$i^4$ & $1$   & ($i^2 \cdot i^2 = (-1)(-1) = 1$ --- \textbf{cykl się powtarza!}) \\
$i^5$ & $i$   & jak $i^1$ \\
$i^6$ & $-1$  & jak $i^2$ \\
\bottomrule
\end{tabular}
\end{center}
Cykl: $1, \; i, \; -1, \; -i, \; 1, \; i, \; -1, \; -i, \; \ldots$

\medskip
\textbf{Krok 5.} Wstawiamy potęgi $i$ do każdego wyrazu:
\begin{align*}
e^{i\theta} &= \underbrace{1}_{i^0}
+ \underbrace{i\theta}_{i^1}
+ \underbrace{\frac{i^2\theta^2}{2!}}_{=\, -\frac{\theta^2}{2!}}
+ \underbrace{\frac{i^3\theta^3}{3!}}_{=\, -\frac{i\theta^3}{3!}}
+ \underbrace{\frac{i^4\theta^4}{4!}}_{=\, +\frac{\theta^4}{4!}}
+ \underbrace{\frac{i^5\theta^5}{5!}}_{=\, +\frac{i\theta^5}{5!}}
+ \cdots
\end{align*}

\textbf{Krok 6.} Grupujemy --- wyrazy \textbf{bez $i$} osobno, wyrazy \textbf{z~$i$} osobno:
\[
e^{i\theta} =
\underbrace{\left(1 - \frac{\theta^2}{2!} + \frac{\theta^4}{4!} - \cdots\right)}_{\text{to jest }\cos\theta\text{!}}
+ \;i\;\underbrace{\left(\theta - \frac{\theta^3}{3!} + \frac{\theta^5}{5!} - \cdots\right)}_{\text{to jest }\sin\theta\text{!}}
\]

\medskip
\begin{center}
\fbox{\Large $e^{i\theta} = \cos\theta + i\sin\theta \qquad\checkmark$}
\end{center}

Wzór wyłania się sam --- wystarczyło wstawić $i\theta$ do szeregu $e^x$
i~pogrupować wyrazy!
\end{tcolorbox}

%% --- Dowód, że to okrąg ---
\begin{tcolorbox}[colback=green!5, colframe=green!60!black, breakable,
  title={Dowód, że $e^{i\theta}$ kreśli okrąg jednostkowy}]

Wiemy, że $e^{i\theta} = \cos\theta + i\sin\theta$.
Na płaszczyźnie zespolonej ten punkt ma współrzędne:
\[
(\underbrace{\cos\theta}_{\text{Re}},\; \underbrace{\sin\theta}_{\text{Im}})
\]

\textbf{Pytanie:} Jaką figurę kreślą te punkty, gdy $\theta$ przebiega od $0$ do $2\pi$?

\medskip
\textbf{Odległość od zera} (moduł):
\[
|e^{i\theta}| = \sqrt{(\cos\theta)^2 + (\sin\theta)^2}
= \sqrt{\underbrace{\cos^2\theta + \sin^2\theta}_{= \; 1 \text{ (tożsamość trygonometryczna!)}}}
= \sqrt{1} = 1
\]

Odległość od zera wynosi \textbf{zawsze 1}, niezależnie od~$\theta$.

\medskip
A~co to jest zbiór punktów w~odległości dokładnie $1$ od punktu $(0,0)$?

To jest \textbf{okrąg o~promieniu~1} ze środkiem w~zerze --- \textbf{okrąg jednostkowy}!

\[
\boxed{|e^{i\theta}| = 1 \quad\text{dla każdego }\theta
\qquad\Longrightarrow\qquad
e^{i\theta}\text{ leży na okręgu jednostkowym}}
\]

\medskip
\textbf{Co więcej:}
\begin{itemize}[leftmargin=2em]
  \item Gdy $\theta = 0$: \quad $e^{i\cdot 0} = \cos 0 + i\sin 0 = 1 + 0 = 1$
        \quad(start na prawo)
  \item Gdy $\theta$ rośnie: punkt przesuwa się \textbf{w~górę} i~w~lewo
        (kierunek $\sin$ rośnie)
  \item Gdy $\theta = 2\pi$: \quad $e^{i\cdot 2\pi} = \cos 2\pi + i\sin 2\pi = 1$
        \quad(wraca na start --- pełen obrót!)
\end{itemize}

\begin{center}
\begin{tikzpicture}[scale=1.4]
  \draw[gray!30] (0,0) circle (1);
  \draw[->] (-1.5,0) -- (1.7,0) node[right] {Re};
  \draw[->] (0,-1.5) -- (0,1.7) node[above] {Im};
  % Ścieżka z strzałkami
  \draw[lejepaBlue, thick, ->, >=stealth,
    postaction={decorate, decoration={markings,
      mark=at position 0.15 with {\arrow{>}},
      mark=at position 0.40 with {\arrow{>}},
      mark=at position 0.65 with {\arrow{>}},
      mark=at position 0.90 with {\arrow{>}}
    }}]
    (1,0) arc (0:350:1);
  % Punkty
  \fill[lejepaRed] (1,0) circle (2.5pt)
    node[below right] {$\theta\!=\!0$: start};
  \fill[lejepaBlue] ({cos(90)},{sin(90)}) circle (2pt)
    node[above right] {$\theta\!=\!\frac{\pi}{2}$};
  \fill[lejepaBlue] ({cos(180)},{sin(180)}) circle (2pt)
    node[above left] {$\theta\!=\!\pi$};
  \fill[lejepaBlue] ({cos(270)},{sin(270)}) circle (2pt)
    node[below left] {$\theta\!=\!\frac{3\pi}{2}$};
  % Promień = 1
  \draw[dashed, gray, thick] (0,0) -- ({cos(45)},{sin(45)})
    node[midway, below right] {$r = 1$};
  % Etykieta
  \node[text width=4cm, align=center] at (2.8, 0)
    {$\theta$ rośnie\\$\Downarrow$\\punkt obraca się\\przeciwnie do\\zegara};
\end{tikzpicture}
\end{center}

\end{tcolorbox}

\textbf{Bonus --- najpiękniejszy wzór matematyki.}
Wstawiając $\theta = \pi$ do wzoru Eulera:
\[
e^{i\pi} = \cos\pi + i\sin\pi = -1 + 0 = -1
\]
Przenosząc na drugą stronę:
\begin{equation}
\boxed{e^{i\pi} + 1 = 0}
\label{eq:euler_identity}
\end{equation}
Ten wzór łączy pięć najważniejszych stałych matematyki
($e$, $i$, $\pi$, $1$, $0$) w~jednym równaniu.
Nazywamy go \textbf{tożsamością Eulera}
--- często uważaną za najpiękniejszy wzór w~całej matematyce.

\bigskip
\textbf{Co to znaczy?} Wyrażenie $e^{i\theta}$ to punkt na \textbf{okręgu jednostkowym}
(okrąg o~promieniu~1) na płaszczyźnie zespolonej:

\begin{center}
\begin{tikzpicture}[scale=1.6]
  % okrąg
  \draw[gray!40] (0,0) circle (1);
  \draw[->] (-1.4,0) -- (1.6,0) node[right] {Re};
  \draw[->] (0,-1.4) -- (0,1.6) node[above] {Im};
  % punkt
  \def\ang{40}
  \coordinate (P) at ({cos(\ang)},{sin(\ang)});
  \draw[thick, lejepaBlue, ->] (0,0) -- (P);
  \fill[lejepaBlue] (P) circle (1.5pt) node[above right]
    {$e^{i\theta} = \cos\theta + i\sin\theta$};
  % kąt
  \draw[lejepaRed, thick] (0.3,0) arc (0:\ang:0.3);
  \node[lejepaRed] at (0.45,0.15) {$\theta$};
  % rzuty
  \draw[dashed, gray] (P) -- ({cos(\ang)},0)
    node[below, black] {\small$\cos\theta$};
  \draw[dashed, gray] (P) -- (0,{sin(\ang)})
    node[left, black] {\small$\sin\theta$};
  % specjalne punkty
  \fill[gray] (1,0) circle (1pt) node[below right] {\small$e^{i\cdot 0}=1$};
  \fill[gray] (0,1) circle (1pt) node[above left] {\small$e^{i\pi/2}=i$};
  \fill[gray] (-1,0) circle (1pt) node[below left] {\small$e^{i\pi}=-1$};
  \fill[gray] (0,-1) circle (1pt) node[below left] {\small$e^{i\cdot 3\pi/2}=-i$};
\end{tikzpicture}
\end{center}

\textbf{Kluczowe obserwacje:}
\begin{itemize}[leftmargin=2em]
  \item Gdy $\theta$ rośnie, punkt $e^{i\theta}$ \textbf{obraca się} po okręgu
        (przeciwnie do zegara).
  \item Moduł jest zawsze $|e^{i\theta}| = \sqrt{\cos^2\theta + \sin^2\theta} = 1$ ---
        punkt nigdy nie oddala się od zera.
  \item $e^{i\theta}$ to \textbf{obrót o~kąt $\theta$} na płaszczyźnie zespolonej.
\end{itemize}

\textbf{Przykłady konkretne:}
\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{c c c c}
\toprule
$\theta$ & $\cos\theta$ & $\sin\theta$ & $e^{i\theta}$ \\
\midrule
$0$ & $1$ & $0$ & $1$ \\
$\pi/2 \approx 1{,}57$ & $0$ & $1$ & $i$ \\
$\pi \approx 3{,}14$ & $-1$ & $0$ & $-1$ \\
$2\pi \approx 6{,}28$ & $1$ & $0$ & $1$ (pełen obrót) \\
\bottomrule
\end{tabular}
\end{center}

%% ----- Element 0.7: Od Eulera do CF -----
\subsubsection{Element 0.7: Od wzoru Eulera do funkcji charakterystycznej}

Teraz zastosujmy wzór Eulera. Weźmy zmienną losową $X$
(np.\ nasze rzuty $u$) i~wpiszmy $\theta = tx$:
\[
e^{itx} = \cos(tx) + i\sin(tx)
\]

\begin{tcolorbox}[colback=gray!5, colframe=gray!60, breakable,
  title={Co oznacza $tx$ w~wyrażeniu $e^{itx}$?}]

W~wyrażeniu $e^{itx}$ mamy \textbf{dwie zmienne}:
\begin{itemize}[leftmargin=2em]
  \item $x$ --- \textbf{dana} (wartość z~naszych danych, np.\ rzut embeddingu).
        To jest konkretna liczba, np.\ $x = 1{,}3$ albo $x = -0{,}7$.
  \item $t$ --- \textbf{parametr}, który my wybieramy.
        Można go rozumieć jako ``częstotliwość skanowania''.
\end{itemize}

\textbf{Co robi iloczyn $tx$?} Określa \textbf{kąt obrotu} na okręgu jednostkowym.

Przypomnijmy: $e^{i\theta}$ to punkt na okręgu pod kątem $\theta$.
Tutaj $\theta = tx$, więc:
\begin{itemize}[leftmargin=2em]
  \item Gdy $t$ jest \textbf{małe} (np.\ $t = 0{,}1$),
        kąt $tx$ zmienia się \textbf{powoli} gdy $x$ rośnie ---
        nawet duże różnice w~$x$ dają małe różnice kąta.
  \item Gdy $t$ jest \textbf{duże} (np.\ $t = 10$),
        kąt $tx$ zmienia się \textbf{szybko} ---
        małe różnice w~$x$ powodują duże obroty na okręgu.
\end{itemize}

\textbf{Przykład liczbowy} --- dwa punkty danych $x_1 = 1{,}0$ i~$x_2 = 1{,}5$:

\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{c c c c l}
\toprule
$t$ & $tx_1$ & $tx_2$ & $tx_2 - tx_1$ & Interpretacja \\
\midrule
$0{,}5$ & $0{,}5$\,rad & $0{,}75$\,rad & $0{,}25$\,rad ($14°$)
  & wolne skanowanie --- punkty blisko siebie \\
$2$ & $2$\,rad & $3$\,rad & $1$\,rad ($57°$)
  & średnie skanowanie \\
$10$ & $10$\,rad & $15$\,rad & $5$\,rad ($286°$)
  & szybkie skanowanie --- punkty prawie po drugiej stronie \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Analogia radiowa:} $t$ to ``częstotliwość stacji radiowej'', na którą się stroimy.
Różne częstotliwości $t$ wychwytują różne cechy rozkładu,
tak jak różne stacje radiowe nadają różną muzykę.
CF~$\varphi(t)$ mówi nam ``co słychać na częstotliwości $t$''.
\end{tcolorbox}

\textbf{Wyobraź sobie:} Masz $N$ wartości $x_1, x_2, \ldots, x_N$ (np.\ rzuty embeddingów).
Dla ustalonego $t$, każda wartość $x_j$ daje punkt na okręgu:
\[
x_j \;\longrightarrow\; e^{itx_j} = \cos(tx_j) + i\sin(tx_j)
\quad\text{--- punkt na okręgu jednostkowym}
\]
Mamy więc $N$ punktów na okręgu. Ich \textbf{średnia} (środek ciężkości)
to przybliżenie funkcji charakterystycznej:

\begin{tcolorbox}[colback=orange!5, colframe=orange!70!black, breakable,
  title={Intuicja: CF to średnia punktów na okręgu}]

\begin{center}
\begin{tikzpicture}[scale=1.3]
  % okrąg
  \draw[gray!30] (0,0) circle (1);
  \draw[->] (-1.4,0) -- (1.6,0) node[right] {Re};
  \draw[->] (0,-1.4) -- (0,1.6) node[above] {Im};
  % punkty -- symulacja "blisko Gaussa"
  \foreach \ang/\lab in {15/1, 55/2, 130/3, 200/4, 280/5, 340/6} {
    \fill[lejepaBlue!70] ({cos(\ang)},{sin(\ang)}) circle (2pt);
  }
  % średnia ~blisko (0.1, 0.05)
  \fill[lejepaRed] (0.12,0.05) circle (3pt)
    node[above right] {$\overline{e^{itx_j}}$ = CF};
  \draw[lejepaRed, thick, ->] (0,0) -- (0.12,0.05);
  \node[below, text width=6cm, align=center] at (0,-1.7)
    {$N=6$ punktów (niebieskie) i~ich średnia (czerwona)};
\end{tikzpicture}
\end{center}

Jeśli dane $x_j$ są ``ładnie rozłożone'' (bliskie Gaussowi),
punkty rozkładają się \textbf{równomiernie} po okręgu i~ich średnia
jest bliska zeru. Jeśli dane mają strukturę (nie-Gaussowską),
punkty \textbf{skupiają się} w~jednym miejscu i~średnia jest daleko od zera.

To właśnie mierzy funkcja charakterystyczna!
\end{tcolorbox}

%% ----- Element 1: Formalna definicja CF -----
\subsubsection{Element 1: Formalna definicja funkcji charakterystycznej}

Teraz możemy podać formalną definicję --- po powyższych wyjaśnieniach
powinna być już zrozumiała:

\begin{tcolorbox}[
  colback=lejepaGreen!8,
  colframe=lejepaGreen!80,
  fonttitle=\bfseries,
  title={Definicja: Funkcja charakterystyczna (CF)},
  breakable,
]

Dla zmiennej losowej $X$ o~rozkładzie $p(x)$:
\begin{equation}
\varphi_X(t) = \mathbb{E}\big[e^{itX}\big]
= \int_{-\infty}^{\infty} e^{itx}\,p(x)\,dx
\label{eq:cf_def}
\end{equation}

Rozbijmy ten wzór na elementy:
\begin{center}
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{c p{10cm}}
\toprule
\textbf{Symbol} & \textbf{Znaczenie} \\
\midrule
$\varphi_X(t)$ & wynik: jedna liczba zespolona dla każdej częstotliwości $t$ \\
$\mathbb{E}[\cdot]$ & wartość oczekiwana (``średnia'') \\
$e^{itx}$ & punkt na okręgu jednostkowym (wzór Eulera: $\cos(tx) + i\sin(tx)$) \\
$t$ & parametr --- ``częstotliwość'', po której skanujemy rozkład \\
$p(x)$ & gęstość prawdopodobieństwa rozkładu $X$ \\
$\int_{-\infty}^{\infty} \ldots\, dx$ & sumujemy po wszystkich możliwych wartościach $x$ \\
\bottomrule
\end{tabular}
\end{center}

\medskip
\textbf{Słownie:} ``Dla każdej częstotliwości $t$, bierzemy średnią ważoną
punktów na okręgu $e^{itx}$, gdzie wagami jest prawdopodobieństwo $p(x)$.''

\end{tcolorbox}

\textbf{Kluczowe własności CF} (dlaczego jest tak użyteczna):
\begin{enumerate}[leftmargin=2em]
  \item \textbf{Jednoznaczność:} Dwa rozkłady są identyczne
        $\iff$ ich CF-y są identyczne.
        Dlatego porównując CF-y, porównujemy \textit{całe} rozkłady ---
        nie tylko średnią czy wariancję, ale cały kształt.
  \item \textbf{Zawsze istnieje:} W~przeciwieństwie do momentów (średnia, wariancja, \ldots),
        CF istnieje dla \textit{każdego} rozkładu --- nawet takiego,
        który nie ma skończonej wariancji.
  \item \textbf{Gładkość:} CF jest ciągłą, różniczkowalną funkcją parametru $t$ ---
        idealna do optymalizacji gradientowej.
\end{enumerate}

\textbf{Analogia:} Pomyśl o~strojeniu gitary. Uderzasz w~strunę i~słyszysz dźwięk ---
mieszaninę częstotliwości. Funkcja charakterystyczna robi to samo z~rozkładem
prawdopodobieństwa: ``rozkłada'' go na częstotliwości.
Różne rozkłady dają różne ``dźwięki''.

\begin{tcolorbox}[colback=yellow!5, colframe=yellow!70!black,
  title={Przykład: CF dla rzutu monetą}]
Rzut monetą: $X = -1$ (orzeł) z~prawdopodobieństwem $\frac{1}{2}$,
$X = +1$ (reszka) z~prawdopodobieństwem $\frac{1}{2}$.
\begin{align*}
\varphi_X(t) &= \mathbb{E}[e^{itX}]
= \tfrac{1}{2}\,e^{it\cdot(-1)} + \tfrac{1}{2}\,e^{it\cdot(+1)} \\
&= \tfrac{1}{2}\,e^{-it} + \tfrac{1}{2}\,e^{it}
= \tfrac{1}{2}\big(\underbrace{e^{-it} + e^{it}}_{= 2\cos t}\big) = \cos t
\end{align*}
CF rzutu monetą to po prostu $\cos t$ --- czysta oscylacja!
Wygląda zupełnie inaczej niż CF Gaussa (który jest $e^{-t^2/2}$, gładki dzwonek).
\end{tcolorbox}

%% ----- Element 2: CF standardowego Gaussa -----
\subsubsection{Element 2: CF standardowego Gaussa --- cel, do którego dążymy}

Jeśli $X \sim \mathcal{N}(0,1)$, to jego funkcja charakterystyczna ma piękną, prostą postać:
\begin{equation}
\varphi_{\mathcal{N}(0,1)}(t) = e^{-t^2/2}
\label{eq:cf_gauss}
\end{equation}

\textbf{Skąd ten wzór?} Wstawiamy gęstość Gaussa $p(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}$
do definicji CF:
\[
\varphi(t) = \int_{-\infty}^{\infty} e^{itx} \cdot \frac{1}{\sqrt{2\pi}}e^{-x^2/2}\,dx
= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{itx - x^2/2}\,dx
\]
Wykładnik $itx - x^2/2$ można uzupełnić do kwadratu (``completing the square'').
Pokażemy to krok po kroku:

\medskip
\textbf{Krok 1.} Wyciągamy $-\frac{1}{2}$ przed wyrażenie z~$x$:
\[
itx - \frac{x^2}{2}
= -\frac{1}{2}\!\left(x^2 - 2it\,x\right)
\]
(sprawdzenie: $-\frac{1}{2}\cdot x^2 = -\frac{x^2}{2}$ \checkmark\quad
$-\frac{1}{2}\cdot(-2it\,x) = +itx$ \checkmark)

\medskip
\textbf{Krok 2.} Uzupełniamy do kwadratu wewnątrz nawiasu.
Chcemy zapisać $x^2 - 2it\,x$ jako $(x - \text{coś})^2 - \text{reszta}$.

Przypomnijmy wzór skróconego mnożenia:
\[
(x - a)^2 = x^2 - 2ax + a^2
\]
Porównujemy z~naszym wyrażeniem $x^2 - 2it\,x$:
\[
\underbrace{x^2 - 2\,it\,x}_{\text{nasze}} \quad\longleftrightarrow\quad
\underbrace{x^2 - 2\,a\,x + a^2}_{(x-a)^2}
\qquad\Rightarrow\quad a = it
\]
Zatem:
\[
x^2 - 2it\,x = \underbrace{(x - it)^2}_{= x^2 - 2it\,x + (it)^2} - (it)^2
\]
Musimy odjąć $(it)^2$, bo $(x-it)^2$ zawiera dodatkowy składnik $(it)^2$,
którego nie było w~oryginale.

\medskip
\textbf{Krok 3.} Obliczamy $(it)^2$:
\[
(it)^2 = i^2 \cdot t^2 = (-1)\cdot t^2 = -t^2
\]
(bo $i = \sqrt{-1}$, więc $i^2 = -1$).

\medskip
\textbf{Krok 4.} Wstawiamy z~powrotem:
\[
x^2 - 2it\,x = (x - it)^2 - (-t^2) = (x-it)^2 + t^2
\]
A~z~czynnikiem $-\frac{1}{2}$ z~Kroku~1:
\[
-\frac{1}{2}\!\left(x^2 - 2it\,x\right)
= -\frac{1}{2}\!\left[(x - it)^2 + t^2\right]
= -\frac{1}{2}(x - it)^2 - \frac{t^2}{2}
\]

\begin{tcolorbox}[colback=blue!5, colframe=blue!50!black, title={Wynik: uzupełnienie do kwadratu}]
\[
\boxed{itx - \frac{x^2}{2} = -\frac{1}{2}(x - it)^2 - \frac{t^2}{2}}
\]
\end{tcolorbox}
Czynnik $e^{-t^2/2}$ nie zależy od~$x$, więc wychodzi przed całkę,
a~pozostała całka $\int e^{-(x-it)^2/2}\,dx = \sqrt{2\pi}$ (Gaussowska!),
skraca się z~$1/\sqrt{2\pi}$:
\[
\varphi(t) = e^{-t^2/2} \cdot \underbrace{\frac{1}{\sqrt{2\pi}}\int e^{-(x-it)^2/2}\,dx}_{=\,1} = e^{-t^2/2}
\]

\textbf{Intuicja:} $e^{-t^2/2}$ to ``dzwonek'' --- szybko maleje dla dużych $|t|$.
To znaczy, że Gauss $\mathcal{N}(0,1)$ jest ``gładki'' i~nie ma ostrych krawędzi
(wysokie częstotliwości są tłumione).

\bigskip

\textbf{To jest nasz cel:} chcemy, żeby rzuty embeddingów miały CF
wyglądającą dokładnie jak $e^{-t^2/2}$.

%% ----- Element 3: Empiryczna CF -----
\subsubsection{Element 3: Empiryczna CF --- co mamy w~praktyce}

W~treningu nie znamy rozkładu $p(x)$ analitycznie --- mamy tylko \textbf{próbkę}:
$N$ liczb $u_1, u_2, \ldots, u_N$ (rzuty embeddingów z~batcha).

\textbf{Empiryczna funkcja charakterystyczna (ECF)} to przybliżenie prawdziwej CF
na podstawie próbki:
\begin{equation}
\hat{\varphi}_X(t) = \frac{1}{N}\sum_{j=1}^{N} e^{itu_j}
\label{eq:ecf}
\end{equation}

\textbf{Co tu robimy?}
\begin{enumerate}[leftmargin=2em]
  \item Dla każdego punktu $u_j$ obliczamy $e^{itu_j} = \cos(tu_j) + i\sin(tu_j)$
        --- to punkt na okręgu jednostkowym w~płaszczyźnie zespolonej.
  \item Uśredniamy te $N$ punktów (suma podzielona przez $N$).
  \item Wynik to jedno przybliżenie CF dla częstotliwości $t$.
  \item Powtarzamy dla różnych wartości $t$, żeby dostać całą krzywą $\hat{\varphi}(t)$.
\end{enumerate}

\textbf{Analogia:} Wyobraź sobie, że rzucasz $N$ kamyków do jeziora.
Każdy kamyk tworzy falę. ECF to \textit{średnia} tych fal --- jeśli kamyki
są rozłożone jak Gauss, średnia fal będzie wyglądała jak $e^{-t^2/2}$.
Jeśli nie --- będzie wyglądała inaczej.

\bigskip

Im więcej punktów ($N$ większe), tym ECF jest bliższa prawdziwej CF
(prawo wielkich liczb: średnia z~próbki zbliża się do wartości oczekiwanej).

%% ----- Element 4: Porównanie --- odległość między CF-ami -----
\subsubsection{Element 4: Porównanie --- jak mierzymy różnicę?}

Mamy dwie krzywe:
\begin{itemize}[leftmargin=2em]
  \item $\hat{\varphi}_X(t)$ --- to, co mamy (z~danych),
  \item $\varphi_{\mathcal{N}(0,1)}(t) = e^{-t^2/2}$ --- to, co chcemy.
\end{itemize}

Mierzymy, jak bardzo się różnią --- kwadrat różnicy, scałkowany po wszystkich
częstotliwościach $t$:
\[
\int_{-\infty}^{\infty}
\left|\hat{\varphi}_X(t) - e^{-t^2/2}\right|^2 \, dt
\]

\textbf{Dlaczego kwadrat $|\cdot|^2$?}
Ponieważ $\hat{\varphi}$ i~$\varphi$ to liczby zespolone, ``zwykłe'' odejmowanie
może dawać wartości ujemne i~dodatnie, które by się znosiły.
Kwadrat modułu $|a - b|^2$ jest zawsze $\geq 0$ --- mierzy ``odległość'' bez znaków.

\textbf{Dlaczego całka po~$t$?}
Bo chcemy porównać krzywe \textit{na całej dziedzinie}, nie tylko w~jednym punkcie.
Gdybyśmy sprawdzili tylko $t = 0$, obie CF dają $1$ (zawsze!) --- nic byśmy nie wykryli.

%% ----- Element 5: Waga $w(t)$ -----
\subsubsection{Element 5: Waga $w(t)$ --- dlaczego nie wszystkie częstotliwości są równie ważne}

Jest jeden problem: całka $\int_{-\infty}^{\infty} |\cdot|^2\,dt$ po \textit{wszystkich}
częstotliwościach może nie istnieć (być nieskończona).
Poza tym, bardzo wysokie częstotliwości ($|t| \gg 1$) niosą mało informacji,
bo obie CF i~tak są tam bliskie zeru.

Rozwiązanie: mnożymy przez \textbf{wagę} $w(t)$, która tłumi wysokie częstotliwości:
\begin{equation}
w(t) = e^{-t^2/\sigma^2}
\label{eq:weight}
\end{equation}

To Gaussowski ``filtr'' --- dla małych $|t|$ waga jest bliska $1$ (liczymy normalnie),
dla dużych $|t|$ waga spada do~$0$ (ignorujemy).
Parametr $\sigma$ kontroluje, jak szybko waga maleje (typowo $\sigma = 1$).

%% ----- Element 6: Pełny wzór EP -----
\subsubsection{Element 6: Pełny wzór testu Eppsa-Pulleya}

Składamy wszystkie elementy razem:

\begin{equation}
\boxed{
\text{EP} = N \int_{-\infty}^{\infty}
\underbrace{\left|\hat{\varphi}_X(t) - \varphi_{\mathcal{N}(0,1)}(t)\right|^2}_{\text{kwadrat różnicy CF-ów}}
\;\underbrace{w(t)}_{\text{waga}}\,dt
}
\label{eq:ep}
\end{equation}

gdzie:
\begin{center}
\renewcommand{\arraystretch}{1.8}
\begin{tabular}{p{3cm}p{4cm}p{7cm}}
\toprule
\textbf{Symbol} & \textbf{Wzór} & \textbf{Co robi?} \\
\midrule
$\hat{\varphi}_X(t)$ &
$\dfrac{1}{N}\displaystyle\sum_{j=1}^{N} e^{itu_j}$ &
ECF --- ``odcisk palca'' naszych danych (z~próbki) \\
$\varphi_{\mathcal{N}(0,1)}(t)$ &
$e^{-t^2/2}$ &
CF standardowego Gaussa --- ``odcisk palca'' celu \\
$w(t)$ &
$e^{-t^2/\sigma^2}$ &
Waga --- tłumi wysokie częstotliwości \\
$N$ &
rozmiar batcha &
Skalowanie --- większy batch $\Rightarrow$ bardziej czuły test \\
\bottomrule
\end{tabular}
\end{center}

\bigskip

\textbf{Interpretacja wyniku:}
\begin{itemize}[leftmargin=2em]
  \item $\text{EP} = 0$ $\Rightarrow$ embeddingi mają \textit{dokładnie} rozkład $\mathcal{N}(0,1)$
        w~tym kierunku (CF-y się pokrywają).
  \item $\text{EP} > 0$ $\Rightarrow$ embeddingi \textit{odstają} od Gaussa ---
        im większa wartość, tym gorsze dopasowanie.
  \item SIGReg minimalizuje EP $\Rightarrow$ ``pcha'' embeddingi w~stronę $\mathcal{N}(0,1)$.
\end{itemize}

\begin{keyinsight}[Dlaczego CF a~nie np.\ histogram?]
Histogram wymaga \textbf{sortowania} danych ($O(N \log N)$) i~nie ma gładkiego gradientu
(jest ``schodkowy''). CF jest \textbf{gładką} funkcją parametrów sieci ---
każdy $e^{itu_j}$ jest różniczkowalny po~$u_j$, a~$u_j = \mathbf{a}^\top f_\theta(\mathbf{x}_j)$
jest różniczkowalny po wagach $\theta$.
Dlatego gradient EP po~$\theta$ istnieje i~jest ograniczony --- idealne do SGD.
\end{keyinsight}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/characteristic_functions.pdf}
\caption{\textbf{Lewo}: Gęstości rzutów 1D --- izotropowy ma $\sigma^2=1$ (cel),
anizotropowy ma różne wariancje w~różnych kierunkach.
\textbf{Prawo}: Funkcje charakterystyczne i~błąd Eppsa-Pulleya ---
różnica między empiryczną CF a~docelową $e^{-t^2/2}$.}
\label{fig:cf}
\end{figure}

\subsection{SIGReg: pełna definicja}

\begin{definition}[Sketched Isotropic Gaussian Regularization]
\begin{equation}
\boxed{
\mathrm{SIGReg}_T(\mathbb{A}, \{f_\theta(\mathbf{x}_n)\}_{n=1}^N)
\triangleq \frac{1}{|\mathbb{A}|} \sum_{\mathbf{a} \in \mathbb{A}}
T\!\left(\{\mathbf{a}^\top f_\theta(\mathbf{x}_n)\}_{n=1}^N\right)
}
\label{eq:sigreg}
\end{equation}
gdzie $\mathbb{A} = \{\mathbf{a}_1, \ldots, \mathbf{a}_M\}$ to losowe kierunki jednostkowe,
$T$ to test Eppsa-Pulleya, $M \approx 1024$.
\end{definition}

\subsection{Dlaczego Epps-Pulley a nie inne testy?}

\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Test} & \textbf{Gradient} & \textbf{Stabilność} & \textbf{DDP} \\
\midrule
Momenty (Jarque-Bera) & eksplodujący & niska & tak \\
CDF (Cramér-von Mises) & wymaga sortowania & brak & nie \\
CF (Epps-Pulley) & ograniczony & wysoka & tak \\
\bottomrule
\end{tabular}
\end{center}

Epps-Pulley ma:
\begin{itemize}
  \item Ograniczony gradient: $|\partial \text{EP}/\partial z_i| \leq 4\sigma^2/N$ (Tw.~4 w artykule),
  \item Liniową złożoność: $O(N)$ pamięci i czasu,
  \item Naturalną kompatybilność z DDP: ECF to średnia $\Rightarrow$ \texttt{all\_reduce}.
\end{itemize}

